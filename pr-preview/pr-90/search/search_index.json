{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"VisionAI","text":"<p>Documentation for VisionAI toolkit.</p> <p> </p> <p> Ready to deploy Vision AI scenarios. Try our CLI now.  </p> <p>"},{"location":"#overview","title":"Overview","text":"<p>VisionAI provides a set of command line utilities for running Vision AI scenarios. We also have several industrial use-cases that are ready for production deployment. VisionAI is primarily a CLI (Command Line Interface) - but it also provides a Web-based GUI for managing your cameras. VisionAI is a developed by the team at Visionify - and is a part of the Workplace Safety suite of products.</p> <p>Key features of VisionAI include:</p> <ul> <li>Easy to use: VisionAI is designed to be a no-code platform for deploying Vision AI solutions for common  workplace safety scenarios. The command-line interface (CLI) or Web interface are designed to be used by non-technical users as well as technical users.</li> <li>Production Ready: VisionAI provides a library of production-ready Workplace Safety scenarios that can be directly used out of the box. Each of these scenarios is trained from a large dataset of real-world images and videos.</li> <li>Open Source: VisionAI library is Open Source is available through GPLv3 license. You can use it for free and contribute to it as well. We also offer commercial licenses to enterprises that want to modify the codebase.</li> <li>Custom Scenarios: VisionAI can supports custom use-cases and scenarios. We have a flexible architecture based on NVIDIA triton server to serve multiple models, and common scenario pattern that can be used to easily add new use-cases. Check the customization documentation to get more information on how this can be achieved.</li> <li>Integrations: VisionAI currently integrates with REDIS PubSub and Azure Event hubs for alerts and notifications. We have roadmap plans to add support for other message brokers as well.</li> <li>Cloud Ready: VisionAI is available as as an Azure Marketplace offer. This provides scalable architecture for enterprise installations supporting a large number of cameras and scenarios.</li> </ul>"},{"location":"#scenarios","title":"Scenarios","text":"<p>We support several Workplace health and safety scenarios. These are listed below. We are continuously adding new scenarios and you can contact us if you need a scenario that is not listed here.</p> <p>VisionAI focuses on workplace health and safety models - and majority of the models you see here have been developed with that in mind. We are continuously working on new scenarios - and our current scenario repo consists of over 60 scenarios that are listed here.</p> <p></p>"},{"location":"#install-visionai","title":"Install VisionAI","text":""},{"location":"#quick-start","title":"Quick Start","text":"<ul> <li>Install VisionAI application through <code>PyPI</code>.</li> </ul> <pre><code>$ pip install visionai\n</code></pre> <ul> <li>Initialize visionai by running the following command. This would download the required dependencies on your setup. This might take a few minutes to complete as some of the dependencies are pretty large in size (1GB for Pytorch etc.)</li> </ul> <pre><code>$ visionai init\n</code></pre> <ul> <li>Once the initialization is successful, you can see the following output:</li> </ul> <pre><code>$ visionai init\n- - - - - - - - - - - - - - - - - - - - - - - - - - -\nGrafana server is at: http://localhost:3003\nRedis server is at: localhost:6379\nTriton http server is at: http://localhost:8000\nTriton grpc server is at: grpc://localhost:8001\nTriton prometheus metrics server is at: http://localhost:8002/metrics\nAPI server already running at: http://localhost:3002\nWeb server already running at: http://localhost:3001\n</code></pre> <ul> <li>This indicates that different services required for running VisionAI are running on your machine.</li> </ul> Service Port Purpose <code>Web UI</code> <code>http://localhost:3001</code> VisionAI Web-app <code>Web API</code> <code>http://localhost:3002</code> VisionAI API service <code>Triton HTTP</code> <code>http://localhost:8000</code> Triton Model server (http) <code>Triton GRPC</code> <code>grpc://localhost:8001</code> Triton Model server (grpc) <code>Triton Metrics</code> <code>http://localhost:8002</code> Triton Model metrics server (prometheus) <code>Redis</code> <code>redis://localhost:6379</code> Redis server, currently supports PUBSUB <code>Grafana</code> <code>http://localhost:3003</code> Grafana server for charting &amp; graphing"},{"location":"#visionai-web-app","title":"VisionAI web app","text":"<ul> <li>VisionAI also supports a web-based option for managing cameras, scenarios and pipeline. You can run the following command to start the web-based GUI. Once the web-based GUI is started, you can access it at http://localhost:3001.</li> </ul> <pre><code>$ visionai web start\n\nWeb service API available at: http://localhost:3002\nWeb app available at: http://localhost:3001\n</code></pre> <ul> <li>This would show an initial screen similar to this:</li> </ul> <p>.</p> <ul> <li> <p>You can manage cameras, scenarios, pipelines, see events etc., directly on the web-app. The web-app is running your own local compute instance. All the data is saved in your machine, and it is persistent as long as VisionAI application is not uninstalled.</p> </li> <li> <p>VisionAI supports out-of-box integration with Redis, Prometheus, Grafana and Azure Event Hub. Once the web-app is started, you can view the Grafana dashboard at: http://localhost:3003. The default username and password is <code>admin</code>/<code>admin</code>.</p> </li> </ul> <pre><code>Grafana server is at: http://localhost:3003\nRedis server is at: redis://localhost:6379\n</code></pre>"},{"location":"#list-available-scenarios","title":"List available Scenarios","text":"<p>VisionAI is organized in terms of scenarios. Consider each scenario as being a business use-case, that is solved by a combination of Machine Learning models and an inference algorithm. For example Warn me when max occupancy of this area exceeds 80 people is a business scenario, where as the People detection is an ML model.</p> <p>VisionAI supports 60 scenarios currently and more are being added continuously. Our current focus is on Workplace Safety scenarios. Please contact us if a scenario you need is not present in our repo and we will look into it.</p> <ul> <li>To list down available scenarios by running the following command.</li> </ul> <pre><code>$ visionai scenarios list\n</code></pre>"},{"location":"#get-details-for-a-scenario","title":"Get details for a Scenario","text":"<p>You can get details about a scenario using <code>visionai scenario details</code> command. Specify the scenario you want additional details for. The details of a scenario include the dataset size, model accuracy metrics.</p> <pre><code>$ visionai scenario --name early-smoke-and-fire-detection details\n\n------------------------------------------------\nCategory: Fire safety\nScenario: early-smoke-and-fire-detection\nThis scenario has been trained on open-source datasets consisting of 126,293 images. The datasets images are primarily outdoors (70%), but do contain a good number of indoor images (30%). There is a ~50-50% mix of day vs night images. You can find more details about this scenario at visionify.ai/early-smoke-and-fire-detection.\nModel: smoke-and-fire-detection-1.0.1.pt\nModel size: 127MB\nModel type: Object Detection\nFramework: PyTorch\nModel performance:\nDataset size: 126,293 images\nAccuracy: 94.1%\nRecall: 93%\nF1-Score: 93.5%\nEvents:\nsmoke-detected  | Immediate\nfire-detected   | Immediate\nEvent examples:\n{\n    \"scenario\": \"smoke-and-fire-detection\",\n    \"event_name\": \"smoke-detected\",\n    \"event_details\": {\n        \"camera\": \"camera-01\",\n        \"date\": \"2023-01-04 11:05:02\",\n        \"confidence\": 0.92\n    }\n}\n------------------------------------------------\n</code></pre>"},{"location":"#run-a-scenario","title":"Run a Scenario","text":"<p>Use <code>visionai scenario test</code> command to run a scenario. In its simplest sense, you can run a single scenario on your web-cam. In a more complex use-case, you can specify a pipeline of scenarios, configure notification logic for each scenario, timings for each scenario etc.</p> <p></p> <ul> <li>Run a scenario by running the following command. This would run the scenario on your local web-cam.</li> </ul> <pre><code>$ visionai scenarios test ppe-detection\n</code></pre> <ul> <li> <p>You can observe the command prompt for the output of the scenario. This scenario generates events when a person is detected without a PPE.</p> </li> <li> <p>You can also run this scenario on IP camera (RTSP, RTMP, RTP, HLS etc). For example:</p> </li> </ul> <pre><code>$ visionai scenarios test ppe-detection --camera rtsp://192.168.0.1:554/1\n</code></pre> <ul> <li>You can also run this scenario on a video file. For example:</li> </ul> <pre><code>$ visionai scenarios test ppe-detection --video /path/to/video.mp4\n</code></pre> <ul> <li>You can also create a pipeline to run multiple scenarios on a single camera. For example:</li> </ul> <pre><code>$ visionai camera add --name OFFICE-01 --url rtsp://192.186.0.1:554/1\n$ visionai pipeline create --name test-pipeline --camera OFFICE-01\n$ visionai pipeline add-scenario --name test-pipeline --scenario ppe-detection\n$ visionai pipeline add-scenario --name test-pipeline --scenario face-blur\n$ visionai pipeline add-scenario --name test-pipeline --scenario smoke-and-fire-detection\n$ visionai pipeline start --name test-pipeline\n</code></pre>"},{"location":"#deploy-to-azure","title":"Deploy to Azure","text":"<p>Deploy a fully configured and tested solution directly from Azure Marketplace. VisionAI runs computer vision models, most of which run orders of magnitude faster if executed on a GPU machine. Our Azure Marketplace offer VisionAI Community Edition is available through Azure Marketplace here (TODO). The community edition deploys a fully configured Virtual Machine with the recommended hardware and software options.</p> <p></p> <ul> <li>TODO: Point to ARM template that needs to be deployed (using these instructions and here is an example JSON file).</li> </ul>"},{"location":"#models","title":"Models","text":"<p>To support the running various scenarios - VisionAI relies a set of Machine Learning models that have been specifically trained with Industrial use-cases datasets. These models must be served through NVIDIA triton framework. VisionAI makes serving these models easy through a single command-line interface:</p> <pre><code>$ visionai models serve\n</code></pre> <p>Any time a new scenario is downloaded, the model server is automatically restarted to load and serve the new model. You can check the status of models being served by VisionAI through the following commands.</p> <p><pre><code>$ visionai models list\n</code></pre> </p> <p>Don't think you'll need to shut down the model server. However, if you do, you can do so through the following command.</p> <pre><code>$ visionai models stop\n</code></pre>"},{"location":"#events","title":"Events","text":"<p>VisionAI supports a variety of events that can be used to trigger actions. Our primary mode of events is through PubSub mechanism. VisionAI supports redis pubsub, and Azure Event Hub for posting events. These can be later extended to support emails alerts, SMS alerts, and other mechanisms.</p> <p>Each event is in the form of a JSON object. The following is an example of an event that is posted when a smoke is detected by the smoke-and-fire-detection scenario.</p> <pre><code>{\n\"camera\": \"camera-01\",\n\"scenario\": \"smoke-and-fire-detection\",\n\"event_name\": \"smoke-detected\",\n\"event_details\": {\n\"camera\": \"camera-01\",\n\"date\": \"2023-01-04 11:05:02\",\n\"confidence\": 0.92\n}\n}\n</code></pre> <p>To listen to events, you can subscribe to the redis pubsub mechanism as follows:</p> <pre><code>import redis\nr = redis.Redis(host='localhost', port=6379, db=0)\np = r.pubsub()\np.subscribe('visionai')\nfor message in p.listen():\nprint(message)\n</code></pre>"},{"location":"#get-help-on-commands","title":"Get help on commands","text":"<p>You can get more help on any command by adding --help at the end of the command. For example, if you want to get details about pipeline commands, you can run the following commands.</p> <pre><code>$ visionai pipeline --help\n\n Usage: visionai pipeline [OPTIONS] COMMAND [ARGS]...\n Manage pipelines\n Pipeline is a sequence of preprocess routines and\n scenarios to be run on a given set of cameras. Each\n pipeline can be configured to run specific scenarios -\n each scenario with their own customizations for event\n notifications. This module provides robust methods for\n managing pipelines, showing their details, adding/remove\n cameras from pipelines and running a pipeline.\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --help          Show this message and exit.              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Commands \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 add-camera      Add a camera to a pipeline               \u2502\n\u2502 add-preprocess  Add a preprocess routine to a pipeline   \u2502\n\u2502 add-scenario    Add a scenario to a pipeline             \u2502\n\u2502 create          Create a named pipeline                  \u2502\n\u2502 remove-camera   Remove a camera from a pipeline          \u2502\n\u2502 reset           Reset the pipeline to original state.    \u2502\n\u2502 run             Run a pipeline of scenarios on given     \u2502\n\u2502                 cameras                                  \u2502\n\u2502 show            Show details of a pipeline               \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"#next-steps","title":"Next steps","text":"<p>Congratulations! You have successfully configured and used VisionAI toolkit. Now go through Tutorials to learn about how to run multiple scnearios, how to configure each scenario for the events you need, how to set up pipelines with multiple cameras and scenarios.</p> <p>Or you can also browse through our scenarios section to understand different use-cases that are supported currently. If you have a need for a scenario, do not hesitate to submit a request here.</p>"},{"location":"#contributing","title":"Contributing","text":"<p>We welcome contributions to VisionAI. Please read our contribution guidelines to learn about how you can contribute to VisionAI.</p>"},{"location":"#license","title":"License","text":"<p>VIsionAI is licensed under the GPLv3 License. If you need a commercial license, please contact us.</p>"},{"location":"roadmap/","title":"Roadmap","text":""},{"location":"roadmap/#scenarios-roadmap","title":"Scenarios Roadmap","text":"<ol> <li>Hazard Warning<ol> <li> Smoke and Fire Detection</li> <li> No smoking/no vaping</li> <li> Spills &amp; leaks detection</li> <li> Gask leak detection</li> <li> Missing fire extinguisher</li> <li> Blocked exit monitoring</li> <li> Equipment temperature monitoring</li> <li> Equipment rust and corrosion</li> </ol> </li> <li>Employee Health &amp; Safety<ol> <li> PPE Detection</li> <li> Working at heights</li> <li> Environment monitoring</li> <li> Slip, trip and fall detection</li> <li> Posture &amp; Ergonomics</li> <li> Empty pallets</li> <li> Spills &amp; Leaks detection (Liquids)</li> <li> Hand sanitizer/hand-wash</li> <li> Worker fatigue detection</li> <li> Worker skin tempreature monitoring</li> <li> Confined spaces monitoring</li> </ol> </li> <li>Company Compliance Policies<ol> <li> Max occupancy</li> <li> Restricted areas/times</li> <li> Dwell time</li> <li> Social distancing</li> <li> Station occupancy</li> <li> Occupancy metrics</li> <li> Authorized personnel</li> <li> Tailgating</li> <li> Perimeter control</li> <li> No food or drinks</li> <li> No phone, text, pictures</li> <li> No Smoking zones</li> <li> No children/visitors</li> <li> Waste Management</li> <li> Energy Conservation</li> <li> Restricted Areas</li> </ol> </li> <li>Equipment Monitoring<ol> <li> Equipment temperature monitoring</li> <li> Equipment rust and corrosion</li> <li> Equipment vibration monitoring</li> <li> Equipment noise monitoring</li> <li> Read analog dials</li> <li> Tools check-in/out</li> <li> Spill &amp; leak</li> </ol> </li> <li>Environment Monitoring<ol> <li> Temperature monitoring</li> <li> Humidity monitoring</li> <li> Air quality monitoring</li> <li> Noise monitoring</li> <li> Pressure monitoring</li> <li> Water leak detection</li> <li> Volatile organic compounds (VOCs)</li> <li> Carbon monoxide (CO)</li> <li> Ambient light</li> <li> Dust monitoring</li> <li> Water quality monitoring</li> <li> Energy usage monitoring</li> <li> Waste management</li> <li> Water usage monitoring</li> <li> Water level monitoring</li> <li> Radiation monitoring</li> </ol> </li> <li>Suspicious Activity Detection<ol> <li> Loitering</li> <li> Unattended packages</li> <li> Aggressive behavior</li> <li> Vandalism &amp; property destruction</li> <li> Firearms &amp; knives</li> <li> Sexual harassments</li> <li> Solicitation</li> <li> Theft</li> <li> Shipping activity</li> <li> Intrusion detection</li> </ol> </li> <li>Vehicle Activity Detection<ol> <li> Vehicle usage</li> <li> Vehicle policies</li> <li> Forklift zone breach</li> <li> Vehicle license plate</li> <li> Vehicle speed</li> <li> Vehicle cargo</li> </ol> </li> <li>Privacy<ol> <li> Blur faces</li> <li> Blur signs/text</li> <li> Blur screens</li> <li> Blur license plates</li> <li> Obstructed camera view</li> </ol> </li> </ol>"},{"location":"todo/","title":"Todo","text":"<ul> <li>Figure out how to integrate API/web-app into same codebase. (or through Docker somehow).</li> <li>Move edgeApi to fastapi, enable documentation.</li> <li>Move edge-inference scripts here.</li> <li>Add inference dependencies to the package.</li> <li>Support for pipelines: TODO</li> <li>Add support for docsqa.jina.ai after documentation is complete. This would provide an automated bot for the end-user.</li> <li>Work through these items for the PyPi package improvement (link): <pre><code>Basic info present?         0\nSource repository present?  0\nReadme present?             0\nLicense present?            1\nHas multiple versions?      1\nFollows SemVer?             1\nRecent release?             1\nNot brand new?              0\n1.0.0 or greater?           0\nDependent Packages          0\nDependent Repositories      0\nStars                       0\nContributors                0\nLibraries.io subscribers    0\n</code></pre></li> </ul>"},{"location":"company/about/","title":"VisionAI","text":"<p>Documentation for VisionAI toolkit.</p>"},{"location":"company/about/#overview","title":"Overview","text":"<p>VisionAI provides a set of command line utilities for you to manage different Vision AI scenarios that have been pre-developed and pre-tested. VisionAI focuses on workplace health and safety models - and majority of the models you see here have been developed with that in mind.</p> <p>These are production-ready model trained from open-source and academic datasets. We are continuously working on new scenarios - and our current scenario repo consists of over 60 scenarios that are listed here. They are developed with the intent of being easy-to-use for business. The framework also supports a whole bunch of custom scenarios.</p>"},{"location":"company/about/#install-visionai","title":"Install VisionAI","text":"<p>Install VisionAI application through <code>PyPI</code>. There are other options available for install - including a Docker container option. These are detailed in installation section.</p> <pre><code>$ pip install visionai\n---&gt; 100%\nSuccessfully installed visionai\n\u2728 You are all set to use visionai toolkit \u2728\n</code></pre>"},{"location":"company/about/#deploy-to-azure","title":"Deploy to Azure","text":"<p>Deploy a fully configured and tested solution directly from Azure Marketplace. VisionAI runs computer vision models, most of which run orders of magnitude faster if executed on a GPU machine. Our Azure Marketplace offer VisionAI Community Edition is available through Azure Marketplace here (TODO). The community edition deploys a fully configured Virtual Machine with the recommended hardware and software options. Get more details here.</p> <p></p> <ul> <li>TODO: Point to ARM template that needs to be deployed (using these instructions and here is an example JSON file).</li> </ul>"},{"location":"company/about/#list-available-scenarios","title":"List available Scenarios","text":"<p>VisionAI is organized in terms of scenarios. Consider each scenario as being a business use-case, that is solved by a combination of Machine Learning models and an inference algorithm. For example Warn me when max occupancy of this area exceeds 80 people is a business scenario, where as the People detection is an ML model.</p> <p>VisionAI supports 60 scenarios currently and more are being added continuously. Our current focus is on Workplace Safety scenarios. Please contact us if a scenario you need is not present in our repo and we will look into it.</p> <pre><code>$ visionai scenarios list\n\n------------------------------------------------\nPrivacy Suite\nblur-faces\nblur-text\nFire safety\nearly-smoke-and-fire-detection\nsmoking-and-vaping-detection\nPersonnel safety\nppe-detection\npfas-system-detection\nrailings-detection\nSuspicious activity\nshipping-activity-detection\nagressive-behaivior\nCompliance Policies\nmax-occupancy\nEquipment\nrust-and-corrosion-detection\nIR Camera\ntemperature-monitoring\n------------------------------------------------\n\u2728 More scenarios are added regularly \u2728\n</code></pre>"},{"location":"company/about/#get-details-for-a-scenario","title":"Get details for a Scenario","text":"<p>You can get details about a scenario using <code>visionai scenario details</code> command. Specify the scenario you want additional details for. The details of a scenario include the dataset size, model accuracy metrics,</p> <pre><code>$ visionai scenario --name early-smoke-and-fire-detection details\n\n------------------------------------------------\nCategory: Fire safety\nScenario: early-smoke-and-fire-detection\nThis scenario has been trained on open-source datasets consisting of 126,293 images. The datasets images are primarily outdoors (70%), but do contain a good number of indoor images (30%). There is a ~50-50% mix of day vs night images. You can find more details about this scenario at visionify.ai/early-smoke-and-fire-detection.\nModel: smoke-and-fire-detection-1.0.1.pt\nModel size: 127MB\nModel type: Object Detection\nFramework: PyTorch\nModel performance:\nDataset size: 126,293 images\nAccuracy: 94.1%\nRecall: 93%\nF1-Score: 93.5%\nEvents:\nsmoke-detected  | Immediate\nfire-detected   | Immediate\nEvent examples:\n{\n    \"scenario\": \"smoke-and-fire-detection\",\n    \"event_name\": \"smoke-detected\",\n    \"event_details\": {\n        \"camera\": \"camera-01\",\n        \"date\": \"2023-01-04 11:05:02\",\n        \"confidence\": 0.92\n    }\n}\n------------------------------------------------\n</code></pre>"},{"location":"company/about/#run-a-scenario","title":"Run a Scenario","text":"<p>Use <code>visionai run</code> command to run a scenario. In its simplest sense, you can run a single scenario on your web-cam. In a more complex use-case, you can specify a pipeline of scenarios, configure notification logic for each scenario, timings for each scenario etc.</p> <pre><code>$ visionai run --scenario early-smoke-and-fire-detection --camera OFFICE-01\n\nStarting early-smoke-and-fire-detection\n...\n</code></pre>"},{"location":"company/about/#get-help-on-commands","title":"Get help on commands","text":"<p>You can get more help on any command by adding --help at the end of the command. For example, if you want to get details about pipeline commands, you can run the following commands.</p> <pre><code>$ visionai pipeline --help\n\n Usage: visionai pipeline [OPTIONS] COMMAND [ARGS]...\n Manage pipelines\n Pipeline is a sequence of preprocess routines and\n scenarios to be run on a given set of cameras. Each\n pipeline can be configured to run specific scenarios -\n each scenario with their own customizations for event\n notifications. This module provides robust methods for\n managing pipelines, showing their details, adding/remove\n cameras from pipelines and running a pipeline.\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --help          Show this message and exit.              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Commands \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 add-camera      Add a camera to a pipeline               \u2502\n\u2502 add-preprocess  Add a preprocess routine to a pipeline   \u2502\n\u2502 add-scenario    Add a scenario to a pipeline             \u2502\n\u2502 create          Create a named pipeline                  \u2502\n\u2502 remove-camera   Remove a camera from a pipeline          \u2502\n\u2502 reset           Reset the pipeline to original state.    \u2502\n\u2502 run             Run a pipeline of scenarios on given     \u2502\n\u2502                 cameras                                  \u2502\n\u2502 show            Show details of a pipeline               \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n$ visionai pipeline add-scenario --help\n\n Usage: visionai pipeline add-scenario [OPTIONS]\n Add a scenario to a pipeline\n The order of the scenarios does not matter. All added\n scenarios are run in different threads. All scenarios are\n run after pre-processing stage is done.\n ``` Ex: visionai pipeline --name test_pipe add-scenario\n --name smoke-and-fire visionai pipeline --name test_pipe\n add-scenario --name ppe-detection visionai pipeline --name\n test_pipe run ```\n @arg pipeline - specify a named pipeline @arg scenario -\n specify name of the scenario to run\n @return None\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *  --pipeline        TEXT  pipeline name [default: None] \u2502\n\u2502                            [required]                    \u2502\n\u2502 *  --scenario        TEXT  scenario to add               \u2502\n\u2502                            [default: None]               \u2502\n\u2502                            [required]                    \u2502\n\u2502    --help                  Show this message and exit.   \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"company/about/#next-steps","title":"Next steps","text":"<p>Congratulations! You have successfully run the first scenario. Now go through Tutorials to learn about how to run multiple scnearios, how to configure each scenario for the events you need, how to set up the dependencies etc.</p> <p>Or you can also go through our scenarios page to explore the different scenarios available and their model details. If you have a need for a scenario to be implemented, do not hesitate to submit a request.</p>"},{"location":"company/careers/","title":"VisionAI","text":"<p>Documentation for VisionAI toolkit.</p>"},{"location":"company/careers/#overview","title":"Overview","text":"<p>VisionAI provides a set of command line utilities for you to manage different Vision AI scenarios that have been pre-developed and pre-tested. VisionAI focuses on workplace health and safety models - and majority of the models you see here have been developed with that in mind.</p> <p>These are production-ready model trained from open-source and academic datasets. We are continuously working on new scenarios - and our current scenario repo consists of over 60 scenarios that are listed here. They are developed with the intent of being easy-to-use for business. The framework also supports a whole bunch of custom scenarios.</p>"},{"location":"company/careers/#install-visionai","title":"Install VisionAI","text":"<p>Install VisionAI application through <code>PyPI</code>. There are other options available for install - including a Docker container option. These are detailed in installation section.</p> <pre><code>$ pip install visionai\n---&gt; 100%\nSuccessfully installed visionai\n\u2728 You are all set to use visionai toolkit \u2728\n</code></pre>"},{"location":"company/careers/#deploy-to-azure","title":"Deploy to Azure","text":"<p>Deploy a fully configured and tested solution directly from Azure Marketplace. VisionAI runs computer vision models, most of which run orders of magnitude faster if executed on a GPU machine. Our Azure Marketplace offer VisionAI Community Edition is available through Azure Marketplace here (TODO). The community edition deploys a fully configured Virtual Machine with the recommended hardware and software options. Get more details here.</p> <p></p> <ul> <li>TODO: Point to ARM template that needs to be deployed (using these instructions and here is an example JSON file).</li> </ul>"},{"location":"company/careers/#list-available-scenarios","title":"List available Scenarios","text":"<p>VisionAI is organized in terms of scenarios. Consider each scenario as being a business use-case, that is solved by a combination of Machine Learning models and an inference algorithm. For example Warn me when max occupancy of this area exceeds 80 people is a business scenario, where as the People detection is an ML model.</p> <p>VisionAI supports 60 scenarios currently and more are being added continuously. Our current focus is on Workplace Safety scenarios. Please contact us if a scenario you need is not present in our repo and we will look into it.</p> <pre><code>$ visionai scenarios list\n\n------------------------------------------------\nPrivacy Suite\nblur-faces\nblur-text\nFire safety\nearly-smoke-and-fire-detection\nsmoking-and-vaping-detection\nPersonnel safety\nppe-detection\npfas-system-detection\nrailings-detection\nSuspicious activity\nshipping-activity-detection\nagressive-behaivior\nCompliance Policies\nmax-occupancy\nEquipment\nrust-and-corrosion-detection\nIR Camera\ntemperature-monitoring\n------------------------------------------------\n\u2728 More scenarios are added regularly \u2728\n</code></pre>"},{"location":"company/careers/#get-details-for-a-scenario","title":"Get details for a Scenario","text":"<p>You can get details about a scenario using <code>visionai scenario details</code> command. Specify the scenario you want additional details for. The details of a scenario include the dataset size, model accuracy metrics,</p> <pre><code>$ visionai scenario --name early-smoke-and-fire-detection details\n\n------------------------------------------------\nCategory: Fire safety\nScenario: early-smoke-and-fire-detection\nThis scenario has been trained on open-source datasets consisting of 126,293 images. The datasets images are primarily outdoors (70%), but do contain a good number of indoor images (30%). There is a ~50-50% mix of day vs night images. You can find more details about this scenario at visionify.ai/early-smoke-and-fire-detection.\nModel: smoke-and-fire-detection-1.0.1.pt\nModel size: 127MB\nModel type: Object Detection\nFramework: PyTorch\nModel performance:\nDataset size: 126,293 images\nAccuracy: 94.1%\nRecall: 93%\nF1-Score: 93.5%\nEvents:\nsmoke-detected  | Immediate\nfire-detected   | Immediate\nEvent examples:\n{\n    \"scenario\": \"smoke-and-fire-detection\",\n    \"event_name\": \"smoke-detected\",\n    \"event_details\": {\n        \"camera\": \"camera-01\",\n        \"date\": \"2023-01-04 11:05:02\",\n        \"confidence\": 0.92\n    }\n}\n------------------------------------------------\n</code></pre>"},{"location":"company/careers/#run-a-scenario","title":"Run a Scenario","text":"<p>Use <code>visionai run</code> command to run a scenario. In its simplest sense, you can run a single scenario on your web-cam. In a more complex use-case, you can specify a pipeline of scenarios, configure notification logic for each scenario, timings for each scenario etc.</p> <pre><code>$ visionai run --scenario early-smoke-and-fire-detection --camera OFFICE-01\n\nStarting early-smoke-and-fire-detection\n...\n</code></pre>"},{"location":"company/careers/#get-help-on-commands","title":"Get help on commands","text":"<p>You can get more help on any command by adding --help at the end of the command. For example, if you want to get details about pipeline commands, you can run the following commands.</p> <pre><code>$ visionai pipeline --help\n\n Usage: visionai pipeline [OPTIONS] COMMAND [ARGS]...\n Manage pipelines\n Pipeline is a sequence of preprocess routines and\n scenarios to be run on a given set of cameras. Each\n pipeline can be configured to run specific scenarios -\n each scenario with their own customizations for event\n notifications. This module provides robust methods for\n managing pipelines, showing their details, adding/remove\n cameras from pipelines and running a pipeline.\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --help          Show this message and exit.              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Commands \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 add-camera      Add a camera to a pipeline               \u2502\n\u2502 add-preprocess  Add a preprocess routine to a pipeline   \u2502\n\u2502 add-scenario    Add a scenario to a pipeline             \u2502\n\u2502 create          Create a named pipeline                  \u2502\n\u2502 remove-camera   Remove a camera from a pipeline          \u2502\n\u2502 reset           Reset the pipeline to original state.    \u2502\n\u2502 run             Run a pipeline of scenarios on given     \u2502\n\u2502                 cameras                                  \u2502\n\u2502 show            Show details of a pipeline               \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n$ visionai pipeline add-scenario --help\n\n Usage: visionai pipeline add-scenario [OPTIONS]\n Add a scenario to a pipeline\n The order of the scenarios does not matter. All added\n scenarios are run in different threads. All scenarios are\n run after pre-processing stage is done.\n ``` Ex: visionai pipeline --name test_pipe add-scenario\n --name smoke-and-fire visionai pipeline --name test_pipe\n add-scenario --name ppe-detection visionai pipeline --name\n test_pipe run ```\n @arg pipeline - specify a named pipeline @arg scenario -\n specify name of the scenario to run\n @return None\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *  --pipeline        TEXT  pipeline name [default: None] \u2502\n\u2502                            [required]                    \u2502\n\u2502 *  --scenario        TEXT  scenario to add               \u2502\n\u2502                            [default: None]               \u2502\n\u2502                            [required]                    \u2502\n\u2502    --help                  Show this message and exit.   \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"company/careers/#next-steps","title":"Next steps","text":"<p>Congratulations! You have successfully run the first scenario. Now go through Tutorials to learn about how to run multiple scnearios, how to configure each scenario for the events you need, how to set up the dependencies etc.</p> <p>Or you can also go through our scenarios page to explore the different scenarios available and their model details. If you have a need for a scenario to be implemented, do not hesitate to submit a request.</p>"},{"location":"company/contact/","title":"VisionAI Contact Information","text":"<p>You can contact us at:</p> <p>For general queries: info@visionify.ai</p> <p>For queries related to sales or pricing plans: sales@visionify.ai</p> <p>For queries related to scenarios or models or customization: development@visionify.ai</p> <p>For queries related to VisionAI toolkit usage: support@visionify.ai</p> <p>Note: Please put VisionAI in the subject!</p> <p>More information is available at: https://visionify.ai/</p> <p>Our github contains the source for all of our scenarios: https://github.com/visionify/visionai</p> <p>If you want to set up a meeting with us visit https://www.visionify.ai/contact-us and follow the instructions for scheduling a call.</p>"},{"location":"company/privacy-policy/","title":"Privacy Policy","text":"<p>Last updated: February 20, 2023</p> <p>This Privacy Policy describes Our policies and procedures on the collection, use and disclosure of Your information when You use the Service and tells You about Your privacy rights and how the law protects You.</p> <p>We use Your Personal data to provide and improve the Service. By using the Service, You agree to the collection and use of information in accordance with this Privacy Policy. This Privacy Policy has been created with the help of the TermsFeed Privacy Policy Generator.</p>"},{"location":"company/privacy-policy/#interpretation-and-definitions","title":"Interpretation and Definitions","text":""},{"location":"company/privacy-policy/#interpretation","title":"Interpretation","text":"<p>The words of which the initial letter is capitalized have meanings defined under the following conditions. The following definitions shall have the same meaning regardless of whether they appear in singular or in plural.</p>"},{"location":"company/privacy-policy/#definitions","title":"Definitions","text":"<p>For the purposes of this Privacy Policy:</p> <ul> <li>Account means a unique account created for You to access our Service or parts of our Service.</li> <li> <p>Affiliate means an entity that controls, is controlled by or is under common control with a party, where \"control\" means ownership of 50% or more of the shares, equity interest or other securities entitled to vote for election of directors or other managing authority.</p> </li> <li> <p>Company (referred to as either \"the Company\", \"We\", \"Us\" or \"Our\" in this Agreement) refers to DestinationJ Software Technologies LLC, 1499 W 120th Ave, Ste 110, Westminster CO 80234.</p> </li> <li> <p>Cookies are small files that are placed on Your computer, mobile device or any other device by a website, containing the details of Your browsing history on that website among its many uses.</p> </li> <li> <p>Country refers to: Colorado,  United States</p> </li> <li> <p>Device means any device that can access the Service such as a computer, a cellphone or a digital tablet.</p> </li> <li> <p>Personal Data is any information that relates to an identified or identifiable individual.</p> </li> <li> <p>Service refers to the Website.</p> </li> <li> <p>Service Provider means any natural or legal person who processes the data on behalf of the Company. It refers to third-party companies or individuals employed by the Company to facilitate the Service, to provide the Service on behalf of the Company, to perform services related to the Service or to assist the Company in analyzing how the Service is used.</p> </li> <li> <p>Usage Data refers to data collected automatically, either generated by the use of the Service or from the Service infrastructure itself (for example, the duration of a page visit).</p> </li> <li> <p>Website refers to Visionify: Workplace Safety through Vision AI, accessible from https://www.visionify.ai</p> </li> <li> <p>You means the individual accessing or using the Service, or the company, or other legal entity on behalf of which such individual is accessing or using the Service, as applicable.</p> </li> </ul>"},{"location":"company/privacy-policy/#collecting-and-using-your-personal-data","title":"Collecting and Using Your Personal Data","text":""},{"location":"company/privacy-policy/#types-of-data-collected","title":"Types of Data Collected","text":""},{"location":"company/privacy-policy/#personal-data","title":"Personal Data","text":"<p>While using Our Service, We may ask You to provide Us with certain personally identifiable information that can be used to contact or identify You. Personally identifiable information may include, but is not limited to:</p> <ul> <li>Email address</li> <li>First name and last name</li> <li>Phone number</li> <li> <p>Address, State, Province, ZIP/Postal code, City</p> </li> <li> <p>Usage Data</p> </li> </ul>"},{"location":"company/privacy-policy/#usage-data","title":"Usage Data","text":"<p>Usage Data is collected automatically when using the Service.</p> <p>Usage Data may include information such as Your Device's Internet Protocol address (e.g. IP address), browser type, browser version, the pages of our Service that You visit, the time and date of Your visit, the time spent on those pages, unique device identifiers and other diagnostic data.</p> <p>When You access the Service by or through a mobile device, We may collect certain information automatically, including, but not limited to, the type of mobile device You use, Your mobile device unique ID, the IP address of Your mobile device, Your mobile operating system, the type of mobile Internet browser You use, unique device identifiers and other diagnostic data.</p> <p>We may also collect information that Your browser sends whenever You visit our Service or when You access the Service by or through a mobile device.</p>"},{"location":"company/privacy-policy/#tracking-technologies-and-cookies","title":"Tracking Technologies and Cookies","text":"<p>We use Cookies and similar tracking technologies to track the activity on Our Service and store certain information. Tracking technologies used are beacons, tags, and scripts to collect and track information and to improve and analyze Our Service. The technologies We use may include:</p> <ul> <li>Cookies or Browser Cookies. A cookie is a small file placed on Your Device. You can instruct Your browser to refuse all Cookies or to indicate when a Cookie is being sent. However, if You do not accept Cookies, You may not be able to use some parts of our Service. Unless you have adjusted Your browser setting so that it will refuse Cookies, our Service may use Cookies.</li> <li>Web Beacons. Certain sections of our Service and our emails may contain small electronic files known as web beacons (also referred to as clear gifs, pixel tags, and single-pixel gifs) that permit the Company, for example, to count users who have visited those pages or opened an email and for other related website statistics (for example, recording the popularity of a certain section and verifying system and server integrity).</li> </ul> <p>Cookies can be \"Persistent\" or \"Session\" Cookies. Persistent Cookies remain on Your personal computer or mobile device when You go offline, while Session Cookies are deleted as soon as You close Your web browser. You can learn more about cookies on TermsFeed website article.</p> <p>We use both Session and Persistent Cookies for the purposes set out below:</p> <ul> <li> <p>Necessary / Essential Cookies</p> <p>Type: Session Cookies</p> <p>Administered by: Us</p> <p>Purpose: These Cookies are essential to provide You with services available through the Website and to enable You to use some of its features. They help to authenticate users and prevent fraudulent use of user accounts. Without these Cookies, the services that You have asked for cannot be provided, and We only use these Cookies to provide You with those services. - Cookies Policy / Notice Acceptance Cookies</p> <p>Type: Persistent Cookies</p> <p>Administered by: Us</p> <p>Purpose: These Cookies identify if users have accepted the use of cookies on the Website. - Functionality Cookies</p> <p>Type: Persistent Cookies</p> <p>Administered by: Us</p> <p>Purpose: These Cookies allow us to remember choices You make when You use the Website, such as remembering your login details or language preference. The purpose of these Cookies is to provide You with a more personal experience and to avoid You having to re-enter your preferences every time You use the Website.</p> </li> </ul> <p>For more information about the cookies we use and your choices regarding cookies, please visit our Cookies Policy or the Cookies section of our Privacy Policy.</p>"},{"location":"company/privacy-policy/#use-of-your-personal-data","title":"Use of Your Personal Data","text":"<p>The Company may use Personal Data for the following purposes:</p> <ul> <li>To provide and maintain our Service, including to monitor the usage of our Service.</li> <li>To manage Your Account: to manage Your registration as a user of the Service. The Personal Data You provide can give You access to different functionalities of the Service that are available to You as a registered user.</li> <li>For the performance of a contract: the development, compliance and undertaking of the purchase contract for the products, items or services You have purchased or of any other contract with Us through the Service.</li> <li>To contact You: To contact You by email, telephone calls, SMS, or other equivalent forms of electronic communication, such as a mobile application's push notifications regarding updates or informative communications related to the functionalities, products or contracted services, including the security updates, when necessary or reasonable for their implementation.</li> <li>To provide You with news, special offers and general information about other goods, services and events which we offer that are similar to those that you have already purchased or enquired about unless You have opted not to receive such information.</li> <li> <p>To manage Your requests: To attend and manage Your requests to Us.</p> </li> <li> <p>For business transfers: We may use Your information to evaluate or conduct a merger, divestiture, restructuring, reorganization, dissolution, or other sale or transfer of some or all of Our assets, whether as a going concern or as part of bankruptcy, liquidation, or similar proceeding, in which Personal Data held by Us about our Service users is among the assets transferred.</p> </li> <li>For other purposes: We may use Your information for other purposes, such as data analysis, identifying usage trends, determining the effectiveness of our promotional campaigns and to evaluate and improve our Service, products, services, marketing and your experience. </li> </ul> <p>We may share Your personal information in the following situations:</p> <ul> <li>With Service Providers: We may share Your personal information with Service Providers to monitor and analyze the use of our Service,  to contact You.</li> <li>For business transfers: We may share or transfer Your personal information in connection with, or during negotiations of, any merger, sale of Company assets, financing, or acquisition of all or a portion of Our business to another company.</li> <li>With Affiliates: We may share Your information with Our affiliates, in which case we will require those affiliates to honor this Privacy Policy. Affiliates include Our parent company and any other subsidiaries, joint venture partners or other companies that We control or that are under common control with Us.</li> <li>With business partners: We may share Your information with Our business partners to offer You certain products, services or promotions.</li> <li>With other users: when You share personal information or otherwise interact in the public areas with other users, such information may be viewed by all users and may be publicly distributed outside. </li> <li>With Your consent: We may disclose Your personal information for any other purpose with Your consent.</li> </ul>"},{"location":"company/privacy-policy/#retention-of-your-personal-data","title":"Retention of Your Personal Data","text":"<p>The Company will retain Your Personal Data only for as long as is necessary for the purposes set out in this Privacy Policy. We will retain and use Your Personal Data to the extent necessary to comply with our legal obligations (for example, if we are required to retain your data to comply with applicable laws), resolve disputes, and enforce our legal agreements and policies.</p> <p>The Company will also retain Usage Data for internal analysis purposes. Usage Data is generally retained for a shorter period of time, except when this data is used to strengthen the security or to improve the functionality of Our Service, or We are legally obligated to retain this data for longer time periods.</p>"},{"location":"company/privacy-policy/#transfer-of-your-personal-data","title":"Transfer of Your Personal Data","text":"<p>Your information, including Personal Data, is processed at the Company's operating offices and in any other places where the parties involved in the processing are located. It means that this information may be transferred to \u2014 and maintained on \u2014 computers located outside of Your state, province, country or other governmental jurisdiction where the data protection laws may differ than those from Your jurisdiction.</p> <p>Your consent to this Privacy Policy followed by Your submission of such information represents Your agreement to that transfer.</p> <p>The Company will take all steps reasonably necessary to ensure that Your data is treated securely and in accordance with this Privacy Policy and no transfer of Your Personal Data will take place to an organization or a country unless there are adequate controls in place including the security of Your data and other personal information.</p>"},{"location":"company/privacy-policy/#delete-your-personal-data","title":"Delete Your Personal Data","text":"<p>You have the right to delete or request that We assist in deleting the Personal Data that We have collected about You.</p> <p>Our Service may give You the ability to delete certain information about You from within the Service.</p> <p>You may update, amend, or delete Your information at any time by signing in to Your Account, if you have one, and visiting the account settings section that allows you to manage Your personal information. You may also contact Us to request access to, correct, or delete any personal information that You have provided to Us.</p> <p>Please note, however, that We may need to retain certain information when we have a legal obligation or lawful basis to do so.</p>"},{"location":"company/privacy-policy/#disclosure-of-your-personal-data","title":"Disclosure of Your Personal Data","text":""},{"location":"company/privacy-policy/#business-transactions","title":"Business Transactions","text":"<p>If the Company is involved in a merger, acquisition or asset sale, Your Personal Data may be transferred. We will provide notice before Your Personal Data is transferred and becomes subject to a different Privacy Policy.</p>"},{"location":"company/privacy-policy/#law-enforcement","title":"Law enforcement","text":"<p>Under certain circumstances, the Company may be required to disclose Your Personal Data if required to do so by law or in response to valid requests by public authorities (e.g. a court or a government agency).</p>"},{"location":"company/privacy-policy/#other-legal-requirements","title":"Other legal requirements","text":"<p>The Company may disclose Your Personal Data in the good faith belief that such action is necessary to:</p> <ul> <li>Comply with a legal obligation</li> <li>Protect and defend the rights or property of the Company</li> <li>Prevent or investigate possible wrongdoing in connection with the Service</li> <li>Protect the personal safety of Users of the Service or the public</li> <li>Protect against legal liability</li> </ul>"},{"location":"company/privacy-policy/#security-of-your-personal-data","title":"Security of Your Personal Data","text":"<p>The security of Your Personal Data is important to Us, but remember that no method of transmission over the Internet, or method of electronic storage is 100% secure. While We strive to use commercially acceptable means to protect Your Personal Data, We cannot guarantee its absolute security.</p>"},{"location":"company/privacy-policy/#childrens-privacy","title":"Children's Privacy","text":"<p>Our Service does not address anyone under the age of 13. We do not knowingly collect personally identifiable information from anyone under the age of 13. If You are a parent or guardian and You are aware that Your child has provided Us with Personal Data, please contact Us. If We become aware that We have collected Personal Data from anyone under the age of 13 without verification of parental consent, We take steps to remove that information from Our servers.</p> <p>If We need to rely on consent as a legal basis for processing Your information and Your country requires consent from a parent, We may require Your parent's consent before We collect and use that information.</p>"},{"location":"company/privacy-policy/#links-to-other-websites","title":"Links to Other Websites","text":"<p>Our Service may contain links to other websites that are not operated by Us. If You click on a third party link, You will be directed to that third party's site. We strongly advise You to review the Privacy Policy of every site You visit.</p> <p>We have no control over and assume no responsibility for the content, privacy policies or practices of any third party sites or services.</p>"},{"location":"company/privacy-policy/#changes-to-this-privacy-policy","title":"Changes to this Privacy Policy","text":"<p>We may update Our Privacy Policy from time to time. We will notify You of any changes by posting the new Privacy Policy on this page.</p> <p>We will let You know via email and/or a prominent notice on Our Service, prior to the change becoming effective and update the \"Last updated\" date at the top of this Privacy Policy.</p> <p>You are advised to review this Privacy Policy periodically for any changes. Changes to this Privacy Policy are effective when they are posted on this page.</p>"},{"location":"company/privacy-policy/#contact-us","title":"Contact Us","text":"<p>If you have any questions about this Privacy Policy, You can contact us:</p> <ul> <li> <p>By email: info@visionify.ai</p> </li> <li> <p>By visiting this page on our website: https://www.visionify.ai/contact-us</p> </li> </ul>"},{"location":"company/team/","title":"Changelog","text":""},{"location":"company/team/#visionai-changelog","title":"VisionAI Changelog","text":""},{"location":"company/team/#017-january-22-2023","title":"0.1.7 January 22, 2023","text":"<ul> <li>Implemented camera add/delete functionality</li> </ul>"},{"location":"company/team/#016-january-20-2023","title":"0.1.6 January 20, 2023","text":"<ul> <li>Implemented initial set of commands in different files (dummy implementation)</li> <li>Testing commands individually or through the main application</li> </ul>"},{"location":"company/team/#013-january-16-2023","title":"0.1.3 January 16, 2023","text":"<ul> <li>Basic overview and usage documentation is updated.</li> <li>Started using a termy JS script to show terminal animations nicely</li> </ul>"},{"location":"company/team/#012-january-14-20123","title":"0.1.2 January 14, 20123","text":"<ul> <li>Made MkDocs documents based on Typer format</li> <li>Registered CNAME to point to https://docs.visionify.ai</li> </ul>"},{"location":"company/team/#011-january-11-2023","title":"0.1.1 January 11, 2023","text":"<ul> <li>Updated Azure DevOps CI/CD to automatically publish package on each merge</li> <li>Initial set of commands for visionai application</li> <li>Made <code>visionai</code> as a callable CLI application through poetry</li> </ul>"},{"location":"company/team/#010-january-10-2023","title":"0.1.0 January 10, 2023","text":"<ul> <li>Initial release: <code>pip install visionai</code></li> <li>Pushed package to <code>PyPI</code> repository</li> </ul>"},{"location":"company/terms-and-conditions/","title":"Terms and Conditions","text":"<p>Last updated: February 20, 2023</p> <p>Please read these terms and conditions carefully before using Our Service.</p>"},{"location":"company/terms-and-conditions/#interpretation-and-definitions","title":"Interpretation and Definitions","text":""},{"location":"company/terms-and-conditions/#interpretation","title":"Interpretation","text":"<p>The words of which the initial letter is capitalized have meanings defined under the following conditions. The following definitions shall have the same meaning regardless of whether they appear in singular or in plural.</p>"},{"location":"company/terms-and-conditions/#definitions","title":"Definitions","text":"<p>For the purposes of these Terms and Conditions:</p> <ul> <li> <p>Affiliate means an entity that controls, is controlled by or is under common control with a party, where \"control\" means ownership of 50% or more of the shares, equity interest or other securities entitled to vote for election of directors or other managing authority.</p> </li> <li> <p>Country refers to: Colorado,  United States</p> </li> <li> <p>Company (referred to as either \"the Company\", \"We\", \"Us\" or \"Our\" in this Agreement) refers to DestinationJ Software Technologies LLC, 1499 W 120th Ave, Ste 110, Westminster CO 80234.</p> </li> <li> <p>Device means any device that can access the Service such as a computer, a cellphone or a digital tablet.</p> </li> <li> <p>Service refers to the Website.</p> </li> <li> <p>Terms and Conditions (also referred as \"Terms\") mean these Terms and Conditions that form the entire agreement between You and the Company regarding the use of the Service. This Terms and Conditions agreement has been created with the help of the TermsFeed Terms and Conditions Generator.</p> </li> <li>Third-party Social Media Service means any services or content (including data, information, products or services) provided by a third-party that may be displayed, included or made available by the Service.</li> <li>Website refers to Visionify: Workplace Safety through Vision AI, accessible from https://www.visionify.ai</li> <li>You means the individual accessing or using the Service, or the company, or other legal entity on behalf of which such individual is accessing or using the Service, as applicable.</li> </ul>"},{"location":"company/terms-and-conditions/#acknowledgment","title":"Acknowledgment","text":"<p>These are the Terms and Conditions governing the use of this Service and the agreement that operates between You and the Company. These Terms and Conditions set out the rights and obligations of all users regarding the use of the Service.</p> <p>Your access to and use of the Service is conditioned on Your acceptance of and compliance with these Terms and Conditions. These Terms and Conditions apply to all visitors, users and others who access or use the Service.</p> <p>By accessing or using the Service You agree to be bound by these Terms and Conditions. If You disagree with any part of these Terms and Conditions then You may not access the Service.</p> <p>You represent that you are over the age of 18. The Company does not permit those under 18 to use the Service.</p> <p>Your access to and use of the Service is also conditioned on Your acceptance of and compliance with the Privacy Policy of the Company. Our Privacy Policy describes Our policies and procedures on the collection, use and disclosure of Your personal information when You use the Application or the Website and tells You about Your privacy rights and how the law protects You. Please read Our Privacy Policy carefully before using Our Service.</p>"},{"location":"company/terms-and-conditions/#links-to-other-websites","title":"Links to Other Websites","text":"<p>Our Service may contain links to third-party web sites or services that are not owned or controlled by the Company.</p> <p>The Company has no control over, and assumes no responsibility for, the content, privacy policies, or practices of any third party web sites or services. You further acknowledge and agree that the Company shall not be responsible or liable, directly or indirectly, for any damage or loss caused or alleged to be caused by or in connection with the use of or reliance on any such content, goods or services available on or through any such web sites or services.</p> <p>We strongly advise You to read the terms and conditions and privacy policies of any third-party web sites or services that You visit.</p>"},{"location":"company/terms-and-conditions/#termination","title":"Termination","text":"<p>We may terminate or suspend Your access immediately, without prior notice or liability, for any reason whatsoever, including without limitation if You breach these Terms and Conditions.</p> <p>Upon termination, Your right to use the Service will cease immediately.</p>"},{"location":"company/terms-and-conditions/#limitation-of-liability","title":"Limitation of Liability","text":"<p>Notwithstanding any damages that You might incur, the entire liability of the Company and any of its suppliers under any provision of this Terms and Your exclusive remedy for all of the foregoing shall be limited to the amount actually paid by You through the Service or 100 USD if You haven't purchased anything through the Service.</p> <p>To the maximum extent permitted by applicable law, in no event shall the Company or its suppliers be liable for any special, incidental, indirect, or consequential damages whatsoever (including, but not limited to, damages for loss of profits, loss of data or other information, for business interruption, for personal injury, loss of privacy arising out of or in any way related to the use of or inability to use the Service, third-party software and/or third-party hardware used with the Service, or otherwise in connection with any provision of this Terms), even if the Company or any supplier has been advised of the possibility of such damages and even if the remedy fails of its essential purpose.</p> <p>Some states do not allow the exclusion of implied warranties or limitation of liability for incidental or consequential damages, which means that some of the above limitations may not apply. In these states, each party's liability will be limited to the greatest extent permitted by law.</p>"},{"location":"company/terms-and-conditions/#as-is-and-as-available-disclaimer","title":"\"AS IS\" and \"AS AVAILABLE\" Disclaimer","text":"<p>The Service is provided to You \"AS IS\" and \"AS AVAILABLE\" and with all faults and defects without warranty of any kind. To the maximum extent permitted under applicable law, the Company, on its own behalf and on behalf of its Affiliates and its and their respective licensors and service providers, expressly disclaims all warranties, whether express, implied, statutory or otherwise, with respect to the Service, including all implied warranties of merchantability, fitness for a particular purpose, title and non-infringement, and warranties that may arise out of course of dealing, course of performance, usage or trade practice. Without limitation to the foregoing, the Company provides no warranty or undertaking, and makes no representation of any kind that the Service will meet Your requirements, achieve any intended results, be compatible or work with any other software, applications, systems or services, operate without interruption, meet any performance or reliability standards or be error free or that any errors or defects can or will be corrected.</p> <p>Without limiting the foregoing, neither the Company nor any of the company's provider makes any representation or warranty of any kind, express or implied: (i) as to the operation or availability of the Service, or the information, content, and materials or products included thereon; (ii) that the Service will be uninterrupted or error-free; (iii) as to the accuracy, reliability, or currency of any information or content provided through the Service; or (iv) that the Service, its servers, the content, or e-mails sent from or on behalf of the Company are free of viruses, scripts, trojan horses, worms, malware, timebombs or other harmful components.</p> <p>Some jurisdictions do not allow the exclusion of certain types of warranties or limitations on applicable statutory rights of a consumer, so some or all of the above exclusions and limitations may not apply to You. But in such a case the exclusions and limitations set forth in this section shall be applied to the greatest extent enforceable under applicable law.</p>"},{"location":"company/terms-and-conditions/#governing-law","title":"Governing Law","text":"<p>The laws of the Country, excluding its conflicts of law rules, shall govern this Terms and Your use of the Service. Your use of the Application may also be subject to other local, state, national, or international laws.</p>"},{"location":"company/terms-and-conditions/#disputes-resolution","title":"Disputes Resolution","text":"<p>If You have any concern or dispute about the Service, You agree to first try to resolve the dispute informally by contacting the Company.</p>"},{"location":"company/terms-and-conditions/#for-european-union-eu-users","title":"For European Union (EU) Users","text":"<p>If You are a European Union consumer, you will benefit from any mandatory provisions of the law of the country in which you are resident in.</p>"},{"location":"company/terms-and-conditions/#united-states-legal-compliance","title":"United States Legal Compliance","text":"<p>You represent and warrant that (i) You are not located in a country that is subject to the United States government embargo, or that has been designated by the United States government as a \"terrorist supporting\" country, and (ii) You are not listed on any United States government list of prohibited or restricted parties.</p>"},{"location":"company/terms-and-conditions/#severability-and-waiver","title":"Severability and Waiver","text":""},{"location":"company/terms-and-conditions/#severability","title":"Severability","text":"<p>If any provision of these Terms is held to be unenforceable or invalid, such provision will be changed and interpreted to accomplish the objectives of such provision to the greatest extent possible under applicable law and the remaining provisions will continue in full force and effect.</p>"},{"location":"company/terms-and-conditions/#waiver","title":"Waiver","text":"<p>Except as provided herein, the failure to exercise a right or to require performance of an obligation under these Terms shall not effect a party's ability to exercise such right or require such performance at any time thereafter nor shall the waiver of a breach constitute a waiver of any subsequent breach.</p>"},{"location":"company/terms-and-conditions/#translation-interpretation","title":"Translation Interpretation","text":"<p>These Terms and Conditions may have been translated if We have made them available to You on our Service. You agree that the original English text shall prevail in the case of a dispute.</p>"},{"location":"company/terms-and-conditions/#changes-to-these-terms-and-conditions","title":"Changes to These Terms and Conditions","text":"<p>We reserve the right, at Our sole discretion, to modify or replace these Terms at any time. If a revision is material We will make reasonable efforts to provide at least 30 days' notice prior to any new terms taking effect. What constitutes a material change will be determined at Our sole discretion.</p> <p>By continuing to access or use Our Service after those revisions become effective, You agree to be bound by the revised terms. If You do not agree to the new terms, in whole or in part, please stop using the website and the Service.</p>"},{"location":"company/terms-and-conditions/#contact-us","title":"Contact Us","text":"<p>If you have any questions about these Terms and Conditions, You can contact us:</p> <ul> <li> <p>By email: info@visionify.ai</p> </li> <li> <p>By visiting this page on our website: https://www.visionify.ai/contact-us</p> </li> </ul>"},{"location":"custom/","title":"Index","text":""},{"location":"custom/#python-types","title":"Python types","text":"<p>If you need a refresher about how to use Python type hints, check the first part of FastAPI's Python types intro.</p> <p>You can also check the mypy cheat sheet.</p> <p>In short (very short), you can declare a function with parameters like:</p> <pre><code>from typing import Optional\ndef type_example(name: str, formal: bool = False, intro: Optional[str] = None):\npass\n</code></pre> <p>And your editor (and Typer) will know that:</p> <ul> <li><code>name</code> is of type <code>str</code> and is a required parameter.</li> <li><code>formal</code> is a <code>bool</code> and is by default <code>False</code>.</li> <li><code>intro</code> is an optional <code>str</code>, by default is <code>None</code>.</li> </ul> <p>These type hints are what give you autocomplete in your editor and several other features.</p> <p>Typer is based on these type hints.</p>"},{"location":"custom/#intro","title":"Intro","text":"<p>This tutorial shows you how to use Typer with all its features, step by step.</p> <p>Each section gradually builds on the previous ones, but it's structured to separate topics, so that you can go directly to any specific one to solve your specific CLI needs.</p> <p>It is also built to work as a future reference.</p> <p>So you can come back and see exactly what you need.</p>"},{"location":"custom/#run-the-code","title":"Run the code","text":"<p>All the code blocks can be copied and used directly (they are tested Python files).</p> <p>To run any of the examples, copy the code to a file <code>main.py</code>, and run it:</p> <pre><code>$ python main.py\n\n\u2728 The magic happens here \u2728\n</code></pre> <p>It is HIGHLY encouraged that you write or copy the code, edit it and run it locally.</p> <p>Using it in your editor is what really shows you the benefits of Typer, seeing how little code you have to write, all the type checks, autocompletion, etc.</p> <p>And running the examples is what will really help you understand what is going on.</p> <p>You can learn a lot more by running some examples and playing around with them than by reading all the docs here.</p>"},{"location":"custom/#install-typer","title":"Install Typer","text":"<p>The first step is to install Typer.</p> <p>For the tutorial, you might want to install it with all the optional dependencies and features:</p> <pre><code>$ pip install \"typer[all]\"\n---&gt; 100%\nSuccessfully installed typer click shellingham rich\n</code></pre> <p>...that also includes <code>rich</code> and <code>shellingham</code>.</p>"},{"location":"custom/faqs/","title":"VisionAI Customization FAQs","text":"<p>Learn more about customization of models</p>"},{"location":"custom/faqs/#what-is-customization-of-models","title":"What is customization of models?","text":"<p>Customization of models lets you manage AI models based on your data and its semantics. Our solution help you choose the data and scenarios on which you wish to generate insights.</p>"},{"location":"custom/faqs/#does-visionai-solution-can-interact-with-my-existing-camera","title":"Does VisionAI solution can interact with my existing camera?","text":"<p>Our ready-to-deploy models can be installed on your system with the help of ARM templates. After this, you can add and customize camera settings per the scenarios. Since our solution works with most consumer and commercial-grade cameras, you\u2019ll likely not have difficulty using our services.</p>"},{"location":"custom/faqs/#how-can-i-handle-large-number-of-classes","title":"How can I handle large number of classes?","text":"<p>We have a dedicated team of annotators. Any number of classes can be added/removed from the model easily. We support model customization through class selection exclusively.</p>"},{"location":"custom/faqs/#how-do-i-get-a-solution-for-a-use-case-thats-not-included-in-visionai-tool-kit","title":"How do I get a solution for a use-case that's not included in VisionAI tool kit?","text":"<p>We are open to add new any use-case in VisionAI depending on customer's Business perspective. We would be building custom models for any new case.</p>"},{"location":"custom/licensing/","title":"Licencing","text":"<p>Our VisionAI toolkit is licensed under GPLv3 License. It means that any applications you link this software have to be under GPLv3 as well.</p>"},{"location":"custom/overview/","title":"Customization","text":"<p>We offer extensive cutomization for VisionAI Toolkit in the form of various custom integration services</p>"},{"location":"custom/overview/#overview","title":"Overview","text":"<p>Customization can take many different forms, such as:</p> <ul> <li> <p>Retraining:</p> <p>Model retraining is the process of adding new data to an existing AI model and evaluating the model performance by fine tuning.</p> <p>Find more details about this here.</p> </li> <li> <p>Custom Integration:</p> <p>Our Custom Integration services deal with enabling analytics support in the form of dynamic reporting, online alerts and high scalability with integration of Azure IoT hub.</p> <p>Find more details about this here.</p> </li> </ul>"},{"location":"custom/request/","title":"Custom Integration","text":"<p>Documentation for VisionAI customization</p>"},{"location":"custom/request/#overview","title":"Overview","text":"<p>VisionAI Toolkit provides end-to-end data customization and analytics services. We provide extensive analytics expertise, as well as domain expertise, to assist clients in transforming their businesses.</p>"},{"location":"custom/request/#process","title":"Process","text":"<p>We can assist you in optimising business use-cases so that you can focus on activities that generate true business value. </p> <p> </p> <p>This includes:</p> <ul> <li> <p>Continous Monitoring: We use the ease of a single web console to conveniently monitor your whole work place. We assist you in avoiding problems before they affect or are noticed by users. Using our monitoring service, learn more about:</p> <ol> <li>Event logs</li> <li>Root Cause Analysis</li> <li>Traffic monitoring</li> </ol> </li> <li> <p>Alerts: Quick alerts and notifications for events (eg. accidents/non-compliance) can be send through emails/messages.</p> </li> <li> <p>Reporting: Detailed reports can be generated for daily/monthly basis. Frequency of reports can be customized to accomodate user preferences. Granularity depends on user's business perspective.</p> </li> </ul> <p>For example, </p> <ol> <li> <p>Ensuring true compliance for Personal protective equipment(PPE) by generating instant alerts</p> </li> <li> <p>Providing daily and summary reports to give in-depth insights</p> </li> <li> <p>Providing flexibility by customizing different options </p> </li> </ol> <p>Current, real-time web applications pose unique horizontal scalability challenges.  Therefore, our current architecture uses pub/sub messaging mechanism to push events notifications to Redis/ Graffana. These can be integrated through Azure Event Grid to Azure IoT hub. An architecture to support this would be:</p> <p> </p>"},{"location":"custom/retraining/","title":"Retraining","text":"<p>We offer retraining of models, as we have a well-defined process for working with clients, which could include steps such as data collection and labeling, model fine-tuning, and evaluation. We also have a strong infrastructure in place to support the processing and analysis of large volumes of data.</p> <p>Retraining steps would be: <pre><code>graph LR\nA[Gathering Custom data] --&gt; B[Labeling Data]\nB[Fine-tuning the model] --&gt; C[Evaluating the model]</code></pre></p>"},{"location":"custom/retraining/#overview","title":"Overview","text":"<p>Retraining of computer vision models refers to the process of updating a pre-trained model with additional data or new labels. This is often done to improve the performance of a model or to adapt it to a new task or application.</p> <p>The process of retraining a computer vision model typically involves the following steps:</p> <ul> <li> <p>Collecting new data: This involves gathering new images or video data that is relevant to the new task or application.</p> </li> <li> <p>Labeling the data: The new data needs to be labeled to provide the model with the correct information about what is in the images. We have a dedicated team of annotators to accomodate this.</p> </li> <li> <p>Fine-tuning the model: The pre-trained model is then fine-tuned on the new labeled data. This involves adjusting the model's parameters to better fit the new data and labels.</p> </li> <li> <p>Evaluating the model: The retrained model is then evaluated to determine how well it performs on the new task or application.</p> </li> </ul> <p>Our team  is up-to-date with the latest techniques and best practices in the field, as well as the ethical and legal considerations related to working with sensitive data.  They have a good understanding of the principles of computer vision and machine learning, as well as the specific requirements of the application, to ensure the retraining process is successful.</p>"},{"location":"overview/azure-managed-app/","title":"Azure Managed App","text":"<p>VisionAI Azure Managed Application is a  pre-built cloud solution that is deployed through Azure Marketplace to an Azure environment for end customers. </p>"},{"location":"overview/azure-managed-app/#overview","title":"Overview","text":"<p>VisionAI Azure Managed application is designed to provide a fast and secure way to deliver applications and services to customers while ensuring consistency and control.</p> <p>Basically, Managed Applications is a packaged solution that include all the necessary resources and components, such as virtual machines, storage accounts, networking resources, and security configurations. </p>"},{"location":"overview/azure-managed-app/#access-visionai-app","title":"Access VisionAI App","text":"<p>VisionAI Azure App is accessible by logging into Azure Market Place.</p> <p> It shows its Overview, different plans and ratings. Once we click, Get it Now. Following screen appears.</p> <p></p> <p>Enter all your information and click on Continue. It takes you to your dashboard as:</p> <p></p> <p>In Basics tab, Enter your project details in the following screen.</p> <p></p> <p>If you have empty resource group, please select that otherwise create new by clicking on Create new.</p> <p></p> <p>Verify Virtual Machine Settings. Click on Review+create. It takes some time to perform validation.</p> <p>The Managed Applications can be customized with branding, pricing, and support offerings, allowing MSPs to differentiate their offerings and provide added value to their customers.</p> <p>In summary, Azure Managed Applications offer a simplified and streamlined way to deploy and manage pre-built cloud solutions, enabling customers to focus on their core business functions while leaving the management and maintenance of the underlying infrastructure to Microsoft.</p>"},{"location":"overview/cameras/","title":"Camera","text":"<p>Use your existing camera to integrate with VisionAI platform </p>"},{"location":"overview/cameras/#what-cameras-can-i-use","title":"What Cameras can I use?","text":"<p>Cameras that can be used with VisionAI app would be like this:</p> <ul> <li> <p>IP cameras</p> <p>Use the video feed from a single or a network of digital video security cameras. These could be common security cameras (CCTV) or network cameras (IP cameras) connected to the same network as the edge device (computer).\u00a0\u00a0We offer\u00a0integration for IP cameras.</p> </li> <li> <p>USB cameras or webcams</p> <p>Utilize webcams or USB cameras to process AI models.</p> </li> </ul> <p>The common video output formats are MPEG, WMV, or MPEG-4. RTSP, HTTP, and HTTPS are frequently used protocols for sharing live broadcast streams.</p>"},{"location":"overview/cameras/#camera-placement-guidelines","title":"Camera Placement Guidelines","text":"<p>When positioning cameras for various use situations, take into account the following general guidelines: </p> <ol> <li> <p>Lighting: Install cameras underneath a light fittings so that they do not obscure the cameras.</p> </li> <li> <p>Backlighting: Avoid mounting cameras near to window or other areas to protect from backlighting issue. It affects image quality.</p> </li> <li> <p>Local policies: Take into account local placement policies and laws.</p> </li> <li> <p>Authorization: The installation of cameras should be authorized by a designated person or department within the organization. </p> </li> <li> <p>Maintenance: Cameras should be regularly maintained and checked to ensure they are functioning properly. The policy should specify who is responsible for maintaining the cameras and how often they should be checked.</p> </li> </ol> <p>Note</p> <p>Overall, it's important to develop local policies for camera mounting that balance the need for surveillance with the protection of privacy rights. The policies should be reviewed and updated regularly to ensure they remain relevant and effective.</p>"},{"location":"overview/cameras/#camera-view","title":"Camera View","text":"<p>The camera view refers to the field of vision captured by a camera. The camera view is determined by the placement of the camera and its angle of view.</p> <p>When considering camera view, it's important to think about the following:</p> Camera Front ViewCamera Ceiling View <p>A camera front view refers to the perspective captured by a camera facing forward, usually at a person's eye level or slightly higher. This type of camera view is commonly used in areas where it's necessary to monitor and identify individuals. When positioning a camera for front view, there are several factors to consider:</p> Factors <ul> <li> <p>Purpose: Determine the purpose of the camera view. </p> </li> <li> <p>Height: The camera should be positioned at a height that allows for a clear view of the face and upper body of the person being monitored. The camera should be at or slightly above eye level.</p> </li> <li> <p>Angle: The camera angle should be adjusted so that it's facing straight ahead or slightly downward to capture clear images of individuals' faces.</p> </li> <li> <p>Lighting: Proper lighting is crucial for capturing clear images. Ensure that there is adequate lighting in the area where the camera is positioned.</p> </li> <li> <p>Field of View: Determine the area that needs to be monitored and the degree of coverage required. The field of view will depend on the type of camera, its lens, and its angle of view.</p> </li> </ul> <p>Overall, careful consideration of camera height, angle, lighting, field of view, and privacy concerns can help ensure that a camera front view provides clear and useful images for its intended purpose.</p> <p>The following illustration provides simulations for the camera front views.</p> Example 1 Example 2 <p>A camera mounted on the ceiling can provide a wide and unobstructed view of the surrounding area. This type of camera view can be particularly useful in large indoor spaces.</p> <p>When positioning a camera for ceiling view, there are several factors to consider:</p> Factors <ul> <li> <p>Camera type: Choose a camera that is specifically designed for ceiling mounting. These cameras usually have a dome shape and can be either fixed or adjustable in terms of angle.</p> </li> <li> <p>Camera angle: Determine the best angle for the camera based on the specific area you want to monitor. The camera should be positioned to provide a clear view of the entire area without any blind spots.</p> </li> <li> <p>Height: The camera should be mounted high enough to avoid tampering or obstruction, but low enough to capture detailed images of individuals and objects in the area.</p> </li> <li> <p>Lighting: Consider the lighting conditions in the area where the camera will be installed. If the area is poorly lit, additional lighting may be needed to ensure clear images.</p> </li> <li> <p>Wiring: Ensure that there is a power source and wiring available for the camera. Consider whether it will be necessary to run wiring through the ceiling or walls.</p> </li> </ul> <p>Overall, a camera mounted on the ceiling can provide an effective way to monitor large indoor spaces. Careful consideration of camera type, angle, height, lighting, wiring, and privacy concerns can help ensure that the camera provides clear and useful images while respecting privacy rights.</p> <p>The following illustration provides simulations for the camera ceiling views.</p> Example 1 Example 2"},{"location":"overview/faqs/","title":"VisionAI platform FAQs","text":"<p>Learn more about the platform</p>"},{"location":"overview/faqs/#what-is-visional","title":"What is VisionAl ?","text":"<p>VisionAI is ready-to-use Python Library for various Computer Vision Scenarios.</p>"},{"location":"overview/faqs/#what-scenarios-do-you-support","title":"What scenarios do you support?","text":"<p>VisionAI library is focused on common workplace and employee health &amp; safety scenarios. At a high-level these include the employee health and safety hazard &amp; fire warnings, equipment monitoring, vehicle monitoring people &amp; productivity monitoring, auspicious activity monitoring and common company compliance policies.</p>"},{"location":"overview/faqs/#do-i-need-to-install-get-new-cameras-to-run-this-system","title":"Do I need to install get new cameras to run this system?","text":"<p>No! You do not need any new camera or hardware to run this system. VisionAl works with your existing security camera infrastructure. We support RTSP, RTMP, HLS and other common video platforms. Current safety surveillance systems are just record and playback \u2013 we can bring a lot of operational and safety insights from the current camera systems.</p>"},{"location":"overview/faqs/#is-it-free-to-use-how-does-the-licensing-work","title":"Is it Free to use? How does the licensing work?","text":"<p>Licency details can be found at here. here.</p>"},{"location":"overview/faqs/#how-can-i-try-it-out-quickly","title":"How can I try it out quickly?","text":"<p>We recommend testing this on a beefy machine with a NVIDIA graphics card. To quickly test out a scenario you can follow these commands.</p> <p><pre><code>$ pip install visionai\n\n$ visionai web\n</code></pre> And then browse the different scenarios, create pipelines for our organization. We also provide a Azure Managed App which has all the dependencies pre-installed. </p>"},{"location":"overview/faqs/#how-can-i-customize-the-models-to-work-in-my-environment","title":"How can I customize the models to work in my environment?","text":"<p>We work with our clients to quickly create customized models for their use-cases. This is available to be purchased through Azure marketplace as a Consulting Service-we recommend this option for a quicker transaction. You can find more details here.</p>"},{"location":"overview/faqs/#how-do-i-ensure-that-my-images-are-not-used-in-training-other-models","title":"How do I ensure that my images are not used in training other models?","text":"<p>We take our customers data privacy very seriously. All our current models available in the community edition are based off of open-source datasets We have several customer specific models trained on private data, but those datasets are maintained on their own storage accounts. They are not used for training any publicly available models.</p>"},{"location":"overview/faqs/#we-already-have-some-vision-al-models-running-can-you-integrate-with-them","title":"We already have some Vision Al models running, can you integrate with them?","text":"<p>Our system is isoluated on its own and all it needs is a set of comera output. We are focused on building more use-cases to cover common safety and compliance scenarios As such, we would work with you to build a framework where these use cases are isolated and easy to. integrate into your organization.</p>"},{"location":"overview/faqs/#we-have-an-inhouse-ml-team-how-does-this-help-them","title":"We have an inhouse ML team. How does this help them?","text":"<p>Our license terms are flexible to provide your in-house ML team a starting point where they can build their new use-cases. We also provide a customer success team that can with you to understand your requirements and guide you in coming up with the right solutions for the problems you are working on.</p>"},{"location":"overview/how-it-works/","title":"How it works","text":""},{"location":"overview/how-it-works/#visionai","title":"VisionAI","text":"<p>The VisionAI application monitors employee activity, environment, and equipment in real-time. It uses cameras, sensors and other tools to detect any unsafe conditions or practices. The VisionAI application can also detect any hazardous materials or substances present in the workplace and alert the appropriate personnel.</p> <p>With VisionAI toolkit employers can reduce risks and improve safety for their employees.</p> <p>Workplace safety is an important issue in any business, and having a comprehensive workplace safety application can help to ensure the safety of employees, customers, and visitors. An effective safety application should be designed to integrate with existing security camera infrastructure, provide a wide range of scenarios to run, and enable users to configure their own alerting system. VisionAI toolkit is designed to meet these requirements.</p> <p>VisionAI toolkit works in the following 3 simple steps:</p> <pre><code>graph TD\nA[Start with existing camera infrastructure] --&gt; \nB[Pick-n-Choose scenarios ] --&gt; C[Get alerts and insights]</code></pre>"},{"location":"overview/how-it-works/#existing-camera-infrastructure","title":"Existing camera infrastructure","text":"<p>The first step is getting started with your existing security camera infrastructure. CCTV cameras should be located in all parts of the building, including high-traffic areas, exits, and entrances, as well as any areas that may be considered to be at higher risk. Having a comprehensive view of the building\u2019s layout will enable the safety application to detect any potential safety hazards, such as intruders, or suspicious activities.</p>"},{"location":"overview/how-it-works/#pick-n-choose-scenarios","title":"Pick-n-Choose scenarios","text":"<p>Once the security camera infrastructure is in place, users can then pick-n-choose the scenarios they would like the application to run. These might include the detection of people entering restricted areas, slip-and-fall, or the monitoring of activity in high-risk areas. Furthermore, the application can raise alerts when certain activities are taking place, such as a person loitering in a particular area, or a large group gathering in a restricted area.</p>"},{"location":"overview/how-it-works/#get-alerts-and-insights","title":"Get alerts and insights","text":"<p>Finally, users can configure their alerting system within the web-app. This can include email notifications, text messages, or even automated messages sent to the relevant authorities. Users can also set the parameters for when the alert should be triggered, such as when the number of people in a certain area exceeds a certain threshold, or when a particular activity is detected.</p> <p>In summary, having a comprehensive workplace safety application in place is essential for any business. Integrating the application with existing security camera infrastructure will enable users to detect any potential safety hazards, while setting up the right scenarios and alerting system will ensure that the safety application is always functioning as it should. With the right workplace safety application in place, businesses can enjoy peace of mind, knowing that their employees, customers, and visitors are safe and secure.</p>"},{"location":"overview/next-steps/","title":"Next steps","text":"<p>This provides a comprehensive guideline for the VisionAI toolkit's access path.</p> <p>In summary, the VisionAI toolkit is accessible via direct installation, web-app, and Azure managed app. This makes it more adaptable and dynamic.</p>"},{"location":"overview/next-steps/#install-the-application","title":"Install the application","text":"<p>To test this model &amp; scenario, you can use the following steps:</p> <ul> <li> <p>Install the visionai package from PyPI</p> <p><pre><code>$ pip install visionai\n</code></pre> Test the scenario from your local web-cam by mentioning scenario name</p> <pre><code>$ visionai scenario test [OPTIONS] NAME\n</code></pre> <p>Arguments:</p> <ul> <li><code>NAME</code>: [required]</li> </ul> <p>NAME can be any of the scenarios integrated in VisionAI</p> <ul> <li>Example</li> </ul> <pre><code>$ visionai scenario test ppe-detection\n\nDownloading models for scenario: ppe-detection\nModel: ppe-detection: https://workplaceos.blob.core.windows.net/models/yolov5s-people/yolov5s-people-0.0.4.zip\nStarting scenario: ppe-detection..\n</code></pre> <p>You should be able to see the events generated on your console window with the detections of safety gloves, goggles, helmet, mask, safety-shoes and vest within the camera field of view.</p> </li> </ul>"},{"location":"overview/next-steps/#access-the-visionai-web-app","title":"Access the visionAI Web-app","text":"<p>VisionAI web-app, a software application that runs in a web browser and designed to provide a user-friendly interface and functionality that can be accessed from any device with an intpernet connection, without the need for installation on the device. It can be accessed by here.</p> <p>The app has built-in functionality to accomodate different scenarios and wide range of camera instances.</p> <p></p> <p>Find more details about these sections here.</p>"},{"location":"overview/next-steps/#access-the-azure-managed-app","title":"Access the Azure Managed-app","text":"<p>The VisionAI Azure Managed application is intended to provide customers with a quick and secure way to deliver applications and services while maintaining consistency and control.</p> <p>VisionAI Azure App is accessible by logging into Azure Market Place.</p> <p> The appeared screen shows its Overview, different plans and ratings. To access it, click on Get it Now and follow the sequence of steps. </p> <p>Find more details about these sections here.</p>"},{"location":"overview/scenarios/","title":"Scenarios","text":"<p>Scenarios form the building blocks of VisionAI platform. These scenarios are organized into <code>Suites</code>. Below we talk about different suites and the scenarios that are part of them.</p> <ul> <li>All scenarios are available as pick-n-choose scenarios. You can pick the scenarios you want based on your business needs. Each scenario is independently tested.</li> <li>Events provided by these scenarios are given below. Events are sent to Redis &amp; Azure EventHub pubsub systems for further integration.</li> <li>There are a few common events supported by all scenarios (daily summary, weekly summary etc.)</li> <li>Currently supported scenarios are highlighted by a \u2705. Roadmap scenarios are highlighted by a \ud83d\udcc5.</li> <li>Each of the scenarios can be quickly tested through <code>visionai run &lt;scenario-name&gt;</code> command. For example:</li> </ul> <pre><code>visionai run smoke-and-fire-detection\n</code></pre> <p>New scenario request</p> <p>This section lists down all the scenarios that are supported by the VisionAI platform. There are more scenarios added daily - please send a request to us about any additional scenarios you need.</p>"},{"location":"overview/scenarios/#privacy-suite","title":"Privacy Suite","text":"<p>For a majority of organizations - employee privacy is a top concern. Along with employee privacy, the organization needs to make sure that any data does not leave the premises. Any faces detected through Vision AI system need to be blurred, along with text, signage, computer screens and other sensitive information.</p> <p>Before any other scenarios are run, or before we store or process the images - the images are pre-processed through this privacy suite. As such, privacy suite is treated differently from other scenarios. Below examples provide a high-level overview of the privacy suite.</p> Status Scenario name Details Additional considerations \u2705 <code>face-blurring</code> Blur any faces detected More details \u2705 <code>text-blurring</code> Blue any text detected (paper, computer screens etc) More details \u2705 <code>license-plate-blurring</code> Blur any license plates detected More details \ud83d\udcc5 <code>signs-blurring</code> Blur any signs detected More details \ud83d\udcc5 <code>obstructed-camera</code> If camera feed is obstructed, send an alert More details"},{"location":"overview/scenarios/#hazard-warnings-suite","title":"Hazard Warnings Suite","text":"<p>Following scenarios provide hazard warning examples supported by VisionAI suite. Currently supported scenarios are highlighted by a \u2705. You can run these through VisionAI CLI, for example, you can run the following command for smoke-and-fire-detection. Once the scenario has started - you can use a lighter or a match to generate the events. The events can be viewed on CLI window.</p> <pre><code>visionai run smoke-and-fire-detection\n</code></pre> <p>TODO</p> <ul> <li>TODO: For scenarios requiring IR camera and/or IoT Sensor, point to the exact device this has been tested with.</li> </ul> Status Scenario name Supported Events Additional considerations \u2705 <code>smoke-and-fire-detection</code> <code>Smoke event detected</code> <code>Fire event detected</code> <code>Sparks detected</code> <code>Open flames detection</code> More details \u2705 <code>no-smoking-zone</code> <code>Smoking event detected</code> <code>Vaping event detected</code> More details \ud83d\udcc5 <code>spills-and-leak-detection</code> <code>Water puddle detected</code> <code>Water leak from equipment detected</code> <code>Spill event detected</code> <code>Slippery sign detected</code> \ud83d\udcc5 <code>gas-leak-detection</code> <code>Gas leak event detected</code> IR Camera Required \ud83d\udcc5 <code>missing-fire-extinguisher</code> <code>Fire extinguisher missing</code> \ud83d\udcc5 <code>blocked-exit-monitoring</code> <code>Blocked exit detected</code> \u2705 <code>slip-and-fall-detection</code> <code>Person fall event detected</code> <code>Path block detected</code> More details \ud83d\udcc5 <code>equipment-temperature-ir-camera</code> <code>Temperature exceeds limit</code> <code>Temperature subceeds limit</code> IR Camera Required \u2705 <code>rust-and-corrosion-detection</code> <code>Rust or corrosion event detected</code> More details"},{"location":"overview/scenarios/#worker-health-safety-suite","title":"Worker Health &amp; Safety Suite","text":"<p>Following scenarios provide Worker Health and Safety examples supported by VisionAI suite. (Also referred to as Personnel Health and Safety).</p> <p>Workplace Personnel Health &amp; Safety is important because it ensures that employees are safe and healthy in their work environment. This includes providing a safe and healthy work environment, proper safety training, and regular safety inspections. Additionally, it also includes enforcing safety policies to ensure that all employees are aware of and follow safety procedures, as well as encouraging a culture of safety within the workplace.</p> <p>Currently supported scenarios are highlighted by a \u2705. You can run these through VisionAI CLI, for example:</p> <pre><code>visionai run ppe-detection\n</code></pre> <p>You can see real-time events generated as soon as person is detected without PPE (helmets, gloves, safety boots etc.). There are options to configure what PPE's are required for your scenario. This can be done through the VisionAI web-application which can be accessed on through http://localhost:3001.</p> Status Scenario name Supported Events Additional considerations \u2705 <code>ppe-detection</code> <code>Person detected without helmet</code> <code>Person detected without gloves</code> <code>Person detected without safety boots</code> <code>Person detected without safety goggles</code> <code>Person detected without face mask</code> <code>Person detected without vest</code> <code>Person detected without full-body suit</code> <code>Person detected without PFAS</code> <code>Person detected without ear protection</code> More details \u2705 <code>working-at-heights</code> <code>Person detected without PFAS</code> <code>Steps detected without railings</code> <code>Person detected at height without parapets</code> <code>Ladder detected not in compliance</code> More details \u2705 <code>fall-and-accident-detection</code> <code>Person slip &amp; fall detected</code> <code>Potential collision/accident detected</code> <code>Wet floor detected</code> <code>Debris detected on floor</code> <code>Wet/slippery sign detected</code> \u2705 <code>worker-fatigue-detection</code> <code>Drowsy worker detected</code> Straight camera angle \u2705 <code>posture-and-ergonomics</code> <code>Bend count per individual</code> Straight camera angle  More details \u2705 <code>confined-spaces-monitoring</code> <code>Person detected</code> <code>Person left</code> <code>Person dwell time exceeds limit</code> <code>Person detected without motion</code> <code>Person fall detected</code> More details \ud83d\udcc5 <code>empty-pallets-detection</code> <code>Empty pallets detected</code> <code>Partially empty pallets detected</code> \ud83d\udcc5 <code>spills-and-leaks-detection</code> <code>Water puddle detected</code> <code>Water leak from equipment detected</code> <code>Wet floor detected</code> <code>Spill event detected</code> <code>Slippery sign detected</code> \ud83d\udcc5 <code>hand-wash-compliance</code> <code>Missed hand wash</code> \ud83d\udcc5 <code>environment-monitoring</code> <code>CO out of range</code> <code>CO2 out of range</code> <code>CH4 out of range</code> <code>VOCs out of range</code> <code>Temperature out of range</code> <code>Pressure out of range</code> <code>Humidity out of range</code> \ud83d\udcc5 <code>person-temperature-monitoring</code> <code>Person temperature exceeds threshold</code> IR Camera required"},{"location":"overview/scenarios/#occupancy-policies","title":"Occupancy Policies","text":"<p>Occupancy Policies relate to counting and tracking employees and/or other personnel in the room. These could include people-counting and enforcing max-occupancy policies, or tracking people's dwell time in a confined space.</p> <p>Currently supported scenarios are highlighted by a \u2705. You can run these through VisionAI CLI, for example:</p> <pre><code>visionai run max-occupancy\n</code></pre> <p>Occupancy Metrics</p> <ul> <li>Occupancy metrics is similar in structure to max-occupancy, or restricted areas scenarios.</li> <li>However it sends out a summary event is structured like this. This will give a granular summary event at the end of the day.</li> <li>Users can start with occupancy-metrics and then move to max-occupancy or restricted areas if they need to enforce policies. <pre><code>{\n\"date\": \"2023-02-23\",\n\"stations\": [{\n\"id\": \"station_1\",\n\"hours\": [\n{\n\"start_time\": \"2023-02-23T14:00:01\",\n\"end_time\": \"2023-02-23T15:00:00\",\n\"occupancy_cnt\": 14\n}\n...\n]\n}...]\n}\n</code></pre></li> </ul> <p>Also need to specify that the camera needs to be configured to have a good view of the stations where occupancy metrics need to be checked.</p> Status Scenario name Supported Events Additional considerations \u2705 <code>max-occupancy</code> <code>Person count exceeds max limit</code> More details \u2705 <code>restricted-areas</code> <code>Person detected in restricted area</code> <code>Movement detected in restricted area</code> <code>Person detected after hours</code> <code>Movement detected after hours</code> More details \u2705 <code>dwell-time</code> <code>Person detected</code> <code>Person left</code> <code>Person dwell time exceeds limit</code> <code>Person detected without motion</code> <code>Person fall detected</code> More details \ud83d\udcc5 <code>social-distancing</code> <code>Person detected</code> <code>Person left</code> <code>Person distance event</code> \u2705 <code>desk-occupancy</code> <code>Daily summary event</code> More details \u2705 <code>station-occupancy</code> <code>Daily summary event</code> More details \ud83d\udcc5 <code>occupancy-metrics</code> <code>Daily summary event</code> \ud83d\udcc5 <code>no-children-pets-visitors</code> <code>Children detected</code> <code>Pets detected</code> <code>Visitors detected</code> \ud83d\udcc5 <code>authorized-personnel-only</code> <code>Unauthorized person detected</code>"},{"location":"overview/scenarios/#company-policies","title":"Company Policies","text":"<p>Company policies include specific scenarios that are relevant to your company. These could include scenarios like no-smoking/no-vaping zones, no food or drinks in certain areas, or no cell phones/pictures in certain areas. Some of these scenarios overlap with occupancy policies, but they are still useful to have here as separate scenarios.</p> Status Scenario name Supported Events Additional considerations \u2705 <code>max-occupancy</code> <code>Person count exceeds max limit</code> More details \u2705 <code>restricted-areas</code> <code>Person detected in restricted area</code> <code>Movement detected in restricted area</code> <code>Person detected after hours</code> <code>Movement detected after hours</code> More details \u2705 <code>dwell-time</code> <code>Person detected</code> <code>Person left</code> <code>Person dwell time exceeds limit</code> <code>Person detected without motion</code> <code>Person fall detected</code> More details \ud83d\udcc5 <code>social-distancing</code> <code>Person detected</code> <code>Person left</code> <code>Person distance event</code> \u2705 <code>desk-occupancy</code> <code>Daily summary event</code> More details \u2705 <code>station-occupancy</code> <code>Daily summary event</code> More details \u2705 <code>occupancy-metrics</code> <code>Daily summary event</code> More details \ud83d\udcc5 <code>no-children-pets-visitors</code> <code>Children detected</code> <code>Pets detected</code> <code>Visitors detected</code> \ud83d\udcc5 <code>authorized-personnel-only</code> <code>Unauthorized person detected</code> \ud83d\udcc5 <code>no-food-or-drinks-allowed</code> <code>Person with food detected</code> <code>Person with drinks detected</code> <code>Spill event detected</code> \u2705 <code>no-phone-text-pictures</code> <code>Cellphone usage detected</code> <code>Person detected taking pictures</code> More details \u2705 <code>no-smoking-or-vaping</code> <code>Smoking event detected</code> <code>Vaping event detected</code> More details \u2705 <code>no-children-pets-visitors</code> <code>Children detected</code> <code>Pets detected</code> <code>Visitors detected</code> More details \ud83d\udcc5 <code>authorized-personnel-only</code> <code>Person without uniform detected</code> <code>Person without badge detected</code> \ud83d\udcc5 <code>waste-management</code> <code>Spill event detected</code> <code>Waste bin full</code> <code>Debris detected in Field of View</code> \ud83d\udcc5 <code>energy-conservation</code> <code>Occupancy pattern daily summary</code> <code>Light usage daily summary</code> \ud83d\udcc5 <code>restricted-areas</code> <code>Person detected in restricted area</code> <code>Movement detected in restricted area</code> <code>Person detected after hours</code> <code>Movement detected after hours</code> \u2705 <code>badge-tailgating</code> <code>Multi-entry (tailgating) event detected</code> <code>Unauthorized entry event detected</code> More details \u2705 <code>perimeter-control</code> <code>Person detected near fence/perimeter</code> <code>Movement detected near fence/perimeter</code> IR camera requiredMore details"},{"location":"overview/scenarios/#equipment-monitoring","title":"Equipment Monitoring","text":"<p>Equipment policies include specific scenarios that are relevant monitoring heavy machinaries. These could be through monitoring the temperature of the equipment, or through IoT sensors that are attached to the equipment that allow to monitor vibration, noise, or other parameters for the equipment.</p> Status Scenario name Supported Events Additional considerations \ud83d\udcc5 <code>equipment-temperature</code> <code>Equipment temperature exceeds limit</code> <code>Equipment temperature subsceeds limit</code> \u2705 <code>rust-and-corrosion-detection</code> <code>Rust or corrosion event detected</code> More details \ud83d\udcc5 <code>equipment-vibration</code> <code>Equipment vibration exceeds limit</code> 2 \ud83d\udcc5 <code>equipment-noise</code> <code>Equipment noise exceeds limit</code> 3 \ud83d\udcc5 <code>reading-analog-dials</code> <code>Analog meter reading event</code> \ud83d\udcc5 <code>tools-check-in-check-out</code> <code>Person left without checkout</code> \ud83d\udcc5 <code>equipment-water-leak-puddle</code> <code>Water leak detected from equipment</code>"},{"location":"overview/scenarios/#environment-monitoring","title":"Environment Monitoring","text":"<p>Monitoring the environment like current temperature, humidity, or air quality is important to ensure that the workplace is safe and comfortable for employees. These scenarios are implemented through IoT sensors that are completely integrated into Vision AI suite.</p> Status Scenario name Supported Events Additional considerations \ud83d\udcc5 <code>temperature-monitoring</code> <code>Temperature excceds limit</code> <code>Temperature subsceeds limit</code> More details \ud83d\udcc5 <code>humidity-monitoring</code> <code>Humidity excceds limit</code> <code>Humidity subsceeds limit</code> More details \ud83d\udcc5 <code>pressure-monitoring</code> <code>Pressure excceds limit</code> <code>Pressure subsceeds limit</code> More details \ud83d\udcc5 <code>air-quality</code> <code>CO exceeds limit</code> <code>CO2 exceeds limit</code> <code>NO2 Exceeds limit</code> <code>SO2 exceeds limit</code> <code>VOCs exceeds limit</code> <code>Excessive dust detected</code> <code>Excessive dust detected</code> More details \ud83d\udcc5 <code>light-sensor-monitoring</code> <code>Light intensity exceeds limit</code> <code>Light intensity subsceeds limit</code> More details \ud83d\udcc5 <code>noise-level-monitoring</code> <code>Noise level exceeds limit</code> <code>Noise level subsceeds limit</code> More details \ud83d\udcc5 <code>energy-usage-monitoring</code> <code>Energy usage hourly smmary</code> More details \ud83d\udcc5 <code>water-management</code> <code>TODO</code> More details \ud83d\udcc5 <code>waste-management</code> <code>TODO</code> More details \ud83d\udcc5 <code>radiation-monitoring</code> <code>Radiation level exceeds limit</code> <code>Radiation level subsceeds limit</code> More details"},{"location":"overview/scenarios/#suspicious-activity-detection","title":"Suspicious Activity detection","text":"<p>Suspicious activity detection suite relies on a combination of activity detection models and object detection models. These models are trained to detect suspicious activity in a variety of scenarios.</p> Status Scenario name Supported Events Additional considerations \ud83d\udcc5 <code>loitering-detection</code> <code>Person detected in closed space</code> <code>Person detected during off hours</code> <code>Person dwell time exceeds limit</code> More details \ud83d\udcc5 <code>suspicious-package-detection</code> <code>Suspicious package detected</code> <code>Package abandoned</code> More details \ud83d\udcc5 <code>bullying-fighting-aggressive-behavior</code> <code>Bullying/fighting/aggressive event detected</code> More details \ud83d\udcc5 <code>vandalism-graffiti-company-property-destruction</code> <code>Motion detected in area (gross event)</code> <code>People detected in area (more granular event)</code> <code>Non-uniformed personnel detected in area</code> <code>Non badged personnel detected in area</code> <code>Vandalism detected in area (before &amp; after)</code> <code>Paint/graffiti detected in area (before &amp; after changes)</code> <code>Behavior analysis event showing company property destruction.</code> More details \u2705 <code>firearms-knives-detection</code> <code>Person brandishing firearm</code> <code>Person brandishing knives</code> More details \ud83d\udcc5 <code>solictation-detection</code> <code>Potential solicitation event detected</code> More details \ud83d\udcc5 <code>theft-and-or-shoplifting</code> <code>Potential theft detected</code> <code>Potential shoplifting activity detected</code> More details \ud83d\udcc5 <code>shipping-activity-detection</code> <code>Shipping activity detected during after-hours</code> <code>Shipping activity detected from non-designated areas</code> More details \ud83d\udcc5 <code>intrusion-detection</code> <code>Intrusion event detected</code> More details"},{"location":"overview/scenarios/#vehicle-activity","title":"Vehicle Activity","text":"<p>The below scenarios are designed to detect vehicle activity in and around the factory.</p> Status Scenario name Supported Events Additional considerations \ud83d\udcc5 <code>vehicle-policies</code> <code>Vehicle activity detected in non-designtated areas</code> <code>Vehicle activity detected during after-hours</code> <code>Collision event detected</code> <code>Near collision event detected</code> More details \ud83d\udcc5 <code>vehicle-usage</code> <code>Daily summary event of vehicle usage</code> <code>Path-map of vehicle usage</code> More details \ud83d\udcc5 <code>forklift-zone-breach</code> <code>Forklift observed outside of configured zone</code> <code>Pedestrian observed in forklift zone</code> More details \ud83d\udcc5 <code>vehicle-license-plate-detection</code> <code>Vehicle detected with license plate number</code> More details \ud83d\udcc5 <code>vehicle-speed-monitoring</code> <code>Vehicle speed exceeds limit</code> More details \ud83d\udcc5 <code>vehicle-cargo-volume-limit</code> <code>Vehicle cargo volume exceeds limit</code> More details"},{"location":"overview/scenarios/#next-steps","title":"Next Steps","text":"<p>Now that you have a better understanding of the scenarios that are available, you can start to think about how you can organize these scenarios into a solution that meets your needs. You can also go to the individual scenario page to learn more about it. We can customize each of these models for your use-cases and provide you with a solution that is tailored to your needs. You can contact us through this page</p> <ol> <li> <p>This works by detecting a person's uniform and comparing it to a list of authorized personnel. This is a more advanced scenario and requires a custom model to be trained for your specific use-case.\u00a0\u21a9</p> </li> <li> <p>Vibration sensor needed to implement this scenario.\u00a0\u21a9</p> </li> <li> <p>Noise sensor needed to implement this scenario.\u00a0\u21a9</p> </li> </ol>"},{"location":"overview/web-app/","title":"VisionAI Web-app","text":"<p>VisionAI web-app is a software application that runs in a web browser. It is designed to provide a user-friendly interface and functionality that can be accessed from any device with an intpernet connection, without the need for installation on the device.</p> <p>The web-app primarily consists of parts including Dashboard, Cameras and Scenarios as shown below:</p> <p> The main screen that appears when you open web-app is:</p> <p></p> <p>Let us see each of these parts in detail.</p> <ul> <li>Dashboard: Visionai Webapp's dashboard is a graphical user interface that displays important information about latest events in the right pane of the screen. It shows metrics such as total camera added, active camera, alerts generated on daily basis and available scenarios. It also presents a graph showing camera versus events triggered. This Dashboard's data analytics will help managers, analysts, or decision-makers quickly assess the performance and make informed decisions.</li> </ul> <p> - Cameras:  Cameras screen shows detailed information about added camera in the app. </p> <p> It also has an option to add any new camera by providing its details such as:</p> <p> - Scenario: Scenario page provides information about different scenarios available and added camera instance can be connect with these scenarios.</p> <p></p> <p>To get particular scenario in use, click on get this. It will open the following window:</p> <p></p> <p>Here, we can find detailed information about these scenarios such as their accuracy metrics, events generated and user manual for guided instructions.</p> <p></p>"},{"location":"reference/changelog/","title":"Changelog","text":""},{"location":"reference/changelog/#visionai-changelog","title":"VisionAI Changelog","text":""},{"location":"reference/changelog/#020-february-14-2023","title":"0.2.0  February 14, 2023","text":"<ul> <li>\ud83d\udc9a Migrate all documentation to public site.</li> <li>\ud83c\udfa8 Added documentation for difference scenarios.</li> <li>\ud83d\ude9a Support for occupancy monitoring scenario.</li> <li>\ud83d\udd25 Support for better smoke-and-fire detection scenario.</li> </ul>"},{"location":"reference/changelog/#0118-february-14-2023","title":"0.1.18  February 14, 2023","text":"<ul> <li>\ud83d\udc9a Added support for grafana and redis servers.</li> <li>\ud83c\udfa8 Added support for event engine, and publishing to redis</li> <li>\ud83d\ude9a Added commands for <code>visionai init|status|stop</code> which can install all dependencies.</li> <li>\ud83d\udd25 Removed dependency on torch and OpenCV packages. Now the package size goes down significantly.</li> <li>\ud83d\udcdd Updated documentation to reflect the changes.</li> <li>\u2728 Docker networking changes - now all containers connect to bridge network.</li> <li>\ud83d\udd25 Added support for slip-and-fall detection model.</li> <li>\ud83d\udd25 Added support for phone detection and people taking pictures scenarios.</li> </ul>"},{"location":"reference/changelog/#0117-february-9-2023","title":"0.1.17 February 9, 2023","text":"<ul> <li>\ud83d\udccc Added support for <code>visionai web start|stop|status</code> commands with API server support.</li> <li>Ensure We can pull visionify/visionai-api to local machine</li> <li>Run this as a container with model-repo/ and config/ folder shared.</li> <li>Ensure back-to-back stop/start would work.</li> <li>Ensure we can just do <code>web start</code> without doing <code>web install</code></li> <li>Removed <code>web install</code> as it can cause confusion</li> </ul>"},{"location":"reference/changelog/#0116-february-8-2023","title":"0.1.16 February 8, 2023","text":"<ul> <li>\u2728 Support for <code>visionai web start|stop|status</code> commands.</li> <li>\ud83c\udfa8 Pull latest images from dockerhub before starting web server.</li> <li>\ud83d\ude9a Support for alias for all commands (like <code>visionai camera add</code> and <code>visionai cameras add</code>)</li> <li>\ud83d\udd25 Add support for <code>face-blur</code> scenario. You can test it with <code>visionai scenario test face-blur</code> now.</li> <li>\ud83d\udcdd Tested support for Ubuntu (with NVIDIA graphics card), MacOS, and Windows 10.</li> </ul>"},{"location":"reference/changelog/#0115-february-7-2023","title":"0.1.15 February 7, 2023","text":"<ul> <li>\ud83d\udc1b On linux we were using incorrect nvidia_smi package.</li> <li>\ud83c\udfa8 Add support for common spelling errors during commands (like scenarios instead of scenario)</li> <li>\ud83d\ude9a Move scenario.json file to this repo - so everything is in one place.</li> </ul>"},{"location":"reference/changelog/#0114-february-3-2023","title":"0.1.14 February 3, 2023","text":"<ul> <li>\u2728 Support for <code>visionai scenario test</code> command.</li> <li>\u2728 Support for Triton server running on MacOS (tested)</li> <li>\ud83d\udd25 Simplified scenario command names (don't have to specify --name anymore)</li> <li>\ud83d\udcdd Renamed all cli files to _app - to avoid confusion between models.py &amp; models/ module.</li> <li>\ud83d\udcdd Move add-scenario and remove-scenario to camera module (these are camera operations.)</li> <li>\ud83d\udd25 Show nice progress bar while any docker image is being pulled.</li> <li>\ud83e\uddea Added results.show() method to detection that uses cv2.imshow() to show the results locally.</li> </ul>"},{"location":"reference/changelog/#0112-january-31-2023","title":"0.1.12 January 31, 2023","text":"<ul> <li>\u2728 Support for managing triton server</li> <li>\ud83c\udfa8 Start/stop triton server from CLI.</li> <li>\ud83d\udcdd Get/print models status coming from triton.</li> <li>\ud83d\udd25 Implemented pretty printing through rich library for models</li> <li>\ud83e\uddea CI Tests to test both before &amp; after package creation</li> <li>\ud83d\udc1b Fix versioning bug (that broke the previous version)</li> </ul>"},{"location":"reference/changelog/#0111-january-27-2023","title":"0.1.11 January 27, 2023","text":"<ul> <li>Support for Triton models (through http/grpc)</li> <li>Implemented yolov5 backend for triton</li> <li>Implemented Autoshape wrapper for NMS &amp; scaling</li> <li>Added easy test case for reproducing.</li> <li>Updated schema for models, fix test cases for it.</li> </ul>"},{"location":"reference/changelog/#0110-january-25-2023","title":"0.1.10 January 25, 2023","text":"<ul> <li>Implemented download models for scenarios</li> <li>Added cv2, torch, numpy dependencies for inference</li> <li>Added support for <code>--version</code> &amp; <code>--verbose</code> options to cli</li> <li>CLI Test cases to use <code>python -m visionai</code> to replicate user behavior</li> </ul>"},{"location":"reference/changelog/#017-january-24-2023","title":"0.1.7 January 24, 2023","text":"<ul> <li>Implemented scenarios functionality</li> <li>Docker compose integration</li> <li>Makefile integration</li> </ul>"},{"location":"reference/changelog/#017-january-22-2023","title":"0.1.7 January 22, 2023","text":"<ul> <li>Implemented camera add/delete functionality</li> </ul>"},{"location":"reference/changelog/#016-january-20-2023","title":"0.1.6 January 20, 2023","text":"<ul> <li>Implemented initial set of commands in different files (dummy implementation)</li> <li>Testing commands individually or through the main application</li> </ul>"},{"location":"reference/changelog/#013-january-16-2023","title":"0.1.3 January 16, 2023","text":"<ul> <li>Basic overview and usage documentation is updated.</li> <li>Started using a termy JS script to show terminal animations nicely</li> </ul>"},{"location":"reference/changelog/#012-january-14-20123","title":"0.1.2 January 14, 20123","text":"<ul> <li>Made MkDocs documents based on Typer format</li> <li>Registered CNAME to point to https://docs.visionify.ai</li> </ul>"},{"location":"reference/changelog/#011-january-11-2023","title":"0.1.1 January 11, 2023","text":"<ul> <li>Updated Azure DevOps CI/CD to automatically publish package on each merge</li> <li>Initial set of commands for visionai application</li> <li>Made <code>visionai</code> as a callable CLI application through poetry</li> </ul>"},{"location":"reference/changelog/#010-january-10-2023","title":"0.1.0 January 10, 2023","text":"<ul> <li>Initial release: <code>pip install visionai</code></li> <li>Pushed package to <code>PyPI</code> repository</li> </ul>"},{"location":"reference/equipment-temperature/","title":"Equipment Temperature Monitoring","text":"<p>IR Camera based scenario. For critical machines, which can overheat and cause problems - monitor temperatures continually - and notify when anomalous temperatures are detected. Some times event small durations of high temperatures may indicate that something is wrong - or cause more wear for the machines in the long run</p> <p>TODO: Provide an overview</p>"},{"location":"reference/equipment-temperature/#vision-ai-based-monitoring","title":"Vision AI based monitoring","text":"<p>!!! Note TODO: Continuous equipment temperature monitoring.     How Vision AI can help with this     Set up thresholds what areas we are monitoring.     Set up event thresholds of how often we should be informed.     Put some images &amp; diagrams for different machines &amp; industry use-cases here.</p>"},{"location":"reference/equipment-temperature/#events","title":"Events","text":"<p>!!! Note TODO: Event details     What type of events are generated.     What is the frequency of thse events     What is the event data which is sent out.</p>"},{"location":"reference/equipment-temperature/#configuration","title":"Configuration","text":"<p>!!! Note TODO: Configuration     How to configure this scenario.     Cameras can see an area, mark areas where exits are present.</p>"},{"location":"reference/equipment-temperature/#model-details","title":"Model Details","text":""},{"location":"reference/equipment-temperature/#dataset","title":"Dataset","text":"<p>TODO:</p> <ul> <li>Indoor vs Outdoor environments</li> <li>Male vs Female</li> <li>Day vs Night</li> <li>Different types of clothing</li> <li>Different distances from the camera</li> <li>Various lighting conditions</li> <li>Various camera angles and resolutions</li> <li>Using seurity camera feeds</li> </ul>"},{"location":"reference/equipment-temperature/#model","title":"Model","text":"<p>TODO: mode metrics.</p> Model Name Precision Recall  mAP   PERSON DETECTION 84.0%  85.1%  81.5%  <p>and landmark detection model gives the following metrics:</p> Model Name Precision Recall  mAP   LANDMARK DETECTION 84.0%  72.8%  84.9%  <p>The model is light-weight enough to be run on any edge devices.</p>"},{"location":"reference/equipment-temperature/#scenario-details","title":"Scenario details","text":"<p>TODO: Enforcement scenarios. How to configure &amp; use this scenario.</p> <p>TODO: Implement these as multi-tab content views. - Test now with online Web-Cam - With RTSP Camera - Pipelines - With Azure Setup</p>"},{"location":"reference/equipment-temperature/#features","title":"Features","text":"<p>TODO: List of features here. Highlight why this is the most efficient way to implement this.</p>"},{"location":"reference/equipment-temperature/#training-with-custom-data","title":"Training with custom data","text":"<p>The scenario is provided as part of our GPL-v3 package for VisionAI. If you wish to train this with custom datasets, please contact us and we can provide you with the training code. You can do custom training with your own datasets for free, as long as it complies with GPLv3 license (you give back the code to the community). If you are interested in a custom license, please (contact us)[contact.md].</p>"},{"location":"reference/equipment-temperature/#contact-us","title":"Contact Us","text":"<ul> <li>For technical issues, you can open a Github issue here.</li> <li>For business inquiries, you can contact us through our website.</li> </ul>"},{"location":"reference/notes/","title":"Notes","text":""},{"location":"reference/notes/#definition-list","title":"Definition List","text":"<code>Lorem ipsum dolor sit amet</code> <p>Sed sagittis eleifend rutrum. Donec vitae suscipit est. Nullam tempus tellus non sem sollicitudin, quis rutrum leo facilisis.</p> <code>Cras arcu libero</code> <p>Aliquam metus eros, pretium sed nulla venenatis, faucibus auctor ex. Proin ut eros sed sapien ullamcorper consequat. Nunc ligula ante.</p> <p>Duis mollis est eget nibh volutpat, fermentum aliquet dui mollis. Nam vulputate tincidunt fringilla. Nullam dignissim ultrices urna non auctor.</p>"},{"location":"reference/notes/#partially-completed-list","title":"Partially completed list","text":"<ul> <li> Lorem ipsum dolor sit amet, consectetur adipiscing elit</li> <li> Vestibulum convallis sit amet nisi a tincidunt<ul> <li> In hac habitasse platea dictumst</li> <li> In scelerisque nibh non dolor mollis congue sed et metus</li> <li> Praesent sed risus massa</li> </ul> </li> <li> Aenean pretium efficitur erat, donec pharetra, ligula non scelerisque</li> </ul>"},{"location":"reference/notes/#tables","title":"Tables","text":"<p>Suite   Scenarios   Supported events Hazard Warnings Smoke and Fire Detection    \"Smoke event detected Fire event detected Sparks detected Open flames detection\"     No smoking/no-vaping zones  \"Smoking event detected Vaping event detected\"     Spills &amp; Leaks detection (Liquids)  \"Water puddle detected Water leak from equipment detected Spill event detected Slippery sign detected\"     Gas leak detection  Gas leak detected     Missing fire-extinguisher   Missing fire extinguisher     Blocked exit monitoring Blocked exit detected     Equipment temperature   \"Temperature exceeds limit Temperature subceeds limit\"     Slip/trip and fall detection    Blocker on pathway detected.     Equipment rust-and-corrosion    Rust or corrosion event detected</p>"},{"location":"reference/notes/#hazard-warnings","title":"Hazard Warnings","text":"Scenario name Supported Events <code>smoke-and-fire-detection</code> \u2705 Fire event detected  Smoke Event Detected  Sparks Detected  Open Flames Detected <code>no-smoking-no-vaping-zones</code> \u2705 Smoking event detected  Vaping event detected <code>smoke-and-fire-detection</code> \u2705 Fire event detected  Smoke Event Detected  Sparks Detected  Open Flames Detected <p>Smoke and Fire Detection   No smoking/no-vaping zones   Spills &amp; Leaks detection (Liquids)   Gas leak detection   Missing fire-extinguisher   Blocked exit monitoring   Equipment temperature   Slip/trip and fall detection   Equipment rust-and-corrosion</p>"},{"location":"reference/notes/#callout","title":"Callout","text":"<p>Note</p> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.</p> <p>Abstract</p> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.</p> <p>Warning</p> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.</p> <p>Danger</p> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.</p> <p>Success</p> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.</p> <p>Question</p> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.</p> <p>Tip</p> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.</p> <p>Quote</p> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.</p> <p>VisionAI Comment</p> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.</p>"},{"location":"reference/notes/#images","title":"Images","text":"Image caption Image caption"},{"location":"reference/notes/#side-by-side-code-blocks","title":"Side-by-side code-blocks","text":"Material for MkDocsInsiders <pre><code>name: ci # (1)!\non:\npush:\nbranches:\n- master # (2)!\n- main\npermissions:\ncontents: write\njobs:\ndeploy:\nruns-on: ubuntu-latest\nsteps:\n- uses: actions/checkout@v3\n- uses: actions/setup-python@v4\nwith:\npython-version: 3.x\n- uses: actions/cache@v2\nwith:\nkey: ${{ github.ref }}\npath: .cache\n- run: pip install mkdocs-material # (3)!\n- run: mkdocs gh-deploy --force\n</code></pre> <ol> <li> <p>You can change the name to your liking.</p> </li> <li> <p>At some point, GitHub renamed <code>master</code> to <code>main</code>. If your default branch     is named <code>master</code>, you can safely remove <code>main</code>, vice versa.</p> </li> <li> <p>This is the place to install further [MkDocs plugins] or Markdown     extensions with <code>pip</code> to be used during the build:</p> <pre><code>pip install \\\nmkdocs-material \\\nmkdocs-awesome-pages-plugin \\\n...\n</code></pre> </li> </ol> <pre><code>name: ci\non:\npush:\nbranches:\n- master\n- main\npermissions:\ncontents: write\njobs:\ndeploy:\nruns-on: ubuntu-latest\nif: github.event.repository.fork == false\nsteps:\n- uses: actions/checkout@v3\n- uses: actions/setup-python@v4\nwith:\npython-version: 3.x\n- uses: actions/cache@v2\nwith:\nkey: ${{ github.ref }}\npath: .cache\n- run: apt-get install pngquant # (1)!\n- run: pip install git+https://${GH_TOKEN}@github.com/squidfunk/mkdocs-material-insiders.git\n- run: mkdocs gh-deploy --force\nenv:\nGH_TOKEN: ${{ secrets.GH_TOKEN }} # (2)!\n</code></pre>"},{"location":"reference/notes/#code-blocks-annotations","title":"Code blocks annotations","text":"<p>Material for MkDocs is published as a [Python package] and can be installed with <code>pip</code>, ideally by using a [virtual environment]. Open up a terminal and install Material for MkDocs with:</p> <pre><code>theme:\nfeatures:\n- content.code.annotate # (1)\n</code></pre> <ol> <li>\ud83d\udc77 I'm a code annotation! I can contain <code>code</code>, formatted     text, images, ... basically anything that can be written in Markdown.</li> </ol>"},{"location":"reference/notes/#versiontags-flags","title":"Version/tags flags","text":"<p>[ 9.0.0][Code copy button support] \u00b7  Feature flag</p>"},{"location":"reference/notes/#adding-a-title-to-a-code-block","title":"Adding a title to a code-block","text":"bubble_sort.py<pre><code>def bubble_sort(items):\nfor i in range(len(items)):\nfor j in range(len(items) - 1 - i):\nif items[j] &gt; items[j + 1]:\nitems[j], items[j + 1] = items[j + 1], items[j]\n</code></pre>"},{"location":"reference/notes/#hover-important-information","title":"Hover, Important information","text":"<p>TODO:</p> Icon with tooltip<pre><code> {title=\"Important information\" }\n</code></pre> <p></p> <p>hello this is important.</p> <p></p>"},{"location":"reference/notes/#adding-footnotes","title":"Adding footnotes","text":"<p>This is how you add footnotes.</p>"},{"location":"reference/notes/#add-buttons","title":"Add buttons","text":"<p>Subscribe to our newsletter</p> <p>another button</p> <p>Subscribe to our newsletter</p>"},{"location":"reference/notes/#data-tables","title":"Data tables","text":"This is table 1<pre><code>| Method      | Description                          |\n| ----------- | ------------------------------------ |\n| `GET`       | :material-check:     Fetch resource  |\n| `PUT`       | :material-check-all: Update resource |\n| `DELETE`    | :material-close:     Delete resource |\n</code></pre> Method Description <code>GET</code>      Fetch resource <code>PUT</code>  Update resource <code>DELETE</code>      Delete resource"},{"location":"reference/notes/#text-highlighting","title":"Text highlighting","text":"<ul> <li>This was marked</li> <li>This was inserted</li> <li>This was deleted</li> </ul>"},{"location":"reference/notes/#text-block-with-title","title":"Text block with title","text":"<p>Emoji<pre><code>Here you can add anything..\n</code></pre> </p>"},{"location":"reference/notes/#mermaid","title":"Mermaid","text":""},{"location":"reference/notes/#sequence-diagram","title":"Sequence Diagram","text":"<pre><code>sequenceDiagram\n    actor       user     as User\n    participant p   as Button\n\n    user     -&gt;&gt;  p: click</code></pre>"},{"location":"reference/notes/#flowchart","title":"Flowchart","text":"<pre><code>graph TD\n    A[Hard edge] --&gt;|Link text| B(Round edge)\n    B --&gt; C{Decision}\n    C --&gt;|One| D[Result one]\n    C --&gt;|Two| E[Result two]</code></pre>"},{"location":"reference/notes/#another-chart","title":"Another chart","text":"<pre><code>flowchart TD\n    Start --&gt;|Decision 1| A\n    A --&gt;|Yes| B\n    A --&gt;|No| C\n    B --&gt;|Action 1| End\n    C --&gt;|Decision 2| D\n    D --&gt;|Yes| End\n    D --&gt;|No| A</code></pre>"},{"location":"reference/notes/#pi-chart","title":"Pi chart","text":"<pre><code>pie title Accuracy\n    \"Dogs\" : 386\n    \"Cats\" : 85\n    \"Rats\" : 15</code></pre>"},{"location":"reference/notes/#user-journey","title":"User Journey","text":"<pre><code>---\ntitle: My working day\n---\njourney\n    section Go to work\n      Make tea: 5: Me\n      Go upstairs: 3: Me\n      Do work: 1: Me, Cat\n    section Go home\n      Go downstairs: 5: Me\n      Sit down: 5: Me</code></pre>"},{"location":"reference/notes/#class-diagram","title":"Class Diagram","text":"<pre><code>---\ntitle: Animal example\n---\nclassDiagram\n    note \"From Duck till Zebra\"\n    Animal &lt;|-- Duck\n    note for Duck \"can fly\\ncan swim\\ncan dive\\ncan help in debugging\"\n    Animal &lt;|-- Fish\n    Animal &lt;|-- Zebra\n    Animal : +int age\n    Animal : +String gender\n    Animal: +isMammal()\n    Animal: +mate()\n    class Duck{\n        +String beakColor\n        +swim()\n        +quack()\n    }\n    class Fish{\n        -int sizeInFeet\n        -canEat()\n    }\n    class Zebra{\n        +bool is_wild\n        +run()\n    }</code></pre>"},{"location":"reference/notes/#sequence-diagram_1","title":"Sequence Diagram","text":"<pre><code>---\ntitle: Sequence Diagram Example\n---\nsequenceDiagram\n    Consumer--&gt;API: Book something\n    API--&gt;BookingService: Start booking process\n    break when the booking process fails\n        API--&gt;Consumer: show failure\n    end\n    API--&gt;BillingService: Start billing process\n</code></pre> <ol> <li> <p>This is a footnote.\u00a0\u21a9</p> </li> <li> <p>This is another footnote.\u00a0\u21a9</p> </li> </ol>"},{"location":"reference/reference/","title":"<code>visionai</code>","text":"<p>VisionAI Toolkit</p> <p>VisionAI tookit provides a large number of ready-to-deploy scenarios built using latest computer vision frameworks. Supports many of the common workplace health and safety use-cases.</p> <p>Start by exploring scenarios through visionai scenario list command. After that, you can create a pipeline through the pipeline commands. Once a pipeline is configured, you can run the pipeline on the any number of cameras.</p> <p>Running the toolkit does assume a NVIDIA GPU powered machine for efficient performance. Please see the system requirements on the documentation.</p> <p>You can instead opt to install it through Azure Managed VM, with preconfigured machines &amp; recommended hardware support. You can find information about this on our documentation website.</p> <p>Visit https://docs.visionify.ai for more details.</p> <p>Usage:</p> <pre><code>$ visionai [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <ul> <li><code>--verbose</code>: [default: False]</li> <li><code>--version</code>: [default: False]</li> <li><code>--install-completion</code>: Install completion for the current shell.</li> <li><code>--show-completion</code>: Show completion for the current shell, to copy it or customize the installation.</li> <li><code>--help</code>: Show this message and exit.</li> </ul> <p>Commands:</p> <pre><code>\u256d\u2500 Commands \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 auth         Authentication commands                           \u2502\n\u2502 camera       Add/remove/manage cameras                         \u2502\n\u2502 device       Device commands                                   \u2502\n\u2502 init         Initialize VisionAI library                       \u2502\n\u2502 model        Manage models                                     \u2502\n\u2502 pipeline     Manage pipelines                                  \u2502\n\u2502 scenario     Add/remove scenarios to camera                    \u2502\n\u2502 status       Print status of all running containers.           \u2502\n\u2502 stop         Stop all running containers.                      \u2502\n\u2502 web          Start/stop web server                             \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"reference/reference/#visionai-auth","title":"<code>visionai auth</code>","text":"<p>Authorization (logging in/out)</p> <p>Login and get authorization token etc.</p> <p>You can login/logout check authorization token with this.</p> <p>Usage:</p> <pre><code>$ visionai auth [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul> <p>Commands:</p> <ul> <li><code>login</code>: Login with an application token.</li> <li><code>logout</code>: Logout from your session Get the auth token...</li> <li><code>status</code>: Check login status Check the current login...</li> </ul>"},{"location":"reference/reference/#visionai-auth-login","title":"<code>visionai auth login</code>","text":"<p>Login with an application token.</p> <p>Get the auth token from our website</p> <p>Usage:</p> <pre><code>$ visionai auth login [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--token TEXT</code>: Authenticate the app through token  [required]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-auth-logout","title":"<code>visionai auth logout</code>","text":"<p>Logout from your session</p> <p>Get the auth token from our website</p> <p>Usage:</p> <pre><code>$ visionai auth logout [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-auth-status","title":"<code>visionai auth status</code>","text":"<p>Check login status</p> <p>Check the current login system.</p> <p>Usage:</p> <pre><code>$ visionai auth status [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-camera","title":"<code>visionai camera</code>","text":"<p>Manage cameras</p> <p>An organization can have multiple cameras that are installed at different places. They may be from different vendors and/or maybe using different security surveillance software. Most cameras however do support RTSP, RTMP or HLS streams as an output. Please refer to your camera vendor documentation to find this out. This module will help you onboard those cameras on visionai systems by using a simple named instance for each camera.</p> <p>Usage:</p> <pre><code>$ visionai camera [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul> <p>Commands:</p> <ul> <li><code>add</code>: Add a named camera instance Add a camera as a...</li> <li><code>add-scenario</code>: Add a scenario for a camera Add an individual...</li> <li><code>list</code>: List available cameras Print cameras...</li> <li><code>list-scenario</code>: List scenarios configured for a camera</li> <li><code>list-scenarios</code>: List scenarios configured for a camera</li> <li><code>preview</code>: Preview the camera system View the camera...</li> <li><code>remove</code>: Remove a camera from the system Specify a...</li> <li><code>remove-scenario</code>: Remove a scenario from a camera Specify a...</li> <li><code>reset</code>: Reset all camera configuration.</li> </ul>"},{"location":"reference/reference/#visionai-camera-add","title":"<code>visionai camera add</code>","text":"<p>Add a named camera instance</p> <p>Add a camera as a named instance in the system. For adding a camera we support RTSP, HLS, HTTP(S) systems. To add a camera you need to provide a name for the camera, URI for the camera (including any username/password within the URI itself), description for camera (about its location, where its pointing, who is the vendor etc.).</p> <p>Before the camera is added - we need to test out if the camera instance is valid. We need to be able to read from the camera and calculate its FPS. Show this information on the screen.</p> <p>Usage:</p> <pre><code>$ visionai camera add [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--name TEXT</code>: Camera Name  [required]</li> <li><code>--uri TEXT</code>: URI for camera  [required]</li> <li><code>--description TEXT</code>: Description  [required]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-camera-add-scenario","title":"<code>visionai camera add-scenario</code>","text":"<p>Add a scenario for a camera</p> <p>Add an individual scenario to be run for a camera. Specify the names for scenario and camera.</p> <p>Usage:</p> <pre><code>$ visionai camera add-scenario [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--camera TEXT</code>: camera name  [required]</li> <li><code>--scenario TEXT</code>: scenario name  [required]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-camera-list","title":"<code>visionai camera list</code>","text":"<p>List available cameras</p> <p>Print cameras available in the system and the scenarios / routines that are set up for them.</p> <p>Usage:</p> <pre><code>$ visionai camera list [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-camera-list-scenario","title":"<code>visionai camera list-scenario</code>","text":"<p>List scenarios configured for a camera</p> <p>Usage:</p> <pre><code>$ visionai camera list-scenario [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--camera TEXT</code>: Camera name  [default: ]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-camera-list-scenarios","title":"<code>visionai camera list-scenarios</code>","text":"<p>List scenarios configured for a camera</p> <p>Usage:</p> <pre><code>$ visionai camera list-scenarios [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--camera TEXT</code>: Camera name  [default: ]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-camera-preview","title":"<code>visionai camera preview</code>","text":"<p>Preview the camera system</p> <p>View the camera feed, review FPS etc available for camera.</p> <p>Usage:</p> <pre><code>$ visionai camera preview [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--name TEXT</code>: camera name to preview  [required]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-camera-remove","title":"<code>visionai camera remove</code>","text":"<p>Remove a camera from the system</p> <p>Specify a named camera that needs to be removed from the system. Once removed, all the scenarios and pre-process routines associated with the camera will be removed.</p> <p>Usage:</p> <pre><code>$ visionai camera remove [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--name TEXT</code>: [default: Camera name]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-camera-remove-scenario","title":"<code>visionai camera remove-scenario</code>","text":"<p>Remove a scenario from a camera</p> <p>Specify a named scenario that needs to be removed from the system. Once removed, all the scenarios and pre-process routines associated with the scenario will be removed.</p> <p>Usage:</p> <pre><code>$ visionai camera remove-scenario [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--camera TEXT</code>: camera  [required]</li> <li><code>--scenario TEXT</code>: scenario name  [required]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-camera-reset","title":"<code>visionai camera reset</code>","text":"<p>Reset all camera configuration.</p> <p>All cameras and their scenarios would be removed from the system. Any earlier configuration is backed up as a timed json backup file.</p> <p>Usage:</p> <pre><code>$ visionai camera reset [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--confirm / --no-confirm</code>: Confirm delete  [default: False]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-cameras","title":"<code>visionai cameras</code>","text":"<p>... alias for camera</p> <p>Usage:</p> <pre><code>$ visionai cameras [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul> <p>Commands:</p> <ul> <li><code>add</code>: Add a named camera instance Add a camera as a...</li> <li><code>add-scenario</code>: Add a scenario for a camera Add an individual...</li> <li><code>list</code>: List available cameras Print cameras...</li> <li><code>list-scenario</code>: List scenarios configured for a camera</li> <li><code>list-scenarios</code>: List scenarios configured for a camera</li> <li><code>preview</code>: Preview the camera system View the camera...</li> <li><code>remove</code>: Remove a camera from the system Specify a...</li> <li><code>remove-scenario</code>: Remove a scenario from a camera Specify a...</li> <li><code>reset</code>: Reset all camera configuration.</li> </ul>"},{"location":"reference/reference/#visionai-cameras-add","title":"<code>visionai cameras add</code>","text":"<p>Add a named camera instance</p> <p>Add a camera as a named instance in the system. For adding a camera we support RTSP, HLS, HTTP(S) systems. To add a camera you need to provide a name for the camera, URI for the camera (including any username/password within the URI itself), description for camera (about its location, where its pointing, who is the vendor etc.).</p> <p>Before the camera is added - we need to test out if the camera instance is valid. We need to be able to read from the camera and calculate its FPS. Show this information on the screen.</p> <p>Usage:</p> <pre><code>$ visionai cameras add [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--name TEXT</code>: Camera Name  [required]</li> <li><code>--uri TEXT</code>: URI for camera  [required]</li> <li><code>--description TEXT</code>: Description  [required]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-cameras-add-scenario","title":"<code>visionai cameras add-scenario</code>","text":"<p>Add a scenario for a camera</p> <p>Add an individual scenario to be run for a camera. Specify the names for scenario and camera.</p> <p>Usage:</p> <pre><code>$ visionai cameras add-scenario [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--camera TEXT</code>: camera name  [required]</li> <li><code>--scenario TEXT</code>: scenario name  [required]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-cameras-list","title":"<code>visionai cameras list</code>","text":"<p>List available cameras</p> <p>Print cameras available in the system and the scenarios / routines that are set up for them.</p> <p>Usage:</p> <pre><code>$ visionai cameras list [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-cameras-list-scenario","title":"<code>visionai cameras list-scenario</code>","text":"<p>List scenarios configured for a camera</p> <p>Usage:</p> <pre><code>$ visionai cameras list-scenario [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--camera TEXT</code>: Camera name  [default: ]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-cameras-list-scenarios","title":"<code>visionai cameras list-scenarios</code>","text":"<p>List scenarios configured for a camera</p> <p>Usage:</p> <pre><code>$ visionai cameras list-scenarios [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--camera TEXT</code>: Camera name  [default: ]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-cameras-preview","title":"<code>visionai cameras preview</code>","text":"<p>Preview the camera system</p> <p>View the camera feed, review FPS etc available for camera.</p> <p>Usage:</p> <pre><code>$ visionai cameras preview [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--name TEXT</code>: camera name to preview  [required]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-cameras-remove","title":"<code>visionai cameras remove</code>","text":"<p>Remove a camera from the system</p> <p>Specify a named camera that needs to be removed from the system. Once removed, all the scenarios and pre-process routines associated with the camera will be removed.</p> <p>Usage:</p> <pre><code>$ visionai cameras remove [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--name TEXT</code>: [default: Camera name]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-cameras-remove-scenario","title":"<code>visionai cameras remove-scenario</code>","text":"<p>Remove a scenario from a camera</p> <p>Specify a named scenario that needs to be removed from the system. Once removed, all the scenarios and pre-process routines associated with the scenario will be removed.</p> <p>Usage:</p> <pre><code>$ visionai cameras remove-scenario [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--camera TEXT</code>: camera  [required]</li> <li><code>--scenario TEXT</code>: scenario name  [required]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-cameras-reset","title":"<code>visionai cameras reset</code>","text":"<p>Reset all camera configuration.</p> <p>All cameras and their scenarios would be removed from the system. Any earlier configuration is backed up as a timed json backup file.</p> <p>Usage:</p> <pre><code>$ visionai cameras reset [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--confirm / --no-confirm</code>: Confirm delete  [default: False]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-device","title":"<code>visionai device</code>","text":"<p>Manage device features</p> <p>Since scenarios run on individual edge-devices, and we don't have enough control over the CPU, Memory, GPU statistics - it is imperative that we have strong methods for validating if a scenario can run on a chosen platform. This module provides many utilities to check CPU, Memory and GPU statistics for the edge device. We also provide an Azure Managed service where these scenarios can be configured and run on your premise on pre-validated VM machines.</p> <p>Usage:</p> <pre><code>$ visionai device [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul> <p>Commands:</p> <ul> <li><code>list</code>: List available devices Get a list of all...</li> <li><code>modules</code>: List running modules on the device Again this...</li> <li><code>select</code>: Select a device Not sure why is this needed...</li> <li><code>stats</code>: Machine health (GPU/Mem stats) Show machine...</li> </ul>"},{"location":"reference/reference/#visionai-device-list","title":"<code>visionai device list</code>","text":"<p>List available devices</p> <p>Get a list of all available [processing] devices</p> <p>Usage:</p> <pre><code>$ visionai device list [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-device-modules","title":"<code>visionai device modules</code>","text":"<p>List running modules on the device</p> <p>Again this does not make much sense at this time. Let's revisit.</p> <p>Usage:</p> <pre><code>$ visionai device modules [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-device-select","title":"<code>visionai device select</code>","text":"<p>Select a device</p> <p>Not sure why is this needed at this time.</p> <p>Usage:</p> <pre><code>$ visionai device select [OPTIONS] DEVICE\n</code></pre> <p>Arguments:</p> <ul> <li><code>DEVICE</code>: [required]</li> </ul> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-device-stats","title":"<code>visionai device stats</code>","text":"<p>Machine health (GPU/Mem stats)</p> <p>Show machine health (GPU/memory stats). This can be used to determine if more scenarios can be run on the machine or not.</p> <p>Usage:</p> <pre><code>$ visionai device stats [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-devices","title":"<code>visionai devices</code>","text":"<p>... alias for device</p> <p>Usage:</p> <pre><code>$ visionai devices [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul> <p>Commands:</p> <ul> <li><code>list</code>: List available devices Get a list of all...</li> <li><code>modules</code>: List running modules on the device Again this...</li> <li><code>select</code>: Select a device Not sure why is this needed...</li> <li><code>stats</code>: Machine health (GPU/Mem stats) Show machine...</li> </ul>"},{"location":"reference/reference/#visionai-devices-list","title":"<code>visionai devices list</code>","text":"<p>List available devices</p> <p>Get a list of all available [processing] devices</p> <p>Usage:</p> <pre><code>$ visionai devices list [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-devices-modules","title":"<code>visionai devices modules</code>","text":"<p>List running modules on the device</p> <p>Again this does not make much sense at this time. Let's revisit.</p> <p>Usage:</p> <pre><code>$ visionai devices modules [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-devices-select","title":"<code>visionai devices select</code>","text":"<p>Select a device</p> <p>Not sure why is this needed at this time.</p> <p>Usage:</p> <pre><code>$ visionai devices select [OPTIONS] DEVICE\n</code></pre> <p>Arguments:</p> <ul> <li><code>DEVICE</code>: [required]</li> </ul> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-devices-stats","title":"<code>visionai devices stats</code>","text":"<p>Machine health (GPU/Mem stats)</p> <p>Show machine health (GPU/memory stats). This can be used to determine if more scenarios can be run on the machine or not.</p> <p>Usage:</p> <pre><code>$ visionai devices stats [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-model","title":"<code>visionai model</code>","text":"<p>Serve models</p> <p>Before we can run any scenarios - the models necessary for them must be ready. We use Triton inference server to make the best use of GPU/CPU resources available on the machine in order to serve our models. Any models that are available in models-repo folder would be served after this (TODO - only serve models configured in scenarios).</p> <p>Usage:</p> <pre><code>$ visionai model [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul> <p>Commands:</p> <ul> <li><code>check</code>: Check model-server status &amp; print helpful...</li> <li><code>serve</code>: Start serving all available models.</li> <li><code>start</code>: Start serving all available models.</li> <li><code>status</code>: Show the status of serving models Shows how...</li> <li><code>stop</code>: Stop serving all models.</li> </ul>"},{"location":"reference/reference/#visionai-model-check","title":"<code>visionai model check</code>","text":"<p>Check model-server status &amp; print helpful debug info.</p> <p>TODO: Goal of the check command is to identify any configuration/dependency issues that we can inform to user that he can fix on his end. This could be like missing dependency, missing software package, missing driver details etc.</p> <ul> <li>Check if model-server is running or not.</li> <li>Check if triton-client can access model-server</li> <li>Check what are the models served</li> <li>Print all of this in a pretty manner [checkbox based]</li> <li>Check container logs &amp; show them here.</li> <li>grep container logs for common errors &amp; highlight that in output</li> </ul> <p>Usage:</p> <pre><code>$ visionai model check [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-model-serve","title":"<code>visionai model serve</code>","text":"<p>Start serving all available models.</p> <p>All models present in the models-repo/ will be served. We use triton inference server to serve them. The triton server will be at http://localhost:8000, grpc://localhost:8001.</p> <p>Please make sure these two ports are not used by anyone else.</p> <p>Usage:</p> <pre><code>$ visionai model serve [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-model-start","title":"<code>visionai model start</code>","text":"<p>Start serving all available models.</p> <p>All models present in the models-repo/ will be served. We use triton inference server to serve them. The triton server will be at http://localhost:8000, grpc://localhost:8001.</p> <p>Please make sure these two ports are not used by anyone else.</p> <p>Usage:</p> <pre><code>$ visionai model start [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-model-status","title":"<code>visionai model status</code>","text":"<p>Show the status of serving models</p> <p>Shows how many models are being served, metrics for the models etc.</p> <p>Usage:</p> <pre><code>$ visionai model status [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-model-stop","title":"<code>visionai model stop</code>","text":"<p>Stop serving all models.</p> <p>This method will stop serving all models. Any inference running will all be stopped as well.</p> <p>Usage:</p> <pre><code>$ visionai model stop [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-models","title":"<code>visionai models</code>","text":"<p>... alias for model</p> <p>Usage:</p> <pre><code>$ visionai models [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul> <p>Commands:</p> <ul> <li><code>check</code>: Check model-server status &amp; print helpful...</li> <li><code>serve</code>: Start serving all available models.</li> <li><code>start</code>: Start serving all available models.</li> <li><code>status</code>: Show the status of serving models Shows how...</li> <li><code>stop</code>: Stop serving all models.</li> </ul>"},{"location":"reference/reference/#visionai-models-check","title":"<code>visionai models check</code>","text":"<p>Check model-server status &amp; print helpful debug info.</p> <p>TODO: Goal of the check command is to identify any configuration/dependency issues that we can inform to user that he can fix on his end. This could be like missing dependency, missing software package, missing driver details etc.</p> <ul> <li>Check if model-server is running or not.</li> <li>Check if triton-client can access model-server</li> <li>Check what are the models served</li> <li>Print all of this in a pretty manner [checkbox based]</li> <li>Check container logs &amp; show them here.</li> <li>grep container logs for common errors &amp; highlight that in output</li> </ul> <p>Usage:</p> <pre><code>$ visionai models check [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-models-serve","title":"<code>visionai models serve</code>","text":"<p>Start serving all available models.</p> <p>All models present in the models-repo/ will be served. We use triton inference server to serve them. The triton server will be at http://localhost:8000, grpc://localhost:8001.</p> <p>Please make sure these two ports are not used by anyone else.</p> <p>Usage:</p> <pre><code>$ visionai models serve [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-models-start","title":"<code>visionai models start</code>","text":"<p>Start serving all available models.</p> <p>All models present in the models-repo/ will be served. We use triton inference server to serve them. The triton server will be at http://localhost:8000, grpc://localhost:8001.</p> <p>Please make sure these two ports are not used by anyone else.</p> <p>Usage:</p> <pre><code>$ visionai models start [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-models-status","title":"<code>visionai models status</code>","text":"<p>Show the status of serving models</p> <p>Shows how many models are being served, metrics for the models etc.</p> <p>Usage:</p> <pre><code>$ visionai models status [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-models-stop","title":"<code>visionai models stop</code>","text":"<p>Stop serving all models.</p> <p>This method will stop serving all models. Any inference running will all be stopped as well.</p> <p>Usage:</p> <pre><code>$ visionai models stop [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-pipeline","title":"<code>visionai pipeline</code>","text":"<p>Manage pipelines</p> <p>Pipeline is a sequence of preprocess routines and scenarios to be run on a given set of cameras. Each pipeline can be configured to run specific scenarios - each scenario with their own customizations for event notifications. This module provides robust methods for managing pipelines, showing their details, adding/remove cameras from pipelines and running a pipeline.</p> <p>Usage:</p> <pre><code>$ visionai pipeline [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul> <p>Commands:</p> <ul> <li><code>add-camera</code>: Add a camera to a pipeline Each pipeline...</li> <li><code>add-preprocess</code>: Add a preprocess routine to a pipeline...</li> <li><code>add-scenario</code>: Add a scenario to a pipeline The order of the...</li> <li><code>create</code>: Create a named pipeline Create a named...</li> <li><code>remove-camera</code>: Remove a camera from a pipeline This method...</li> <li><code>reset</code>: Reset the pipeline to original state.</li> <li><code>run</code>: Run a pipeline of scenarios on given cameras...</li> <li><code>show</code>: Show details of a pipeline Show what is...</li> </ul>"},{"location":"reference/reference/#visionai-pipeline-add-camera","title":"<code>visionai pipeline add-camera</code>","text":"<p>Add a camera to a pipeline</p> <p>Each pipeline consists of a bunch of scenarios to run and which cameras they need to be run on. This method allows the user to add one or more named camera instance to a pipeline. Please note the camera instance has to be created prior to adding it here.</p>"},{"location":"reference/reference/#add-a-camera","title":"add a camera","text":"<p>$ visionai camera add --name OFFICE-01 --uri https://youtube.com</p>"},{"location":"reference/reference/#add-camera-to-pipeline","title":"add camera to pipeline","text":"<p>$ visionai pipeline --name test_pipe add-camera --name OFFICE-01</p> <p>@arg pipeline - specify a named pipeline @arg camera - specify name of the camera to add</p> <p>@return None</p> <p>Usage:</p> <pre><code>$ visionai pipeline add-camera [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--pipeline TEXT</code>: pipeline name  [required]</li> <li><code>--camera TEXT</code>: camera to add  [required]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-pipeline-add-preprocess","title":"<code>visionai pipeline add-preprocess</code>","text":"<p>Add a preprocess routine to a pipeline</p> <p>Preprocessing tasks are run prior to scenarios. The order in which multiple preprocess tasks are added does not matter. All added preprocess routines are executed in different threads.</p> <p>$ visionai pipeline --name test_pipe add-preprocess --name face-blur</p> <p>$ visionai pipeline --name test_pipe add-preprocess --name text-blur</p> <p>$ visionai pipeline --name test_pipe show</p> <p>$ visionai pipeline --name test_pipe run</p> <p>@arg pipeline - specify a named pipeline @arg preprocess - specify name of the preprocess task to run</p> <p>@return None</p> <p>Usage:</p> <pre><code>$ visionai pipeline add-preprocess [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--pipeline TEXT</code>: pipeline name  [required]</li> <li><code>--preprocess TEXT</code>: preprocess routine to add  [required]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-pipeline-add-scenario","title":"<code>visionai pipeline add-scenario</code>","text":"<p>Add a scenario to a pipeline</p> <p>The order of the scenarios does not matter. All added scenarios are run in different threads. All scenarios are run after pre-processing stage is done.</p> <p>$visionai pipeline --name test_pipe add-scenario --name smoke-and-fire</p> <p>$visionai pipeline --name test_pipe add-scenario --name ppe-detection</p> <p>$visionai pipeline --name test_pipe run</p> <p>@arg pipeline - specify a named pipeline @arg scenario - specify name of the scenario to run</p> <p>@return None</p> <p>Usage:</p> <pre><code>$ visionai pipeline add-scenario [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--pipeline TEXT</code>: pipeline name  [required]</li> <li><code>--scenario TEXT</code>: scenario to add  [required]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-pipeline-create","title":"<code>visionai pipeline create</code>","text":"<p>Create a named pipeline</p> <p>Create a named pipeline. Pipeline is a list of scenarios to be run for specific cameras. The flow is as follows. Create a pipeline using:</p> <p>visionai pipeline create --name test_pipe</p> <p>visionai pipeline add-scenario --pipeline test_pipe  --name smoke-and-fire</p> <p>visionai pipeline add-scenario --pipeline test_pipe  --name ppe-detection</p> <p>visionai pipeline add-preprocess --pipeline test_pipe  --name face-blur</p> <p>visionai pipeline add-preprocess --pipeline test_pipe  --name text-blur</p> <p>visionai pipeline add-scenario --pipeline test_pipe  --name max-occupancy</p> <p>visionai pipeline show --pipeline test_pipe</p> <p>visionai pipeline add-camera --pipeline test_pipe  --name CAMERA-01</p> <p>visionai pipeline add-camera --pipeline test_pipe  --name CAMERA-02</p> <p>visionai pipeline show --pipeline test_pipe</p> <p>visionai pipeline run --pipeline test_pipe</p> <p>@arg pipeline - specify a named pipeline</p> <p>@return None</p> <p>Usage:</p> <pre><code>$ visionai pipeline create [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--name TEXT</code>: pipeline name  [required]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-pipeline-remove-camera","title":"<code>visionai pipeline remove-camera</code>","text":"<p>Remove a camera from a pipeline</p> <p>This method can be used to remove a camera from a pipeline.</p> <p>$ visionai pipeline --name test_pipe remove-camera --name OFFICE-01</p> <p>@arg pipeline - specify a named pipeline @arg camera - specify name of the camera to remove</p> <p>@return None</p> <p>Usage:</p> <pre><code>$ visionai pipeline remove-camera [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--pipeline TEXT</code>: pipeline name  [required]</li> <li><code>--camera TEXT</code>: camera to remove  [required]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-pipeline-reset","title":"<code>visionai pipeline reset</code>","text":"<p>Reset the pipeline to original state.</p> <p>Deletes all cameras, scenarios and scenario configuration from the pipeline. Its as if the pipeline has been deleted and created from scratch again.</p> <p>$ visionai pipeline --name test_pipe reset</p> <p>@arg pipeline - pipeline to reset</p> <p>@return None</p> <p>Usage:</p> <pre><code>$ visionai pipeline reset [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--pipeline TEXT</code>: pipeline name  [required]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-pipeline-run","title":"<code>visionai pipeline run</code>","text":"<p>Run a pipeline of scenarios on given cameras</p> <p>Specify different scenarios to run on one or more cameras. This method can be directly used to specify scenarios and cameras directly. Else you can configure a named pipeline and then run it here.</p> <p>@arg pipeline - specify a named pipeline</p> <p>@return None</p> <p>Usage:</p> <pre><code>$ visionai pipeline run [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--pipeline TEXT</code>: Pipeline to run  [required]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-pipeline-show","title":"<code>visionai pipeline show</code>","text":"<p>Show details of a pipeline</p> <p>Show what is configured in the current pipeline.</p> <p>$ visionai pipeline --name test_pipe show</p> <p>@arg pipeline - specify a named pipeline</p> <p>@return None</p> <p>Usage:</p> <pre><code>$ visionai pipeline show [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--pipeline TEXT</code>: pipeline name  [required]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-pipelines","title":"<code>visionai pipelines</code>","text":"<p>... alias for pipeline</p> <p>Usage:</p> <pre><code>$ visionai pipelines [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul> <p>Commands:</p> <ul> <li><code>add-camera</code>: Add a camera to a pipeline Each pipeline...</li> <li><code>add-preprocess</code>: Add a preprocess routine to a pipeline...</li> <li><code>add-scenario</code>: Add a scenario to a pipeline The order of the...</li> <li><code>create</code>: Create a named pipeline Create a named...</li> <li><code>remove-camera</code>: Remove a camera from a pipeline This method...</li> <li><code>reset</code>: Reset the pipeline to original state.</li> <li><code>run</code>: Run a pipeline of scenarios on given cameras...</li> <li><code>show</code>: Show details of a pipeline Show what is...</li> </ul>"},{"location":"reference/reference/#visionai-pipelines-add-camera","title":"<code>visionai pipelines add-camera</code>","text":"<p>Add a camera to a pipeline</p> <p>Each pipeline consists of a bunch of scenarios to run and which cameras they need to be run on. This method allows the user to add one or more named camera instance to a pipeline. Please note the camera instance has to be created prior to adding it here.</p>"},{"location":"reference/reference/#add-a-camera_1","title":"add a camera","text":"<p>$ visionai camera add --name OFFICE-01 --uri https://youtube.com</p>"},{"location":"reference/reference/#add-camera-to-pipeline_1","title":"add camera to pipeline","text":"<p>$ visionai pipeline --name test_pipe add-camera --name OFFICE-01</p> <p>@arg pipeline - specify a named pipeline @arg camera - specify name of the camera to add</p> <p>@return None</p> <p>Usage:</p> <pre><code>$ visionai pipelines add-camera [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--pipeline TEXT</code>: pipeline name  [required]</li> <li><code>--camera TEXT</code>: camera to add  [required]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-pipelines-add-preprocess","title":"<code>visionai pipelines add-preprocess</code>","text":"<p>Add a preprocess routine to a pipeline</p> <p>Preprocessing tasks are run prior to scenarios. The order in which multiple preprocess tasks are added does not matter. All added preprocess routines are executed in different threads.</p> <p>$ visionai pipeline --name test_pipe add-preprocess --name face-blur</p> <p>$ visionai pipeline --name test_pipe add-preprocess --name text-blur</p> <p>$ visionai pipeline --name test_pipe show</p> <p>$ visionai pipeline --name test_pipe run</p> <p>@arg pipeline - specify a named pipeline @arg preprocess - specify name of the preprocess task to run</p> <p>@return None</p> <p>Usage:</p> <pre><code>$ visionai pipelines add-preprocess [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--pipeline TEXT</code>: pipeline name  [required]</li> <li><code>--preprocess TEXT</code>: preprocess routine to add  [required]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-pipelines-add-scenario","title":"<code>visionai pipelines add-scenario</code>","text":"<p>Add a scenario to a pipeline</p> <p>The order of the scenarios does not matter. All added scenarios are run in different threads. All scenarios are run after pre-processing stage is done.</p> <p>$visionai pipeline --name test_pipe add-scenario --name smoke-and-fire</p> <p>$visionai pipeline --name test_pipe add-scenario --name ppe-detection</p> <p>$visionai pipeline --name test_pipe run</p> <p>@arg pipeline - specify a named pipeline @arg scenario - specify name of the scenario to run</p> <p>@return None</p> <p>Usage:</p> <pre><code>$ visionai pipelines add-scenario [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--pipeline TEXT</code>: pipeline name  [required]</li> <li><code>--scenario TEXT</code>: scenario to add  [required]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-pipelines-create","title":"<code>visionai pipelines create</code>","text":"<p>Create a named pipeline</p> <p>Create a named pipeline. Pipeline is a list of scenarios to be run for specific cameras. The flow is as follows. Create a pipeline using:</p> <p>visionai pipeline create --name test_pipe</p> <p>visionai pipeline add-scenario --pipeline test_pipe  --name smoke-and-fire</p> <p>visionai pipeline add-scenario --pipeline test_pipe  --name ppe-detection</p> <p>visionai pipeline add-preprocess --pipeline test_pipe  --name face-blur</p> <p>visionai pipeline add-preprocess --pipeline test_pipe  --name text-blur</p> <p>visionai pipeline add-scenario --pipeline test_pipe  --name max-occupancy</p> <p>visionai pipeline show --pipeline test_pipe</p> <p>visionai pipeline add-camera --pipeline test_pipe  --name CAMERA-01</p> <p>visionai pipeline add-camera --pipeline test_pipe  --name CAMERA-02</p> <p>visionai pipeline show --pipeline test_pipe</p> <p>visionai pipeline run --pipeline test_pipe</p> <p>@arg pipeline - specify a named pipeline</p> <p>@return None</p> <p>Usage:</p> <pre><code>$ visionai pipelines create [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--name TEXT</code>: pipeline name  [required]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-pipelines-remove-camera","title":"<code>visionai pipelines remove-camera</code>","text":"<p>Remove a camera from a pipeline</p> <p>This method can be used to remove a camera from a pipeline.</p> <p>$ visionai pipeline --name test_pipe remove-camera --name OFFICE-01</p> <p>@arg pipeline - specify a named pipeline @arg camera - specify name of the camera to remove</p> <p>@return None</p> <p>Usage:</p> <pre><code>$ visionai pipelines remove-camera [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--pipeline TEXT</code>: pipeline name  [required]</li> <li><code>--camera TEXT</code>: camera to remove  [required]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-pipelines-reset","title":"<code>visionai pipelines reset</code>","text":"<p>Reset the pipeline to original state.</p> <p>Deletes all cameras, scenarios and scenario configuration from the pipeline. Its as if the pipeline has been deleted and created from scratch again.</p> <p>$ visionai pipeline --name test_pipe reset</p> <p>@arg pipeline - pipeline to reset</p> <p>@return None</p> <p>Usage:</p> <pre><code>$ visionai pipelines reset [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--pipeline TEXT</code>: pipeline name  [required]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-pipelines-run","title":"<code>visionai pipelines run</code>","text":"<p>Run a pipeline of scenarios on given cameras</p> <p>Specify different scenarios to run on one or more cameras. This method can be directly used to specify scenarios and cameras directly. Else you can configure a named pipeline and then run it here.</p> <p>@arg pipeline - specify a named pipeline</p> <p>@return None</p> <p>Usage:</p> <pre><code>$ visionai pipelines run [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--pipeline TEXT</code>: Pipeline to run  [required]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-pipelines-show","title":"<code>visionai pipelines show</code>","text":"<p>Show details of a pipeline</p> <p>Show what is configured in the current pipeline.</p> <p>$ visionai pipeline --name test_pipe show</p> <p>@arg pipeline - specify a named pipeline</p> <p>@return None</p> <p>Usage:</p> <pre><code>$ visionai pipelines show [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--pipeline TEXT</code>: pipeline name  [required]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-scenario","title":"<code>visionai scenario</code>","text":"<p>Manage scenarios</p> <p>An organization can have multiple scenarios that are installed at different places. They may be from different vendors and/or maybe using different security surveillance software. Most scenarios however do support RTSP, RTMP or HLS streams as an output. Please refer to your scenario vendor documentation to find this out. This module will help you onboard those scenarios on visionai systems by using a simple named instance for each scenario.</p> <p>Usage:</p> <pre><code>$ visionai scenario [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul> <p>Commands:</p> <ul> <li><code>download</code>: Download models for scenarios Ex: visionai...</li> <li><code>list</code>: List all scenarios available List all...</li> <li><code>test</code>: Run the scenario locally to test it out.</li> </ul>"},{"location":"reference/reference/#visionai-scenario-download","title":"<code>visionai scenario download</code>","text":"<p>Download models for scenarios</p> <p>Ex: visionai scenario download ppe-detection  # download ppe-detection scenario Ex: visionai scenario download all            # download all configured scenarios for the org Ex: visionai scenario download world          # download all available scenarios</p> <p>Download models for a given scenario, or download models for all scenarios that have been configured.</p> <p>Usage:</p> <pre><code>$ visionai scenario download [OPTIONS] NAME\n</code></pre> <p>Arguments:</p> <ul> <li><code>NAME</code>: [required]</li> </ul> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-scenario-list","title":"<code>visionai scenario list</code>","text":"<p>List all scenarios available</p> <p>List all scenarios available in the system. This includes scenarios that may or maynot be applied to any specific camera.</p> <p>Usage:</p> <pre><code>$ visionai scenario list [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-scenario-test","title":"<code>visionai scenario test</code>","text":"<p>Run the scenario locally to test it out.</p> <ul> <li>Download the model if not available.</li> <li>Pull the model server container image.</li> <li>Start the model server container with this model.</li> <li>Run inference with this model</li> </ul> <p>Usage:</p> <pre><code>$ visionai scenario test [OPTIONS] NAME\n</code></pre> <p>Arguments:</p> <ul> <li><code>NAME</code>: [required]</li> </ul> <p>Options:</p> <ul> <li><code>--camera TEXT</code>: Camera name (default is webcam)  [default: 0]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-scenarios","title":"<code>visionai scenarios</code>","text":"<p>... alias for scenario</p> <p>Usage:</p> <pre><code>$ visionai scenarios [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul> <p>Commands:</p> <ul> <li><code>download</code>: Download models for scenarios Ex: visionai...</li> <li><code>list</code>: List all scenarios available List all...</li> <li><code>test</code>: Run the scenario locally to test it out.</li> </ul>"},{"location":"reference/reference/#visionai-scenarios-download","title":"<code>visionai scenarios download</code>","text":"<p>Download models for scenarios</p> <p>Ex: visionai scenario download ppe-detection  # download ppe-detection scenario Ex: visionai scenario download all            # download all configured scenarios for the org Ex: visionai scenario download world          # download all available scenarios</p> <p>Download models for a given scenario, or download models for all scenarios that have been configured.</p> <p>Usage:</p> <pre><code>$ visionai scenarios download [OPTIONS] NAME\n</code></pre> <p>Arguments:</p> <ul> <li><code>NAME</code>: [required]</li> </ul> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-scenarios-list","title":"<code>visionai scenarios list</code>","text":"<p>List all scenarios available</p> <p>List all scenarios available in the system. This includes scenarios that may or maynot be applied to any specific camera.</p> <p>Usage:</p> <pre><code>$ visionai scenarios list [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-scenarios-test","title":"<code>visionai scenarios test</code>","text":"<p>Run the scenario locally to test it out.</p> <ul> <li>Download the model if not available.</li> <li>Pull the model server container image.</li> <li>Start the model server container with this model.</li> <li>Run inference with this model</li> </ul> <p>Usage:</p> <pre><code>$ visionai scenarios test [OPTIONS] NAME\n</code></pre> <p>Arguments:</p> <ul> <li><code>NAME</code>: [required]</li> </ul> <p>Options:</p> <ul> <li><code>--camera TEXT</code>: Camera name (default is webcam)  [default: 0]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-ui","title":"<code>visionai ui</code>","text":"<p>Start/stop web-app</p> <p>Start and stop the VisionAI web-app which can be a more intuitive way of managing cameras, pipelines and scenarios. Web-app also provides a live-stream view of the cameras.</p> <p>Usage:</p> <pre><code>$ visionai ui [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul> <p>Commands:</p> <ul> <li><code>start</code>: Start web server Use this function to start...</li> <li><code>status</code>: Web service status Use this function to get...</li> <li><code>stop</code>: Stop web server Use this function to stop...</li> </ul>"},{"location":"reference/reference/#visionai-ui-start","title":"<code>visionai ui start</code>","text":"<p>Start web server</p> <p>Use this function to start the web-service. Web service can be used for more intuitive configuration for the cameras and scenarios. Web-app is also the place to view event details, camera live-stream etc.</p> <p>Usage:</p> <pre><code>$ visionai ui start [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-ui-status","title":"<code>visionai ui status</code>","text":"<p>Web service status</p> <p>Use this function to get the status of the web-service. (if its running or not. This function also prints diagnostic information like last few log messages etc.)</p> <p>Usage:</p> <pre><code>$ visionai ui status [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--tail INTEGER</code>: tail number of lines  [default: 20]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-ui-stop","title":"<code>visionai ui stop</code>","text":"<p>Stop web server</p> <p>Use this function to stop already running web-service. There can be a single instance of the web-service supported currently. So there is no need for any arguments for this function.</p> <p>Usage:</p> <pre><code>$ visionai ui stop [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--web TEXT</code></li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-web","title":"<code>visionai web</code>","text":"<p>... alias for ui</p> <p>Usage:</p> <pre><code>$ visionai web [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul> <p>Commands:</p> <ul> <li><code>start</code>: Start web server Use this function to start...</li> <li><code>status</code>: Web service status Use this function to get...</li> <li><code>stop</code>: Stop web server Use this function to stop...</li> </ul>"},{"location":"reference/reference/#visionai-web-start","title":"<code>visionai web start</code>","text":"<p>Start web server</p> <p>Use this function to start the web-service. Web service can be used for more intuitive configuration for the cameras and scenarios. Web-app is also the place to view event details, camera live-stream etc.</p> <p>Usage:</p> <pre><code>$ visionai web start [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-web-status","title":"<code>visionai web status</code>","text":"<p>Web service status</p> <p>Use this function to get the status of the web-service. (if its running or not. This function also prints diagnostic information like last few log messages etc.)</p> <p>Usage:</p> <pre><code>$ visionai web status [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--tail INTEGER</code>: tail number of lines  [default: 20]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-web-stop","title":"<code>visionai web stop</code>","text":"<p>Stop web server</p> <p>Use this function to stop already running web-service. There can be a single instance of the web-service supported currently. So there is no need for any arguments for this function.</p> <p>Usage:</p> <pre><code>$ visionai web stop [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--web TEXT</code></li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"scenarios/","title":"Scenarios","text":"<p>Scenarios form the building blocks of VisionAI platform. These scenarios are organized into <code>Suites</code>. Below we talk about different suites and the scenarios that are part of them.</p> <ul> <li>All scenarios are available as pick-n-choose scenarios. You can pick the scenarios you want based on your business needs. Each scenario is independently tested.</li> <li>Events provided by these scenarios are given below. Events are sent to Redis &amp; Azure EventHub pubsub systems for further integration.</li> <li>There are a few common events supported by all scenarios (daily summary, weekly summary etc.)</li> <li>Currently supported scenarios are highlighted by a \u2705. Roadmap scenarios are highlighted by a \ud83d\udcc5.</li> <li>Each of the scenarios can be quickly tested through <code>visionai run &lt;scenario-name&gt;</code> command. For example:</li> </ul> <pre><code>visionai run smoke-and-fire-detection\n</code></pre> <p>New scenario request</p> <p>This section lists down all the scenarios that are supported by the VisionAI platform. There are more scenarios added daily - please send a request to us about any additional scenarios you need.</p>"},{"location":"scenarios/#privacy-suite","title":"Privacy Suite","text":"<p>For a majority of organizations - employee privacy is a top concern. Along with employee privacy, the organization needs to make sure that any data does not leave the premises. Any faces detected through Vision AI system need to be blurred, along with text, signage, computer screens and other sensitive information.</p> <p>Before any other scenarios are run, or before we store or process the images - the images are pre-processed through this privacy suite. As such, privacy suite is treated differently from other scenarios. Below examples provide a high-level overview of the privacy suite.</p> Status Scenario name Details Additional considerations \u2705 <code>face-blurring</code> Blur any faces detected More details \u2705 <code>text-blurring</code> Blue any text detected (paper, computer screens etc) More details \u2705 <code>license-plate-blurring</code> Blur any license plates detected More details \ud83d\udcc5 <code>signs-blurring</code> Blur any signs detected More details \ud83d\udcc5 <code>obstructed-camera</code> If camera feed is obstructed, send an alert More details"},{"location":"scenarios/#hazard-warnings-suite","title":"Hazard Warnings Suite","text":"<p>Following scenarios provide hazard warning examples supported by VisionAI suite. Currently supported scenarios are highlighted by a \u2705. You can run these through VisionAI CLI, for example, you can run the following command for smoke-and-fire-detection. Once the scenario has started - you can use a lighter or a match to generate the events. The events can be viewed on CLI window.</p> <pre><code>visionai run smoke-and-fire-detection\n</code></pre> <p>TODO</p> <ul> <li>TODO: For scenarios requiring IR camera and/or IoT Sensor, point to the exact device this has been tested with.</li> </ul> Status Scenario name Supported Events Additional considerations \u2705 <code>smoke-and-fire-detection</code> <code>Smoke event detected</code> <code>Fire event detected</code> <code>Sparks detected</code> <code>Open flames detection</code> More details \u2705 <code>no-smoking-zone</code> <code>Smoking event detected</code> <code>Vaping event detected</code> More details \ud83d\udcc5 <code>spills-and-leak-detection</code> <code>Water puddle detected</code> <code>Water leak from equipment detected</code> <code>Spill event detected</code> <code>Slippery sign detected</code> \ud83d\udcc5 <code>gas-leak-detection</code> <code>Gas leak event detected</code> IR Camera Required \ud83d\udcc5 <code>missing-fire-extinguisher</code> <code>Fire extinguisher missing</code> \ud83d\udcc5 <code>blocked-exit-monitoring</code> <code>Blocked exit detected</code> \u2705 <code>slip-and-fall-detection</code> <code>Person fall event detected</code> <code>Path block detected</code> More details \ud83d\udcc5 <code>equipment-temperature-ir-camera</code> <code>Temperature exceeds limit</code> <code>Temperature subceeds limit</code> IR Camera Required \u2705 <code>rust-and-corrosion-detection</code> <code>Rust or corrosion event detected</code> More details"},{"location":"scenarios/#worker-health-safety-suite","title":"Worker Health &amp; Safety Suite","text":"<p>Following scenarios provide Worker Health and Safety examples supported by VisionAI suite. (Also referred to as Personnel Health and Safety).</p> <p>Workplace Personnel Health &amp; Safety is important because it ensures that employees are safe and healthy in their work environment. This includes providing a safe and healthy work environment, proper safety training, and regular safety inspections. Additionally, it also includes enforcing safety policies to ensure that all employees are aware of and follow safety procedures, as well as encouraging a culture of safety within the workplace.</p> <p>Currently supported scenarios are highlighted by a \u2705. You can run these through VisionAI CLI, for example:</p> <pre><code>visionai run ppe-detection\n</code></pre> <p>You can see real-time events generated as soon as person is detected without PPE (helmets, gloves, safety boots etc.). There are options to configure what PPE's are required for your scenario. This can be done through the VisionAI web-application which can be accessed on through http://localhost:3001.</p> Status Scenario name Supported Events Additional considerations \u2705 <code>ppe-detection</code> <code>Person detected without helmet</code> <code>Person detected without gloves</code> <code>Person detected without safety boots</code> <code>Person detected without safety goggles</code> <code>Person detected without face mask</code> <code>Person detected without vest</code> <code>Person detected without full-body suit</code> <code>Person detected without PFAS</code> <code>Person detected without ear protection</code> More details \u2705 <code>working-at-heights</code> <code>Person detected without PFAS</code> <code>Steps detected without railings</code> <code>Person detected at height without parapets</code> <code>Ladder detected not in compliance</code> More details \u2705 <code>fall-and-accident-detection</code> <code>Person slip &amp; fall detected</code> <code>Potential collision/accident detected</code> <code>Wet floor detected</code> <code>Debris detected on floor</code> <code>Wet/slippery sign detected</code> \u2705 <code>worker-fatigue-detection</code> <code>Drowsy worker detected</code> Straight camera angle \u2705 <code>posture-and-ergonomics</code> <code>Bend count per individual</code> Straight camera angle  More details \u2705 <code>confined-spaces-monitoring</code> <code>Person detected</code> <code>Person left</code> <code>Person dwell time exceeds limit</code> <code>Person detected without motion</code> <code>Person fall detected</code> More details \ud83d\udcc5 <code>empty-pallets-detection</code> <code>Empty pallets detected</code> <code>Partially empty pallets detected</code> \ud83d\udcc5 <code>spills-and-leaks-detection</code> <code>Water puddle detected</code> <code>Water leak from equipment detected</code> <code>Wet floor detected</code> <code>Spill event detected</code> <code>Slippery sign detected</code> \ud83d\udcc5 <code>hand-wash-compliance</code> <code>Missed hand wash</code> \ud83d\udcc5 <code>environment-monitoring</code> <code>CO out of range</code> <code>CO2 out of range</code> <code>CH4 out of range</code> <code>VOCs out of range</code> <code>Temperature out of range</code> <code>Pressure out of range</code> <code>Humidity out of range</code> \ud83d\udcc5 <code>person-temperature-monitoring</code> <code>Person temperature exceeds threshold</code> IR Camera required"},{"location":"scenarios/#occupancy-policies","title":"Occupancy Policies","text":"<p>Occupancy Policies relate to counting and tracking employees and/or other personnel in the room. These could include people-counting and enforcing max-occupancy policies, or tracking people's dwell time in a confined space.</p> <p>Currently supported scenarios are highlighted by a \u2705. You can run these through VisionAI CLI, for example:</p> <pre><code>visionai run max-occupancy\n</code></pre> <p>Occupancy Metrics</p> <ul> <li>Occupancy metrics is similar in structure to max-occupancy, or restricted areas scenarios.</li> <li>However it sends out a summary event is structured like this. This will give a granular summary event at the end of the day.</li> <li>Users can start with occupancy-metrics and then move to max-occupancy or restricted areas if they need to enforce policies. <pre><code>{\n\"date\": \"2023-02-23\",\n\"stations\": [{\n\"id\": \"station_1\",\n\"hours\": [\n{\n\"start_time\": \"2023-02-23T14:00:01\",\n\"end_time\": \"2023-02-23T15:00:00\",\n\"occupancy_cnt\": 14\n}\n...\n]\n}...]\n}\n</code></pre></li> </ul> <p>Also need to specify that the camera needs to be configured to have a good view of the stations where occupancy metrics need to be checked.</p> Status Scenario name Supported Events Additional considerations \u2705 <code>max-occupancy</code> <code>Person count exceeds max limit</code> More details \u2705 <code>restricted-areas</code> <code>Person detected in restricted area</code> <code>Movement detected in restricted area</code> <code>Person detected after hours</code> <code>Movement detected after hours</code> More details \u2705 <code>dwell-time</code> <code>Person detected</code> <code>Person left</code> <code>Person dwell time exceeds limit</code> <code>Person detected without motion</code> <code>Person fall detected</code> More details \ud83d\udcc5 <code>social-distancing</code> <code>Person detected</code> <code>Person left</code> <code>Person distance event</code> \u2705 <code>desk-occupancy</code> <code>Daily summary event</code> More details \u2705 <code>station-occupancy</code> <code>Daily summary event</code> More details \ud83d\udcc5 <code>occupancy-metrics</code> <code>Daily summary event</code> \ud83d\udcc5 <code>no-children-pets-visitors</code> <code>Children detected</code> <code>Pets detected</code> <code>Visitors detected</code> \ud83d\udcc5 <code>authorized-personnel-only</code> <code>Unauthorized person detected</code>"},{"location":"scenarios/#company-policies","title":"Company Policies","text":"<p>Company policies include specific scenarios that are relevant to your company. These could include scenarios like no-smoking/no-vaping zones, no food or drinks in certain areas, or no cell phones/pictures in certain areas. Some of these scenarios overlap with occupancy policies, but they are still useful to have here as separate scenarios.</p> Status Scenario name Supported Events Additional considerations \u2705 <code>max-occupancy</code> <code>Person count exceeds max limit</code> More details \u2705 <code>restricted-areas</code> <code>Person detected in restricted area</code> <code>Movement detected in restricted area</code> <code>Person detected after hours</code> <code>Movement detected after hours</code> More details \u2705 <code>dwell-time</code> <code>Person detected</code> <code>Person left</code> <code>Person dwell time exceeds limit</code> <code>Person detected without motion</code> <code>Person fall detected</code> More details \ud83d\udcc5 <code>social-distancing</code> <code>Person detected</code> <code>Person left</code> <code>Person distance event</code> \u2705 <code>desk-occupancy</code> <code>Daily summary event</code> More details \u2705 <code>station-occupancy</code> <code>Daily summary event</code> More details \u2705 <code>occupancy-metrics</code> <code>Daily summary event</code> More details \ud83d\udcc5 <code>no-children-pets-visitors</code> <code>Children detected</code> <code>Pets detected</code> <code>Visitors detected</code> \ud83d\udcc5 <code>authorized-personnel-only</code> <code>Unauthorized person detected</code> \ud83d\udcc5 <code>no-food-or-drinks-allowed</code> <code>Person with food detected</code> <code>Person with drinks detected</code> <code>Spill event detected</code> \u2705 <code>no-phone-text-pictures</code> <code>Cellphone usage detected</code> <code>Person detected taking pictures</code> More details \u2705 <code>no-smoking-or-vaping</code> <code>Smoking event detected</code> <code>Vaping event detected</code> More details \u2705 <code>no-children-pets-visitors</code> <code>Children detected</code> <code>Pets detected</code> <code>Visitors detected</code> More details \ud83d\udcc5 <code>authorized-personnel-only</code> <code>Person without uniform detected</code> <code>Person without badge detected</code> \ud83d\udcc5 <code>waste-management</code> <code>Spill event detected</code> <code>Waste bin full</code> <code>Debris detected in Field of View</code> \ud83d\udcc5 <code>energy-conservation</code> <code>Occupancy pattern daily summary</code> <code>Light usage daily summary</code> \ud83d\udcc5 <code>restricted-areas</code> <code>Person detected in restricted area</code> <code>Movement detected in restricted area</code> <code>Person detected after hours</code> <code>Movement detected after hours</code> \u2705 <code>badge-tailgating</code> <code>Multi-entry (tailgating) event detected</code> <code>Unauthorized entry event detected</code> More details \u2705 <code>perimeter-control</code> <code>Person detected near fence/perimeter</code> <code>Movement detected near fence/perimeter</code> IR camera requiredMore details"},{"location":"scenarios/#equipment-monitoring","title":"Equipment Monitoring","text":"<p>Equipment policies include specific scenarios that are relevant monitoring heavy machinaries. These could be through monitoring the temperature of the equipment, or through IoT sensors that are attached to the equipment that allow to monitor vibration, noise, or other parameters for the equipment.</p> Status Scenario name Supported Events Additional considerations \ud83d\udcc5 <code>equipment-temperature</code> <code>Equipment temperature exceeds limit</code> <code>Equipment temperature subsceeds limit</code> \u2705 <code>rust-and-corrosion-detection</code> <code>Rust or corrosion event detected</code> More details \ud83d\udcc5 <code>equipment-vibration</code> <code>Equipment vibration exceeds limit</code> 2 \ud83d\udcc5 <code>equipment-noise</code> <code>Equipment noise exceeds limit</code> 3 \ud83d\udcc5 <code>reading-analog-dials</code> <code>Analog meter reading event</code> \ud83d\udcc5 <code>tools-check-in-check-out</code> <code>Person left without checkout</code> \ud83d\udcc5 <code>equipment-water-leak-puddle</code> <code>Water leak detected from equipment</code>"},{"location":"scenarios/#environment-monitoring","title":"Environment Monitoring","text":"<p>Monitoring the environment like current temperature, humidity, or air quality is important to ensure that the workplace is safe and comfortable for employees. These scenarios are implemented through IoT sensors that are completely integrated into Vision AI suite.</p> Status Scenario name Supported Events Additional considerations \ud83d\udcc5 <code>temperature-monitoring</code> <code>Temperature excceds limit</code> <code>Temperature subsceeds limit</code> More details \ud83d\udcc5 <code>humidity-monitoring</code> <code>Humidity excceds limit</code> <code>Humidity subsceeds limit</code> More details \ud83d\udcc5 <code>pressure-monitoring</code> <code>Pressure excceds limit</code> <code>Pressure subsceeds limit</code> More details \ud83d\udcc5 <code>air-quality</code> <code>CO exceeds limit</code> <code>CO2 exceeds limit</code> <code>NO2 Exceeds limit</code> <code>SO2 exceeds limit</code> <code>VOCs exceeds limit</code> <code>Excessive dust detected</code> <code>Excessive dust detected</code> More details \ud83d\udcc5 <code>light-sensor-monitoring</code> <code>Light intensity exceeds limit</code> <code>Light intensity subsceeds limit</code> More details \ud83d\udcc5 <code>noise-level-monitoring</code> <code>Noise level exceeds limit</code> <code>Noise level subsceeds limit</code> More details \ud83d\udcc5 <code>energy-usage-monitoring</code> <code>Energy usage hourly smmary</code> More details \ud83d\udcc5 <code>water-management</code> <code>TODO</code> More details \ud83d\udcc5 <code>waste-management</code> <code>TODO</code> More details \ud83d\udcc5 <code>radiation-monitoring</code> <code>Radiation level exceeds limit</code> <code>Radiation level subsceeds limit</code> More details"},{"location":"scenarios/#suspicious-activity-detection","title":"Suspicious Activity detection","text":"<p>Suspicious activity detection suite relies on a combination of activity detection models and object detection models. These models are trained to detect suspicious activity in a variety of scenarios.</p> Status Scenario name Supported Events Additional considerations \ud83d\udcc5 <code>loitering-detection</code> <code>Person detected in closed space</code> <code>Person detected during off hours</code> <code>Person dwell time exceeds limit</code> More details \ud83d\udcc5 <code>suspicious-package-detection</code> <code>Suspicious package detected</code> <code>Package abandoned</code> More details \ud83d\udcc5 <code>bullying-fighting-aggressive-behavior</code> <code>Bullying/fighting/aggressive event detected</code> More details \ud83d\udcc5 <code>vandalism-graffiti-company-property-destruction</code> <code>Motion detected in area (gross event)</code> <code>People detected in area (more granular event)</code> <code>Non-uniformed personnel detected in area</code> <code>Non badged personnel detected in area</code> <code>Vandalism detected in area (before &amp; after)</code> <code>Paint/graffiti detected in area (before &amp; after changes)</code> <code>Behavior analysis event showing company property destruction.</code> More details \u2705 <code>firearms-knives-detection</code> <code>Person brandishing firearm</code> <code>Person brandishing knives</code> More details \ud83d\udcc5 <code>solictation-detection</code> <code>Potential solicitation event detected</code> More details \ud83d\udcc5 <code>theft-and-or-shoplifting</code> <code>Potential theft detected</code> <code>Potential shoplifting activity detected</code> More details \ud83d\udcc5 <code>shipping-activity-detection</code> <code>Shipping activity detected during after-hours</code> <code>Shipping activity detected from non-designated areas</code> More details \ud83d\udcc5 <code>intrusion-detection</code> <code>Intrusion event detected</code> More details"},{"location":"scenarios/#vehicle-activity","title":"Vehicle Activity","text":"<p>The below scenarios are designed to detect vehicle activity in and around the factory.</p> Status Scenario name Supported Events Additional considerations \ud83d\udcc5 <code>vehicle-policies</code> <code>Vehicle activity detected in non-designtated areas</code> <code>Vehicle activity detected during after-hours</code> <code>Collision event detected</code> <code>Near collision event detected</code> More details \ud83d\udcc5 <code>vehicle-usage</code> <code>Daily summary event of vehicle usage</code> <code>Path-map of vehicle usage</code> More details \ud83d\udcc5 <code>forklift-zone-breach</code> <code>Forklift observed outside of configured zone</code> <code>Pedestrian observed in forklift zone</code> More details \ud83d\udcc5 <code>vehicle-license-plate-detection</code> <code>Vehicle detected with license plate number</code> More details \ud83d\udcc5 <code>vehicle-speed-monitoring</code> <code>Vehicle speed exceeds limit</code> More details \ud83d\udcc5 <code>vehicle-cargo-volume-limit</code> <code>Vehicle cargo volume exceeds limit</code> More details"},{"location":"scenarios/#next-steps","title":"Next Steps","text":"<p>Now that you have a better understanding of the scenarios that are available, you can start to think about how you can organize these scenarios into a solution that meets your needs. You can also go to the individual scenario page to learn more about it. We can customize each of these models for your use-cases and provide you with a solution that is tailored to your needs. You can contact us through this page</p> <ol> <li> <p>This works by detecting a person's uniform and comparing it to a list of authorized personnel. This is a more advanced scenario and requires a custom model to be trained for your specific use-case.\u00a0\u21a9</p> </li> <li> <p>Vibration sensor needed to implement this scenario.\u00a0\u21a9</p> </li> <li> <p>Noise sensor needed to implement this scenario.\u00a0\u21a9</p> </li> </ol>"},{"location":"scenarios/aggressive-behavior/","title":"Aggressive Behavior","text":"<p>Create a safer and more productive work environment with our real-time Aggressive behaviour detection system.</p>"},{"location":"scenarios/aggressive-behavior/#overview","title":"Overview","text":"<p>A workplace that is free from bullying, fighting, and aggressive behavior can help to improve employee well-being and overall job satisfaction. This can result in higher levels of productivity, better employee retention rates, and a more positive work environment. Aggressive behavior is a serious problem at workplaces. It can lead to serious injuries and even death. It is important to detect these behaviors early on, and preventive measures can be taken to address the issue before it escalates.</p> <p>These models are an important tool for promoting a safe and respectful environment at workplaces and other settings, and they have the potential to make a real difference in the lives of those who may be vulnerable to bullying or aggression.</p>"},{"location":"scenarios/aggressive-behavior/#vision-ai-based-monitoring","title":"Vision AI based monitoring","text":"<p>VisionAI's agressive behaviour detection model is designed to promote a safe, healthy, and productive workplace environment for all employees. The model is able to provide real-time alerts when it detects aggressive behavior. This will enable management to intervene and prevent escalation of the situation.</p>"},{"location":"scenarios/aggressive-behavior/#model-details","title":"Model Details","text":""},{"location":"scenarios/aggressive-behavior/#dataset","title":"Dataset","text":"<p>The dataset for this scenario is based on real-world data from different workplaces. The dataset consists of images and videos collected from various sources.</p>"},{"location":"scenarios/aggressive-behavior/#model","title":"Model","text":"<p>The model to detect agressive behaviour events is in progress and it will be released soon.</p>"},{"location":"scenarios/aggressive-behavior/#scenario-details","title":"Scenario details","text":"<p>The business logic for this scenario is as follows: </p> <ul> <li> <p>We use existing camera feeds from the premises to monitor the presence of people and analyse their behaviour.</p> </li> <li> <p>VisionAI system is able to run on edge devices. It uses camera feeds for processing. </p> </li> <li> <p>An alarming system is in place as part of an aggressive behavior detection solution.</p> </li> </ul> Test now with online Web-CamWith RTSP Camera - PipelinesWith Azure Setup <p>To test this model &amp; scenario, you can use the following steps:</p> <ul> <li> <p>Install the visionai package from PyPI</p> <pre><code>$ pip install visionai\n</code></pre> </li> <li> <p>Test the scenario from your local web-cam</p> <pre><code>$ visionai scenario test aggressive-behavior-detection\n\nDownloading models for scenario: aggressive-behavior-detection\nModel: aggressive-behavior-detection: https://workplaceos.blob.core.windows.net/models/yolov5s-aggressive-behavior-detection/yolov5s-aggressive-behavior-detection-0.0.1.zip\nStarting scenario: aggressive-behavior-detection..\n</code></pre> </li> <li> <p>You should be able to see the events generated on your console window with the detections of aggressive behavior within the camera field of view.</p> </li> </ul> <p>[TODO]</p> <p>VisionAI app is available at a Azure Market place, one can download and use it by following steps mentioned here</p>"},{"location":"scenarios/aggressive-behavior/#features","title":"Features","text":"<ul> <li> <p>Easy to use: The model is easy to use and can be deployed in a variety of settings, including workplaces and other public spaces.</p> </li> <li> <p>Alert system: The model is able to generate alerts when it detects signs of bullying, fighting, or aggressive behavior, allowing management to take appropriate action to address the issue.</p> </li> <li> <p>Real-time monitoring: The detection model is be able to monitor and analyze interactions among employees in real-time to identify any signs of bullying, fighting, or aggressive behavior.</p> </li> </ul>"},{"location":"scenarios/aggressive-behavior/#training-with-custom-data","title":"Training with custom data","text":"<p>The scenario is provided as part of our GPL-v3 package for VisionAI. If you wish to train this with custom datasets, please contact us and we can provide you with the training code. You can do custom training with your own datasets for free, as long as it complies with GPLv3 license (you give back the code to the community). If you are interested in a custom license, please contact us.</p>"},{"location":"scenarios/aggressive-behavior/#contact-us","title":"Contact Us","text":"<ul> <li>For technical issues, you can open a Github issue here.</li> <li>For business inquiries, you can contact us through our website.</li> </ul>"},{"location":"scenarios/air-quality/","title":"Gas Sensors for Leakage and Air Quality Monitoring","text":"<p>Monitor CO, CO2, VOCs, NOx, SOx, O2, and Particulate matters in real-time </p>"},{"location":"scenarios/air-quality/#overview","title":"Overview","text":"<p>Gas leakage, equipment damage, and low air quality are significant challenges for businesses across industries. Therefore, you need advanced gas sensing devices to ensure desired air quality and minimum leakage in your facility. </p> <p>Go easy knowing that you can track and monitor the air quality in your surroundings with our wide range of gas monitoring solutions. Get yourself covered with our specialized indoor and outdoor air quality monitoring solution, including breakouts for projects detecting indoor carbon monoxide, Volatile Organic Compounds (VOCs), SO2, NOX, O2, and particulate matter.</p>"},{"location":"scenarios/air-quality/#vision-ai-based-monitoring","title":"Vision AI based monitoring","text":"<p>VisionAI's sensor-powered gas detection solutions can identify CO, CO2, VOCs, NOx, SOx, O2, and particulate matter in real time. In addition, our models integrated with advanced sensors can sense gas and dust particles in the air as soon as their percentage level increases or falls below a threshold value. With a high response rate and accuracy, these solutions are your best companion for dealing with leakage and poor air quality. </p>"},{"location":"scenarios/air-quality/#model-details","title":"Model Details","text":""},{"location":"scenarios/air-quality/#dataset","title":"Dataset","text":"<p>The dataset comprises relevant, high-quality, labeled images and videos from diverse sources. It is evenly distributed and balanced with equal examples from each category to deliver accurate results.</p> <ul> <li> <p>Measure safe gas values: The dataset includes safe gas, voltage, and current output values. Based on the gas levels, you can monitor sensor pumps directly for diagnostic purposes.</p> </li> <li> <p>Permissible gas temperature: The dataset includes permissible gas temperature under various conditions. This temperature range varies from gas to gas and sensor to sensor.</p> </li> <li> <p>Indoor vs. Outdoor environments: The dataset includes images and videos of indoor and outdoor environments. The gas sensors can differentiate between the two settings and share permissible gas levels for each surrounding.</p> </li> <li> <p>Different types of clothing: The dataset includes images and videos of workers in different clothing with/without safety necessities like PPE kits and safety helmets.</p> </li> <li> <p>Flow rate: Checkout the gas flow rate and make proactive adjustments to fix it. The dataset can help you figure out what\u2019s the best flow rate the gas should have in a particular condition.</p> </li> <li> <p>Various lighting conditions: The dataset contains images and videos under different lighting conditions. The model may need to adjust its settings to continue detecting leakage accurately.</p> </li> <li> <p>Multiple camera angles and resolutions The dataset includes images taken from different angles, such as top-down, side view, or angled view, and available in varied resolutions.</p> </li> </ul>"},{"location":"scenarios/air-quality/#model","title":"Model","text":"<p>The model to detect air quality events is in progress and it will be released soon.</p>"},{"location":"scenarios/air-quality/#scenario-details","title":"Scenario details","text":"<ul> <li>When the gas or particulate percentage of a gas surpasses or falls a threshold value, the model triggers an alarm.</li> <li>The model can be used to detect sources of leakage, fire, or elevated levels of gas within a vicinity. </li> <li>When the equipment is faulty, and there is a high risk of an accident, the camera sensors can ring an alarm or mark safe exits with the help of VisionAI capabilities integrated with your existing setup.</li> <li>The model allows you to upload the most recent version of the baseline image onto your camera before the inspection.</li> <li>The solution can also be used to measure toxic gas concentration, smoke, and fire, and to check air quality.</li> </ul> Test now with online Web-CamWith RTSP Camera - PipelinesWith Azure Setup <p>To test this model &amp; scenario, you can use the following steps:</p> <ul> <li> <p>Install the visionai package from PyPI</p> <pre><code>$ pip install visionai\n</code></pre> </li> <li> <p>Test the scenario from your local web-cam</p> <pre><code>$ visionai scenario test air-quality-monitoring\n\nDownloading models for scenario: air-quality-monitoring\nModel: air-quality-monitoring: https://workplaceos.blob.core.windows.net/models/yolov5s-air-quality-monitoring/yolov5s-air-quality-monitoring-0.0.1.zip\nStarting scenario: air-quality-monitoring..\n</code></pre> </li> <li> <p>You should be able to see the information generated on your console window with the air quality events within the camera field of view.</p> </li> </ul> <p>[TODO]</p> <p>VisionAI app is available at a Azure Market place, one can download and use it by following steps mentioned here</p>"},{"location":"scenarios/air-quality/#features","title":"Features","text":"<ul> <li> <p>Real-time reporting: The model can provide real-time reporting of air quality data, so that people can take immediate action to reduce exposure to pollutants.</p> </li> <li> <p>Integration with other systems: The model is able to integrate with other systems, such as weather monitoring systems or emergency response systems, to provide a more comprehensive view of air quality and its impact on public health.</p> </li> <li> <p>Historical data tracking: To identify long-term air quality trends and patterns, air quality monitoring models may track and store historical data over extended periods of time. This can provide valuable insights into the effectiveness of environmental policies and initiatives, as well as the impact of climate change on air quality.</p> </li> </ul>"},{"location":"scenarios/air-quality/#training-with-custom-data","title":"Training with custom data","text":"<p>The scenario is provided as part of our GPL-v3 package for VisionAI. If you wish to train this with custom datasets, please contact us and we can provide you with the training code. You can do custom training with your own datasets for free, as long as it complies with GPLv3 license (you give back the code to the community). If you are interested in a custom license, please contact us.</p>"},{"location":"scenarios/air-quality/#contact-us","title":"Contact Us","text":"<ul> <li>For technical issues, you can open a Github issue here.</li> <li>For business inquiries, you can contact us through our website.</li> </ul>"},{"location":"scenarios/authorized-personnel-occupancy/","title":"Authorized Personnel","text":"<p>An intelligent way to enhance security and prevent unauthorized access to restricted areas.</p>"},{"location":"scenarios/authorized-personnel-occupancy/#overview","title":"Overview","text":"<p>Unauthorized entry or access refers to any member of the public other than employees entering areas of the restricted business premises or a warehouse. This could be done by an individual, a group of people, visitors or children/pets wrongly entering a restricted area. There can be different ways in which a security breach can happen and an unauthorized access can pose various risks;</p> <ul> <li>Safety risk</li> <li>Security risk</li> <li>Risk of non-compliance</li> <li>Risk of injury</li> </ul> <p>It is important for organizations to maintain security and controlled access at the workplace. However, conventional surveillance methods are often complex, human-oriented, expensive and challenging to automate. In addition, the existing solutions cannot detect intrusion after an unauthorized entry has taken place.</p>"},{"location":"scenarios/authorized-personnel-occupancy/#vision-ai-based-monitoring","title":"Vision AI based monitoring","text":"<p>Make your workplace safer and smarter with our VisionAI monitoring, a complete solution that helps you enforce security policy adherence and deter intruders effectively. Now, you can easily detect any attempts to gain unauthorized access with our fully automated system that guards your facility 24/7 and sends instant alerts to help you prevent a security breach before it occurs. </p> <p>Our system offers reliable detection and is easy to integrate with your existing camera infrastructure, allowing you to scale your system with a few simple clicks.</p>"},{"location":"scenarios/authorized-personnel-occupancy/#events","title":"Events","text":"<p>VisionAI model's generated events would be:</p> <ul> <li>Children detected in area</li> <li>Visitors detected in area</li> <li>Person without uniform detected</li> <li>Person without badge detected</li> </ul> <p>It is recommended that any instance of above detected events be reported to the appropriate authority. An event data for this scenario has information such as:</p> <ul> <li>Date and time of the event</li> <li>Location of the event</li> <li>Type of personnel identified</li> </ul>"},{"location":"scenarios/authorized-personnel-occupancy/#configuration","title":"Configuration","text":"<p>It is recommended to set up camera in ceiling view to detect and identify children, visitors, person without uniform and badges.</p>"},{"location":"scenarios/authorized-personnel-occupancy/#model-details","title":"Model Details","text":""},{"location":"scenarios/authorized-personnel-occupancy/#dataset","title":"Dataset","text":"<p>The dataset comprises relevant, high-quality, labeled videos and images from diverse sources. The dataset is evenly distributed and balanced with an equal number of examples for each category to avoid bias toward one class. It contains variations with different real-world scenarios to render effective and efficient results.</p>"},{"location":"scenarios/authorized-personnel-occupancy/#model","title":"Model","text":"<p>The model to detect only authorized personnel event is in progress and it will be released soon.</p>"},{"location":"scenarios/authorized-personnel-occupancy/#scenario-details","title":"Scenario details","text":"<p>Our VisionAI solution for access control protects against unauthorized access and physical security incidents before they occur. Different scenarios of unauthorized access have been taken into account for real-time detection and alerts which include;</p> <ul> <li>Visitors detected in the area</li> <li>Employees without uniform trying to gain access </li> <li>Employees without badges trying to enter the premise </li> <li>Children/pets wrongly entering restricted areas</li> </ul> Test now with online Web-CamWith RTSP Camera - PipelinesWith Azure Setup <p>To test this model &amp; scenario, you can use the following steps:</p> <ul> <li> <p>Install the visionai package from PyPI</p> <pre><code>$ pip install visionai\n</code></pre> </li> <li> <p>Test the scenario from your local web-cam</p> <pre><code>$ visionai scenario test auth-personnel-detection\n\nDownloading models for scenario: auth-personnel-detection\nModel: miss-fire-exting-detection: https://workplaceos.blob.core.windows.net/models/yolov5s-people/yolov5s-people-0.0.4.zip\nStarting scenario: auth-personnel-detection..\n</code></pre> </li> <li> <p>You should be able to see the events generated on your console window with authorized personnel identified within the camera field of view.</p> </li> </ul> <p>[TODO]</p> <p>VisionAI app is available at a Azure Market place, one can download and use it by following steps mentioned here</p>"},{"location":"scenarios/authorized-personnel-occupancy/#features","title":"Features","text":"<p>Some potential features of VisionAI for identifying authorized personnel could include: - Improved safety and security - Enhanced visual monitoring of each entry and exit point across all locations - Remotely monitor and instantly investigate any security concerns taking place even after working hours - Immediately get notified of potential physical security breaches, address them quickly and operate effectively.</p>"},{"location":"scenarios/authorized-personnel-occupancy/#training-with-custom-data","title":"Training with custom data","text":"<p>The scenario is provided as part of our GPL-v3 package for VisionAI. If you wish to train this with custom datasets, please contact us and we can provide you with the training code. You can do custom training with your own datasets for free, as long as it complies with GPLv3 license (you give back the code to the community). If you are interested in a custom license, please contact us.</p>"},{"location":"scenarios/authorized-personnel-occupancy/#contact-us","title":"Contact Us","text":"<ul> <li>For technical issues, you can open a Github issue here.</li> <li>For business inquiries, you can contact us through our website.</li> </ul>"},{"location":"scenarios/authorized-personnel/","title":"Authorized Personnel","text":"<p>An intelligent way to enhance security and prevent unauthorized access to restricted areas.</p>"},{"location":"scenarios/authorized-personnel/#overview","title":"Overview","text":"<p>Unauthorized entry or access refers to any member of the public other than employees entering areas of the restricted business premises or a warehouse. This could be done by an individual, a group of people, visitors or children/pets wrongly entering a restricted area. There can be different ways in which a security breach can happen and an unauthorized access can pose various risks;</p> <ul> <li>Safety risk</li> <li>Security risk</li> <li>Risk of non-compliance</li> <li>Risk of injury</li> </ul> <p>It is important for organizations to maintain security and controlled access at the workplace. However, conventional surveillance methods are often complex, human-oriented, expensive and challenging to automate. In addition, the existing solutions cannot detect intrusion after an unauthorized entry has taken place.</p>"},{"location":"scenarios/authorized-personnel/#vision-ai-based-monitoring","title":"Vision AI based monitoring","text":"<p>Make your workplace safer and smarter with our VisionAI monitoring, a complete solution that helps you enforce security policy adherence and deter intruders effectively. Now, you can easily detect any attempts to gain unauthorized access with our fully automated system that guards your facility 24/7 and sends instant alerts to help you prevent a security breach before it occurs. </p> <p>Our system offers reliable detection and is easy to integrate with your existing camera infrastructure, allowing you to scale your system with a few simple clicks.</p>"},{"location":"scenarios/authorized-personnel/#events","title":"Events","text":"<p>VisionAI model's generated events would be:</p> <ul> <li>Children detected in area</li> <li>Visitors detected in area</li> <li>Person without uniform detected</li> <li>Person without badge detected</li> </ul> <p>It is recommended that any instance of above detected events be reported to the appropriate authority. An event data for this scenario has information such as:</p> <ul> <li>Date and time of the event</li> <li>Location of the event</li> <li>Type of personnel identified</li> </ul>"},{"location":"scenarios/authorized-personnel/#configuration","title":"Configuration","text":"<p>It is recommended to set up camera in ceiling view to detect and identify children, visitors, person without uniform and badges.</p>"},{"location":"scenarios/authorized-personnel/#model-details","title":"Model Details","text":""},{"location":"scenarios/authorized-personnel/#dataset","title":"Dataset","text":"<p>The dataset comprises relevant, high-quality, labeled videos and images from diverse sources. The dataset is evenly distributed and balanced with an equal number of examples for each category to avoid bias toward one class. It contains variations with different real-world scenarios to render effective and efficient results.</p>"},{"location":"scenarios/authorized-personnel/#model","title":"Model","text":"<p>The model to detect only authorized personnel event is in progress and it will be released soon.</p>"},{"location":"scenarios/authorized-personnel/#scenario-details","title":"Scenario details","text":"<p>Our VisionAI solution for access control protects against unauthorized access and physical security incidents before they occur. Different scenarios of unauthorized access have been taken into account for real-time detection and alerts which include;</p> <ul> <li>Visitors detected in the area</li> <li>Employees without uniform trying to gain access </li> <li>Employees without badges trying to enter the premise </li> <li>Children/pets wrongly entering restricted areas</li> </ul> Test now with online Web-CamWith RTSP Camera - PipelinesWith Azure Setup <p>To test this model &amp; scenario, you can use the following steps:</p> <ul> <li> <p>Install the visionai package from PyPI</p> <pre><code>$ pip install visionai\n</code></pre> </li> <li> <p>Test the scenario from your local web-cam</p> <pre><code>$ visionai scenario test auth-personnel-detection\n\nDownloading models for scenario: auth-personnel-detection\nModel: miss-fire-exting-detection: https://workplaceos.blob.core.windows.net/models/yolov5s-people/yolov5s-people-0.0.4.zip\nStarting scenario: auth-personnel-detection..\n</code></pre> </li> <li> <p>You should be able to see the events generated on your console window with authorized personnel identified within the camera field of view.</p> </li> </ul> <p>[TODO]</p> <p>VisionAI app is available at a Azure Market place, one can download and use it by following steps mentioned here</p>"},{"location":"scenarios/authorized-personnel/#features","title":"Features","text":"<p>Some potential features of VisionAI for identifying authorized personnel could include: - Improved safety and security - Enhanced visual monitoring of each entry and exit point across all locations - Remotely monitor and instantly investigate any security concerns taking place even after working hours - Immediately get notified of potential physical security breaches, address them quickly and operate effectively.</p>"},{"location":"scenarios/authorized-personnel/#training-with-custom-data","title":"Training with custom data","text":"<p>The scenario is provided as part of our GPL-v3 package for VisionAI. If you wish to train this with custom datasets, please contact us and we can provide you with the training code. You can do custom training with your own datasets for free, as long as it complies with GPLv3 license (you give back the code to the community). If you are interested in a custom license, please contact us.</p>"},{"location":"scenarios/authorized-personnel/#contact-us","title":"Contact Us","text":"<ul> <li>For technical issues, you can open a Github issue here.</li> <li>For business inquiries, you can contact us through our website.</li> </ul>"},{"location":"scenarios/blocked-exit/","title":"Blocked Exit monitoring","text":"<p>Ensure that your organziation is safe &amp; compliant for any emergency evacuations. If any of your exits are blocked - get an event notifications so it can be quickly resolved.</p> <p>Detection of blocked exit</p>"},{"location":"scenarios/blocked-exit/#overview","title":"Overview","text":"<p>Every second counts in emergency situations such as fires, earthquakes, or other disasters, and people need to evacuate the premises as quickly as possible to avoid being trapped or injured. Therefore, the presence of functional emergency exits and a well-designed escape route plan is of paramount importance. </p> <p>Any blockages or obstructions in the exits could prove fatal as they hinder people's ability to evacuate safely and quickly. However, the accumulation of debris, fire flames, and toxic gasses can block exits. Time is of the essence, and wrong escape plans can impede people from escaping quickly and safely. </p> <p>Moreover, People may try to force their way through blocked exits, leading to injuries or even fatalities. In addition, blocked exits can hinder the efforts of rescue teams trying to enter the building to help those in need. This makes it critical to preempt operational exits and detect blocked ones before the situation worsens.</p>"},{"location":"scenarios/blocked-exit/#vision-ai-based-monitoring","title":"Vision AI based monitoring","text":"<p>VisionAI based emergency rescue solution leverages cutting-edge AI-powered, real-time detection and monitoring of emergency exits with alerts and warnings for safe and secure evacuation in times of crisis. </p> <p>With our Blocked Exit Monitoring solution, you can rest assured that your premises are being monitored continuously, providing early warnings in case of any blockages in emergency exits. Our models can be deployed instantly and can augment your existing camera infrastructure with just a few clicks.</p>"},{"location":"scenarios/blocked-exit/#events","title":"Events","text":"<p>VisionAI model's generated events would be:</p> <ul> <li>Blocked exit detected</li> </ul>"},{"location":"scenarios/blocked-exit/#configuration","title":"Configuration","text":"<p>It is recommended to set up camera to monitor blocked exits in the workplace. The location of cameras to monitor blocked exits will depend on the specific policies being enforced and the nature of the work environment.</p>"},{"location":"scenarios/blocked-exit/#model-details","title":"Model Details","text":""},{"location":"scenarios/blocked-exit/#dataset","title":"Dataset","text":"<p>The dataset consists of images and videos collected from diverse sources and is designed to reflect real-world scenarios. It is evenly distributed with:</p> <ul> <li>Images of blocked exits: These images would provide visual examples of what a typical blocked exit looks like.</li> </ul> <p>Images of different types of blockages which includes: </p> <ul> <li> <p>Images of debris: Images of different types of debris, such as fallen objects or construction materials, would help the model to recognize obstructions in the path to the exit.</p> </li> <li> <p>Images of fire: Providing images of fire, smoke, and flames would help the model to recognize the different visual parameters that indicate a fire hazard.</p> </li> <li> <p>Images of toxic gasses: Images of different types of toxic gasses, such as carbon monoxide, would help the model to recognize the signs of gas leaks and other potential hazards.</p> </li> <li> <p>Images of overcrowding: Providing images of overcrowded spaces would help the model to recognize the risk of blockages in case of an emergency.</p> </li> <li> <p>Images of unobstructed exits: Providing images of exits that are not blocked would help the model to distinguish between blocked and unblocked exits.</p> </li> </ul>"},{"location":"scenarios/blocked-exit/#model","title":"Model","text":"<p>The model to monitor blocked exits is in progress and it will be released soon.</p>"},{"location":"scenarios/blocked-exit/#scenario-details","title":"Scenario details","text":"<p>The business logic for this scenario is as follows: - We use existing camera feeds from the premises to monitor blocked exits - VisionAI system is run at the edge. It uses the camera feeds for processing.</p> Test now with online Web-CamWith RTSP Camera - PipelinesWith Azure Setup <p>To test this model &amp; scenario, you can use the following steps:</p> <ul> <li> <p>Install the visionai package from PyPI</p> <pre><code>$ pip install visionai\n</code></pre> </li> <li> <p>Test the scenario from your local web-cam</p> <pre><code>$ visionai scenario test blocked-exit\n</code></pre> <p>Downloading models for scenario: blocked-exit Model: blocked-exit: https://workplaceos.blob.core.windows.net/models/yolov5s-people/yolov5s-people-0.0.4.zip</p> <p>Starting scenario: blocked-exit..</p> <p>```</p> </li> <li> <p>You should be able to see the events generated on your console window with the detections of maximum occupancy event within the camera field of view.</p> </li> </ul> <p>[TODO]</p> <p>VisionAI app is available at a Azure Market place, one can download and use it by following steps mentioned here</p>"},{"location":"scenarios/blocked-exit/#features","title":"Features","text":"<p>Some potential features of VisionAI for monitoring maximum occupancy include:</p> <ul> <li> <p>Real-time monitoring of maximum occupancy: VisionAI can monitor blocked exits in real-time, providing an automated and seamless approach to crowd management.</p> </li> <li> <p>Instant alerts and warnings: VisionAI can send instant alerts and warnings for detected blocked exits</p> </li> <li> <p>Easy to deploy: VisionAI can be easily deployed with minimal effort, allowing businesses to leverage our AI-based technology with minimal effort.</p> </li> </ul>"},{"location":"scenarios/blocked-exit/#training-with-custom-data","title":"Training with custom data","text":"<p>The scenario is provided as part of our GPL-v3 package for VisionAI. If you wish to train this with custom datasets, please contact us and we can provide you with the training code. You can do custom training with your own datasets for free, as long as it complies with GPLv3 license (you give back the code to the community). If you are interested in a custom license, please contact us.</p>"},{"location":"scenarios/blocked-exit/#contact-us","title":"Contact Us","text":"<ul> <li>For technical issues, you can open a Github issue here.</li> <li>For business inquiries, you can contact us through our website.</li> </ul>"},{"location":"scenarios/blur-documents/","title":"Document blur","text":"<p>Ensure the privacy of individuals by obscuring documents and other text in camera feeds</p> <p></p>"},{"location":"scenarios/blur-documents/#overview","title":"Overview","text":"<p>Blurring documents and other text is a common technique used to protect the privacy of individuals in images and videos. A computer vision-based model is used in the process to make the characters on a document/text unreadable.</p> <p>Blurring documents can be an effective way to protect sensitive information and preserve privacy. By blurring or obscuring sensitive information on a document, you can help prevent others from accessing confidential information or using the information for malicious purposes.</p> <p>Document blurring technology can be relatively easy to use and understand. The algorithm detects documents/text in the scene and blurs them. The tool can be used on images as well as on videos.</p>"},{"location":"scenarios/blur-documents/#vision-ai-based-monitoring","title":"Vision AI-based monitoring","text":"<p>Vision AI-based model for document blurring can be particularly useful when sharing documents containing personally identifiable information (PII), such as social security numbers, driver's license numbers, and financial account numbers. It can also be useful when sharing documents that contain trade secrets, confidential business information, or other sensitive data that could be used to harm individuals or organizations.</p> <p>The model uses a detection algorithm followed by computer vision techniques to obscure texts in images and videos. The model works in a way that it ensures that documents are fully and effectively obscured so that it cannot be read or easily recovered by others.</p> <p>There are many use cases for document blurring, where blurring or obscuring sensitive information can help protect privacy and prevent unauthorized access to sensitive data. Here are some common use cases:</p> <ul> <li> <p>Redacting personal information in legal documents: Legal documents often contain sensitive information, such as social security numbers, addresses, and birth dates. By blurring or redacting this information, legal professionals can help protect their clients' privacy and prevent identity theft.</p> </li> <li> <p>Blurring confidential business information in corporate documents: Corporate documents, such as financial reports and contracts, often contain confidential business information. Blurring or redacting this information can help protect the company's intellectual property.</p> </li> <li> <p>Blurring identifying information in online images: Online images, including social media posts and blog articles, often include identifiable information. By blurring this information, content creators can help protect the privacy of individuals.</p> </li> <li> <p>Blurring confidential information in government documents: Government documents, including classified information and sensitive documents, often require blurring or redaction to prevent unauthorized access to confidential information.</p> </li> </ul>"},{"location":"scenarios/blur-documents/#dataset","title":"Dataset","text":"<p>The dataset for this scenario consists of images and videos with different types of documents. It is constructed in a manner to reflect real-world intricacies. The dataset has documents with:</p> <ul> <li>Variations in the environment</li> <li>Different types of documents</li> <li>Different distances from the camera</li> <li>Different lighting conditions</li> <li>Various camera angles and resolutions</li> <li>Using security camera feeds</li> </ul>"},{"location":"scenarios/blur-documents/#model","title":"Model","text":"<p>The model is based on the YOLOv5 algorithm to detect documents. Document/text blurring is performed using computer vision-based blurring algorithms. The model is developed in a way that it generalizes well for different environments and situations.</p> <p>The document blurring model based on Yolov5 recorded the following performance metrics:</p> Model Name Precision Recall  mAP   DOCUMENT BLUR 97.4%  97.5%  99.1%  <p>The model is adaptable enough to run on any edge computing device.</p>"},{"location":"scenarios/blur-documents/#scenario-details","title":"Scenario details","text":"<p>The business logic for this scenario is as follows: </p> <ul> <li>We use existing camera feeds from the premises to ensure the privacy of individuals.</li> <li>VisionAI system is able to run on edge devices. It uses camera feeds for processing. </li> <li>We detect and blur the documents identified in this camera feed.</li> </ul>"},{"location":"scenarios/blur-documents/#try-it-now","title":"Try it now","text":""},{"location":"scenarios/blur-documents/#quick-method-using-your-local-web-cam","title":"Quick method - using your local web-cam","text":"<p>To test this model &amp; scenario, you can use the following steps:</p> <ul> <li>Install the visionai package from PyPI</li> </ul> <pre><code>$ pip install visionai\n---&gt; 100%\n</code></pre> <ul> <li>Test the scenario from your local web-cam</li> </ul> <pre><code>$ visionai scenario test doc-blur\n\nDownloading models for scenario: doc-blur Model: doc-blur: https://workplaceos.blob.core.windows.net/models/yolov5s-people/yolov5s-people-0.0.4.zip\n---&gt; 100%\nStarting scenario: doc-blur..\n</code></pre>"},{"location":"scenarios/blur-documents/#in-an-actual-environment","title":"In an actual environment","text":"<p>To use this scenario in an actual environment, you can follow these steps:</p> <ul> <li>Install the visionai package from PyPI</li> </ul> <pre><code>$ pip install visionai\n---&gt; 100%\n</code></pre> <ul> <li>Download the scenario</li> </ul> <pre><code>$ visionai scenario download doc-blur\n\nDownloading models for scenario: doc-blur\nModel: doc-blur\nhttps://workplaceos.blob.core.windows.net/models/yolov5s-people/yolov5s-people-0.0.4.zip\n---&gt; 100%\n</code></pre> <ul> <li>Add the camera feed to the scenario</li> </ul> <pre><code>$ visionai camera add OFFICE-01 --url rtsp://192.168.0.1/stream1\n$ visionai camera OFFICE-01 add-scenario doc-blur\n$ visionai run\n\nStarting scenario: doc-blur..\n</code></pre> <p>For more details visit VisionAI web application</p>"},{"location":"scenarios/blur-documents/#training-with-custom-data","title":"Training with custom data","text":"<p>The scenario is provided as part of our GPL-v3 package for VisionAI. If you wish to train this with custom datasets, please contact us and we can provide you with the training code. You can do custom training with your own datasets for free, as long as it complies with the GPLv3 license (you give back the code to the community). If you are interested in a custom license, please (contact us)[contact.md].</p>"},{"location":"scenarios/blur-documents/#contact-us","title":"Contact Us","text":"<ul> <li>For technical issues, you can open a Github issue here.</li> <li>For business inquiries, you can contact us through our website.</li> </ul>"},{"location":"scenarios/blur-faces/","title":"Face Blur","text":"<p>Ensure the privacy of individuals in public spaces</p> <p> </p> Face blur as part of preprocessing"},{"location":"scenarios/blur-faces/#overview","title":"Overview","text":"<p>Face blurring is a privacy model which is becoming increasingly popular in the digital age. It involves the use of technology to blur or obscure the facial features of individuals in digital images and videos. This technology can be used to protect the identity of individuals in images.</p> <p>The concept of face blurring is based on the idea that a person\u2019s identity should remain private, and that images of a person should not be shared without their consent. In a world where people are increasingly sharing images and videos of themselves and others, face blurring is becoming a necessary tool to protect people\u2019s privacy. This technology can be used to blur the faces of individuals in images, or even to remove them entirely.</p> <p>The face blurring technology is designed to be easy to use and understand. It can be used on both still images and videos, and can be applied in a matter of seconds with just a few clicks. It is also fairly simple to configure and requires no technical expertise. The user simply choose the image or video that they want to blur and the algorithm will automatically detect and blur the faces.</p>"},{"location":"scenarios/blur-faces/#vision-ai-based-monitoring","title":"Vision AI-based monitoring","text":"<p>Vision AI-based Model for Face Blurring is designed to ensure that the privacy of individuals is respected while still allowing the public to have access to the video feed.</p> <p>This model uses a combination of facial recognition algorithms and image processing techniques to automatically blur faces in real-time video streams. The system is designed to detect faces in real-time, and then blur them out so that they are not recognizable. This model has been used in various applications including public surveillance, online video streaming, and social media platforms.</p>"},{"location":"scenarios/blur-faces/#model-details","title":"Model Details","text":""},{"location":"scenarios/blur-faces/#dataset","title":"Dataset","text":"<p>WIDER FACE dataset is a face detection benchmark dataset, of which images are selected from the publicly available WIDER dataset. WIDER FACE dataset is organized based on 61 event classes. For each event class, we randomly select 40%/10%/50% data as training, validation and testing sets.  The dataset contains faces with:</p> <ul> <li>Variant illumination scene images</li> <li>Multiple face expressions</li> <li>Different lighting conditions</li> <li>Variations in scale, pose and occlusion</li> </ul> <p>Total number of mages used was 32,203</p>"},{"location":"scenarios/blur-faces/#model","title":"Model","text":"<p>The model is based off of the YOLOv5-face algorithm. The model is trained on WIDER FACE dataset. We intend to develop a model that generalizes well in real world situations. Implemented a custom logic for face blurring with the help of face detections from yolo face.</p> <p>The model recorded the following performance metrics:</p> Precision  Recall  mAP  <p>The model is adaptable enough to run on any edge computing device.</p>"},{"location":"scenarios/blur-faces/#scenario-details","title":"Scenario details","text":"<p>The business logic for this scenario is as follows: </p> <ul> <li>We use existing camera feeds from the premises to Ensure the privacy of individuals.</li> <li>VisionAI system is able to run on edge devices. It uses camera feeds for processing. </li> <li>We detect and blur the faces identified in this camera feed.</li> </ul> Test now with online Web-CamWith RTSP Camera - PipelinesWith Azure Setup <p>To test this model &amp; scenario, you can use the following steps:</p> <ul> <li> <p>Install the visionai package from PyPI</p> <pre><code>$ pip install visionai\n</code></pre> </li> <li> <p>Test the scenario from your local web-cam</p> <pre><code>$ visionai scenario test face-blur\n\nDownloading models for scenario: face-blur\nModel: face-blur: https://workplaceos.blob.core.windows.net/models/yolov5s-face-blur/yolov5s-face-blur-0.0.1.zip\nStarting scenario: face-blur..\n</code></pre> </li> <li> <p>You should be able to see faces being blurred as part of preprocessing.</p> </li> </ul> <p>[TODO]</p> <p>VisionAI app is available at a Azure Market place, one can download and use it by following steps mentioned here</p>"},{"location":"scenarios/blur-faces/#training-with-custom-data","title":"Training with custom data","text":"<p>The scenario is provided as part of our GPL-v3 package for VisionAI. If you wish to train this with custom datasets, please contact us and we can provide you with the training code. You can do custom training with your own datasets for free, as long as it complies with GPLv3 license (you give back the code to the community). If you are interested in a custom license, please contact us.</p>"},{"location":"scenarios/blur-faces/#contact-us","title":"Contact Us","text":"<ul> <li>For technical issues, you can open a Github issue here.</li> <li>For business inquiries, you can contact us through our website.</li> </ul>"},{"location":"scenarios/blur-licence-plates/","title":"Licence  Plates blur","text":"<p>Ensure the privacy of vehicle owners in images and videos by blurring licence plates</p> <p></p>"},{"location":"scenarios/blur-licence-plates/#overview","title":"Overview","text":"<p>Blurring license plates is a common technique used to protect the privacy of vehicle owners in images and videos. The process involves using computer vision based model to obscure the alphanumeric characters on a license plate, making them unreadable.</p> <p>Blurring license plates can be an effective way to protect the privacy of vehicle owners, particularly in situations where their vehicles may be captured on camera without their knowledge or consent. This can include surveillance footage, dashcam footage, or even photographs taken by bystanders.</p> <p>Licence plate blurring technology can be relatively easy to use and understand. The algorithm detects licence plates and blurs them. The tool can be used on images as well as on videos.</p>"},{"location":"scenarios/blur-licence-plates/#vision-ai-based-monitoring","title":"Vision AI-based monitoring","text":"<p>Vision AI-based model for license plate blurring can be a useful and intended application in certain contexts, such as when capturing images or video in public places or in situations where license plates may contain sensitive or identifying information. This technology can help protect the privacy and security of individuals by blurring or obscuring license plate numbers.</p> <p>The model uses a detection algorithm followed by computer vision techniques to obscure licence plates in images and videos. This model has been can be used for various applications including privacy protection, surveillance or investigation operations.</p> <p>Overall, license plate blurring models have a range of applications in various industries, all of which aim to protect individual privacy and prevent the misuse of sensitive information.</p>"},{"location":"scenarios/blur-licence-plates/#dataset","title":"Dataset","text":"<p>The datasets for this scenario is consists of images and videos with licence plates. It is compiled in a manner to reflect real-world complexities. The dataset has licence plaes with:</p> <ul> <li>Variations in the environment</li> <li>Different seasonal changes</li> <li>Different types of vehicles</li> <li>Different distances from the camera</li> <li>Different lighting conditions</li> <li>Various camera angles and resolutions</li> <li>Using security camera feeds</li> </ul> <p>Total number of images used was 23,219.</p>"},{"location":"scenarios/blur-licence-plates/#model","title":"Model","text":"<p>The model is based on the YOLOv5 algorithm to detect licence plates. It is trained on the curated dataset. Licence plate blurring is performed using computer vision-based blurring operations. The model is developed in a way that it generalizes well for different environments and situations.</p> <p>The licence plate detection model based on Yolov5 recorded the following performance metrics:</p> Model Name Precision Recall  mAP   LICENSE PLATE BLUR 97.4%   96.3%   98.4%   <p>The model is adaptable enough to run on any edge computing device.</p>"},{"location":"scenarios/blur-licence-plates/#scenario-details","title":"Scenario details","text":"<p>The business logic for this scenario is as follows: </p> <ul> <li>We use existing camera feeds from the premises to ensure the privacy of vehicle owners.</li> <li>VisionAI system is able to run on edge devices. It uses camera feeds for processing. </li> <li>We detect and blur the licence plates identified in this camera feed.</li> </ul>"},{"location":"scenarios/blur-licence-plates/#try-it-now","title":"Try it now","text":""},{"location":"scenarios/blur-licence-plates/#quick-method-using-your-local-web-cam","title":"Quick method - using your local web-cam","text":"<p>To test this model &amp; scenario, you can use the following steps:</p> <ul> <li>Install the visionai package from PyPI</li> </ul> <pre><code>$ pip install visionai\n---&gt; 100%\n</code></pre> <ul> <li>Test the scenario from your local web-cam</li> </ul> <pre><code>$ visionai scenario test licence-plate-blur\n\nDownloading models for scenario: licence-plate-blur Model: licence-plate-blur: https://workplaceos.blob.core.windows.net/models/yolov5s-people/yolov5s-people-0.0.4.zip\n---&gt; 100%\nStarting scenario: licence-plate-blur..\n</code></pre>"},{"location":"scenarios/blur-licence-plates/#in-an-actual-environment","title":"In an actual environment","text":"<p>To use this scenario in an actual environment, you can follow these steps:</p> <ul> <li>Install the visionai package from PyPI</li> </ul> <pre><code>$ pip install visionai\n---&gt; 100%\n</code></pre> <ul> <li>Download the scenario</li> </ul> <pre><code>$ visionai scenario download licence-plate-blur\n\nDownloading models for scenario: licence-plate-blur\nModel: licence-plate-blur\nhttps://workplaceos.blob.core.windows.net/models/yolov5s-people/yolov5s-people-0.0.4.zip\n---&gt; 100%\n</code></pre> <ul> <li>Add the camera feed to the scenario</li> </ul> <pre><code>$ visionai camera add OFFICE-01 --url rtsp://192.168.0.1/stream1\n$ visionai camera OFFICE-01 add-scenario licence-plate-blur\n$ visionai run\n\nStarting scenario: licence-plate-blur..\n</code></pre> <p>For more details visit VisionAI web application</p>"},{"location":"scenarios/blur-licence-plates/#training-with-custom-data","title":"Training with custom data","text":"<p>The scenario is provided as part of our GPL-v3 package for VisionAI. If you wish to train this with custom datasets, please contact us and we can provide you with the training code. You can do custom training with your own datasets for free, as long as it complies with the GPLv3 license (you give back the code to the community). If you are interested in a custom license, please (contact us)[contact.md].</p>"},{"location":"scenarios/blur-licence-plates/#contact-us","title":"Contact Us","text":"<ul> <li>For technical issues, you can open a Github issue here.</li> <li>For business inquiries, you can contact us through our website.</li> </ul>"},{"location":"scenarios/compliance-policies/","title":"Compliance policies","text":"<p>Compliance policies are put in place by companies to ensure the safety of employees, customers, and the general public. To send real-time alerts to employees, managers, and other stakeholders if there is a potential violation or deviation from established policies and procedures.</p> <p>These are a set of rules and regulations that a company creates and enforces to ensure that it operates in accordance with applicable laws, regulations, and ethical standards. The purpose of compliance policies is to help companies prevent legal and ethical violations, promote responsible conduct, and maintain their reputation and public trust. Some of these are  No pictures and no mobile phones in certain areas etc. There are many events that could trigger an alert for non-adherence to a compliance policy. Here are a few use cases:</p> <ul> <li>No food or drinks</li> <li>No phone, text, pictures</li> <li>No Smoking zones</li> <li>No children/visitors</li> <li>Waste Management</li> <li>Energy Conservation</li> <li>Restricted Areas</li> </ul>"},{"location":"scenarios/confined-spaces-monitoring/","title":"Confined Spaces Monitoring","text":"<p>Ensure safety of employees in confined spaces. Get real-time alerts when workers are present in the space for too long.</p>"},{"location":"scenarios/confined-spaces-monitoring/#overview","title":"Overview","text":"<p>Confined spaces refer to areas that are partially or fully enclosed and are not designed for continuous human occupancy. Examples include tanks, silos, storage bins, manholes, and underground vaults. These spaces can be hazardous due to limited ventilation, lack of natural light, and potential for hazardous atmospheric conditions.</p> <p>Workers entering confined spaces are at risk of being overcome by toxic gases, asphyxiation, or other hazards. In addition, workers may be trapped in the confined space if an emergency occurs. Therefore, it is important to monitor confined spaces to ensure that they remain safe for workers.</p> <p>To monitor confined spaces, cameras can be used - with an oversight manager observing these spaces. Along with cameras other IoT devices can be used to measure atmospheric conditions such as air quality, temperature, humidity, and toxic gas levels.</p>"},{"location":"scenarios/confined-spaces-monitoring/#vision-ai-based-monitoring","title":"Vision AI based monitoring","text":"<p>Vision AI based monitors can be used to monitor confined spaces by providing real-time video feeds of the area. These cameras can be used to monitor the presence of workers in the confined space, as well as the duration of how long they are present within the space. Companies can put compliance policies in place to ensure that workers are not present in the confined space for an extended period of time. Camera based monitors can track workers entering the premises, and monitor their total duration of stay; and if that exceeds the compliance policy, an alert can be raised.</p> <p>It is important to note that these camera based monitoring provides should be supplanted by strong compliance processes to ensure their accuracy and reliability. In addition, workers entering confined spaces should always be trained on proper use of the monitoring equipment and be familiar with the hazards associated with confined spaces.</p>"},{"location":"scenarios/confined-spaces-monitoring/#model-details","title":"Model Details","text":""},{"location":"scenarios/confined-spaces-monitoring/#dataset","title":"Dataset","text":"<p>The datasets for this scenario is based off of people detection and tracking algorithms that are used in the industry. The dataset is a combination of images and videos from various sources. The dataset is curated to ensure that it is representative of the real world. It has equal distributions for:</p> <ul> <li>Indoor vs Outdoor environments</li> <li>Male vs Female</li> <li>Day vs Night</li> <li>Different types of clothing</li> <li>Different distances from the camera</li> <li>Various lighting conditions</li> <li>Various camera angles and resolutions</li> <li>Using seurity camera feeds</li> </ul> <p>Total number of images used was 387,644</p>"},{"location":"scenarios/confined-spaces-monitoring/#model","title":"Model","text":"<p>The model is based off of the YOLOv5 algorithm. The model is trained on a custom dataset of images and videos. The model is trained based on the above dataset curated by our team.</p> <p>The model provides the following metrics:</p> Precision  Recall  mAP  <p>The model is light-weight enough to be run on any edge devices.</p>"},{"location":"scenarios/confined-spaces-monitoring/#scenario-details","title":"Scenario details","text":"<p>The business logic for this scenario is as follows: - We use existing camera feeds from the premises to monitor the presence of workers in the confined space. - VisionAI system is run at the edge. It uses the camera feeds for processing. - We detect and track people identified in this camera feed. - We monitor the total duration of stay of these people in the confined space. - If the duration of stay exceeds the compliance policy, an alert is raised.</p> Test now with online Web-CamWith RTSP Camera - PipelinesWith Azure Setup <p>To test this model &amp; scenario, you can use the following steps:</p> <ul> <li> <p>Install the visionai package from PyPI</p> <pre><code>$ pip install visionai\n</code></pre> </li> <li> <p>Test the scenario from your local web-cam</p> <pre><code>$ visionai scenario test obstructed-camera-detection\n\nDownloading models for scenario: obstructed-camera-detection\nModel: obstructed-camera-detection: https://workplaceos.blob.core.windows.net/models/yolov5s-obstructed-camera-detection/yolov5s-obstructed-camera-detection-0.0.1.zip\nStarting scenario: obstructed-camera-detection..\n</code></pre> </li> <li> <p>You should be able to see the events generated on your console window with the detections of people exceeding the duration limit within the camera field of view.</p> </li> </ul> <p>[TODO]</p> <p>VisionAI app is available at a Azure Market place, one can download and use it by following steps mentioned here</p>"},{"location":"scenarios/confined-spaces-monitoring/#events-supported","title":"Events Supported","text":"<p>This scenario supports the following events:</p> <ul> <li>Person detected: This event is generated when a person is detected in the camera feed.</li> <li>Person left: This event is generated when a person is no longer detected in the camera feed.</li> <li>Person duration exceeded: This event is generated when a person is detected for more than the specified duration in the camera feed. The duration amount is configurable through the web-app.</li> </ul>"},{"location":"scenarios/confined-spaces-monitoring/#training-with-custom-data","title":"Training with custom data","text":"<p>The scenario is provided as part of our GPL-v3 package for VisionAI. If you wish to train this with custom datasets, please contact us and we can provide you with the training code. You can do custom training with your own datasets for free, as long as it complies with GPLv3 license (you give back the code to the community). If you are interested in a custom license, please contact us.</p>"},{"location":"scenarios/confined-spaces-monitoring/#contact-us","title":"Contact Us","text":"<ul> <li>For technical issues, you can open a Github issue here.</li> <li>For business inquiries, you can contact us through our website.</li> </ul>"},{"location":"scenarios/custom-scenarios/","title":"Custom scenarios","text":"<p>Coming soon</p>"},{"location":"scenarios/employee-privacy/","title":"Employee Privacy","text":"<p>Employee privacy is a concern for many companies. The use of cameras in the workplace can be a sensitive issue. Employees may feel that their privacy is being violated. They may also feel that their personal space is being invaded. This can lead to a loss of trust and confidence in the company.</p> <p>Privacy policies are put in place to protect employees from unwarranted surveillance. They also help to ensure that employees are not subjected to any form of discrimination or harassment.</p> <p>To send real-time alerts to employees, managers, and other stakeholders if there is a potential violation or deviation from established policies and procedures.</p> <p>These are a set of rules and regulations that a company creates and enforces to ensure that it operates in accordance with applicable laws, regulations, and ethical standards. The purpose of compliance policies is to help companies prevent legal and ethical violations, promote responsible conduct, and maintain their reputation and public trust. Some of these are  No pictures and no mobile phones in certain areas etc. There are many events that could trigger an alert for non-adherence to privacy policies. Here are a few use cases:</p> <ul> <li>Blur faces</li> <li>Blur signs/text</li> <li>Blur screens</li> <li>Blur license plates</li> <li>Obstructed camera view</li> </ul>"},{"location":"scenarios/empty-pallets/","title":"Empty Pallet Detection","text":"<p>Maximize warehouse productivity and safety with our reliable empty pallet detection model.</p>"},{"location":"scenarios/empty-pallets/#overview","title":"Overview","text":"<p>Empty pallet detection is the process of identifying whether a pallet is empty or not in a warehouse or manufacturing facility. This is important because it can help optimize storage space and ensure efficient inventory management. Empty pallet detection can be automated through the use of various technologies such as sensors, cameras, and machine learning algorithms.</p> <p>Empty pallet detection can increase productivity in several ways:</p> <ul> <li> <p>Time savings: With empty pallet detection technology, the palletizer can quickly identify an empty pallet and remove it from the production line. This can save valuable time that would otherwise be spent manually inspecting pallets for emptiness, or worse, mistakenly stacking products on top of an empty pallet.</p> </li> <li> <p>Reduced errors: Empty pallet detection technology can help reduce errors in the production process by ensuring that products are stacked only on properly loaded pallets. This can reduce the likelihood of accidents or damage to products, which can further reduce downtime and the need for rework.</p> </li> <li> <p>Increased throughput: When empty pallets are detected and removed from the production line quickly, the palletizer can keep working at a high rate of speed without interruptions. This can increase the overall throughput of the production line, allowing more products to be packaged and shipped out in a shorter amount of time.</p> </li> <li> <p>Improved safety: Empty pallet detection technology can also help improve safety in the workplace by preventing the stacking of products on unstable or improperly loaded pallets. This can reduce the risk of workplace accidents and injuries, which can further reduce downtime and lost productivity.</p> </li> </ul>"},{"location":"scenarios/empty-pallets/#vision-ai-based-monitoring","title":"Vision AI based monitoring","text":"<p>Vision AI-based system can be used to detect empty pallets in real-time. This can help improve productivity in several key areas, including time savings, reduced errors, increased throughput, and improved safety. Manufacturers can easily streamline their production processes and optimize their workflows, ultimately improving their bottom line.</p> <p>Empty pallet detection is an important task in many warehouse and manufacturing settings. By identifying empty pallets, companies can optimize storage space and ensure efficient inventory management. There are various ways to detect empty pallets, including manual inspections by workers and automated methods that use sensors, cameras, and machine learning algorithms. Automation can greatly improve the efficiency and accuracy of empty pallet detection, leading to cost savings and improved productivity.</p>"},{"location":"scenarios/empty-pallets/#model-details","title":"Model Details","text":""},{"location":"scenarios/empty-pallets/#dataset","title":"Dataset","text":"<p>The dataset consists of images and videos collected from various sources. </p>"},{"location":"scenarios/empty-pallets/#model","title":"Model","text":"<p>The model to detect empty pallets is in progress and it will be released soon.</p>"},{"location":"scenarios/empty-pallets/#scenario-details","title":"Scenario details","text":"<p>The business logic for this scenario is as follows:</p> <ul> <li> <p>We use existing camera feeds from the premises to monitor pallets in real-time, allowing it to quickly detect when a pallet is empty.</p> </li> <li> <p>VisionAI system is able to run on edge devices. It uses camera feeds for processing.</p> </li> <li> <p>When an instance of empty pallet is detected, an alert will be raised.</p> </li> </ul>"},{"location":"scenarios/empty-pallets/#features","title":"Features","text":"<ul> <li> <p>Real-Time Monitoring: The AI model is designed to monitor pallets in real-time, allowing it to quickly detect when a pallet becomes empty. This feature is especially useful in warehouse and logistics settings where timely pallet retrieval is crucial to maintaining efficient operations.</p> </li> <li> <p>Customization: The empty pallet detection AI model can be customized to suit the specific needs of different businesses. For example, it can be configured to detect different types of pallets or to operate under different lighting conditions.</p> </li> <li> <p>Integration: The AI model can be integrated with other software and hardware systems, such as warehouse management systems, to provide seamless operation and maximize efficiency.</p> </li> <li> <p>Scalability: The AI model can be scaled up or down to meet the needs of businesses of different sizes. Whether a business operates a single warehouse or multiple warehouses across different locations, the AI model can be adapted to suit its needs.</p> </li> </ul>"},{"location":"scenarios/empty-pallets/#training-with-custom-data","title":"Training with custom data","text":"<p>The scenario is provided as part of our GPL-v3 package for VisionAI. If you wish to train this with custom datasets, please contact us and we can provide you with the training code. You can do custom training with your own datasets for free, as long as it complies with GPLv3 license (you give back the code to the community). If you are interested in a custom license, please contact us.</p>"},{"location":"scenarios/empty-pallets/#contact-us","title":"Contact Us","text":"<ul> <li>For technical issues, you can open a Github issue here.</li> <li>For business inquiries, you can contact us through our website.</li> </ul>"},{"location":"scenarios/energy-conservation/","title":"Energy Conservation","text":"<p>Going Green: Innovative Solutions for Energy Conservation and Sustainable Living with Vision AI.</p> <p> </p> Energy Conservation event"},{"location":"scenarios/energy-conservation/#overview","title":"Overview","text":"<p>Energy conservation refers to the practice of reducing energy consumption and waste, in order to minimize the impact on the environment and lower energy costs. Artificial intelligence (AI) techniques can be used to optimize energy consumption, by analyzing data and identifying patterns that can be used to improve energy efficiency.</p> <p>One application of AI in energy conservation is in the development of smart grids, which use sensors and other monitoring technologies to collect data on energy consumption and distribution. AI algorithms can then be used to analyze this data and identify ways to optimize energy usage, such as by adjusting power generation and distribution to match demand.</p> <p>Another application of AI in energy conservation is in the development of smart buildings, which use sensors and other monitoring technologies to optimize energy usage within a building. AI algorithms can analyze data on factors such as temperature, occupancy, and lighting levels, and adjust energy usage accordingly to minimize waste and reduce costs.</p> <p>Overall, the use of AI techniques in energy conservation is an important area of research and development, with the potential to significantly reduce energy consumption and promote sustainable living. As AI technology continues to evolve, it is likely that we will see even more innovative solutions for energy conservation and environmental sustainability in the years to come.</p>"},{"location":"scenarios/energy-conservation/#vision-ai-based-monitoring","title":"Vision AI based monitoring","text":"<p>Vision AI based monitors can be used for the detection of energy conservation events by providing real-time video feeds of the factory area. The cameras scan every frame and monitor energy consumption and identify opportunities for energy conservation. For example, computer vision can analyze building occupancy patterns to optimize heating and cooling systems, identify lighting fixtures that consume excessive energy, or identify appliances and equipment that need to be replaced to improve energy efficiency.</p>"},{"location":"scenarios/energy-conservation/#events","title":"Events","text":"<p>VisionAI model's generated events would be:</p> <ul> <li>Occupancy pattern summary &amp; lighting pattern recommendations</li> </ul>"},{"location":"scenarios/energy-conservation/#configuration","title":"Configuration","text":"<p>It is recommended to set up camera to monitor energy conservation events.</p>"},{"location":"scenarios/energy-conservation/#model-details","title":"Model Details","text":""},{"location":"scenarios/energy-conservation/#dataset","title":"Dataset","text":"<p>The dataset for this scenario is based on real-world detection of energy conservation events. The dataset consists of images and videos collected from various sources. </p>"},{"location":"scenarios/energy-conservation/#model","title":"Model","text":"<p>The model to detect energy conservation is in progress and it will be released soon.</p>"},{"location":"scenarios/energy-conservation/#scenario-details","title":"Scenario details","text":"<p>The business logic for this scenario is as follows:</p> <ul> <li>We use existing camera feeds from the premises for raising energy conservation events.</li> <li>VisionAI system is able to run on edge devices. It uses camera feeds for processing.</li> <li>From the camera feed monitor energy consumption and identify opportunities for energy conservation. For example, computer vision can analyze building occupancy patterns to optimize heating and cooling systems, identify lighting fixtures that consume excessive energy, or identify appliances and equipment that need to be replaced to improve energy efficiency.</li> <li>If energy conservation event is detected, an alert is raised.</li> </ul> Test now with online Web-CamWith RTSP Camera - PipelinesWith Azure Setup <p>To test this model &amp; scenario, you can use the following steps:</p> <ul> <li> <p>Install the visionai package from PyPI</p> <pre><code>$ pip install visionai\n</code></pre> </li> <li> <p>Test the scenario from your local web-cam</p> <pre><code>$ visionai scenario test energy-conservation\n\nDownloading models for scenario: energy-conservation\nStarting scenario: energy-conservation..\n</code></pre> </li> <li> <p>You should be able to see the events generated on your console window with the detections of energy conservation within the camera field of view.</p> </li> </ul> <p>[TODO]</p> <p>VisionAI app is available at a Azure Market place, one can download and use it by following steps mentioned here</p>"},{"location":"scenarios/energy-conservation/#features","title":"Features","text":"<p>The VisionAI solution is the most efficient way of implementing this scenario, as evidenced by the following features:</p> <ul> <li> <p>Real-time monitoring - VisionAI can be used to monitor energy conservation events in real-time, using camera feeds from the premises.</p> </li> <li> <p>Edge computing - VisionAI can be deployed on edge devices, such as Raspberry Pi, to monitor energy conservation events in real-time, using camera feeds from the premises.</p> </li> <li> <p>Cloud computing - VisionAI can also be deployed on cloud servers, such as AWS, to monitor energy conservation events in real-time, using camera feeds from the premises.</p> </li> </ul>"},{"location":"scenarios/energy-conservation/#training-with-custom-data","title":"Training with custom data","text":"<p>The scenario is provided as part of our GPL-v3 package for VisionAI. If you wish to train this with custom datasets, please contact us and we can provide you with the training code. You can do custom training with your own datasets for free, as long as it complies with GPLv3 license (you give back the code to the community). If you are interested in a custom license, please (contact us)[contact.md].</p>"},{"location":"scenarios/energy-conservation/#contact-us","title":"Contact Us","text":"<ul> <li>For technical issues, you can open a Github issue here.</li> <li>For business inquiries, you can contact us through our website.</li> </ul>"},{"location":"scenarios/environment-energy-usage/","title":"Energy Usage Monitoring","text":"<p>Empowering Sustainability: Innovations in Energy Usage Monitoring and Management with Vision AI.</p> <p> </p> Energy Usage Monitoring events"},{"location":"scenarios/environment-energy-usage/#overview","title":"Overview","text":"<p>Energy usage monitoring refers to the process of collecting and analyzing data on how energy is being used in buildings, facilities, and other settings. The purpose of energy usage monitoring is to identify opportunities for energy conservation and efficiency improvements, reduce energy waste and costs, and promote sustainability.</p> <p>Overall, energy usage monitoring is an essential component of sustainable energy management, helping to reduce energy waste, lower costs, and promote a more sustainable future.</p>"},{"location":"scenarios/environment-energy-usage/#vision-ai-based-monitoring","title":"Vision AI based monitoring","text":"<p>Vision AI based monitors can be used for the detection of energy usage monitoring events by providing real-time video feeds of the factory area. The cameras scan every frame and monitor the energy usage of machines and equipment in a factory and help identify inefficiencies, optimize processes, and reduce energy costs.</p>"},{"location":"scenarios/environment-energy-usage/#events","title":"Events","text":"<p>VisionAI model's generated events would be:</p> <ul> <li>Energy usage exceeds limit</li> </ul> <p>It is recommended that any instance of energy level's exceeding should be reported. This is because it is a sign of inefficiency and can be used to optimize processes and reduce energy costs.</p> <p>An event data for this may include the following information:</p> <ul> <li> <p>Date and Time: The date and time when the event was detected.</p> </li> <li> <p>Location: The location where the event was detected.</p> </li> <li> <p>Device: The device where the event was detected.   </p> </li> </ul>"},{"location":"scenarios/environment-energy-usage/#configuration","title":"Configuration","text":"<p>To set up a camera system for energy usage monitoring,You will need a camera capable of capturing footage of your energy meter or other relevant devices that you want to monitor. </p>"},{"location":"scenarios/environment-energy-usage/#model-details","title":"Model Details","text":""},{"location":"scenarios/environment-energy-usage/#dataset","title":"Dataset","text":"<p>The dataset for this scenario is based on real-world energy usage monitoring detection events. The dataset consists of images and videos collected from various sources. </p>"},{"location":"scenarios/environment-energy-usage/#model","title":"Model","text":"<p>The model to detect energy usage monitoring is in progress and it will be released soon.</p>"},{"location":"scenarios/environment-energy-usage/#scenario-details","title":"Scenario details","text":"<p>The business logic for this scenario is as follows:</p> <ul> <li>We use existing camera feeds from the premises for raising energy usage monitoring events.</li> <li>VisionAI system is able to run on edge devices. It uses camera feeds for processing.</li> <li>From the camera feed we identify inefficiencies, optimize processes, and reduce energy costs.</li> <li>If energy usage monitoring event is detected, an alert is raised.</li> </ul> Test now with online Web-CamWith RTSP Camera - PipelinesWith Azure Setup <p>To test this model &amp; scenario, you can use the following steps:</p> <ul> <li> <p>Install the visionai package from PyPI</p> <pre><code>$ pip install visionai\n</code></pre> </li> <li> <p>Test the scenario from your local web-cam</p> <pre><code>$ visionai scenario test energy-usage-monitoring-detection\n\nDownloading models for scenario: energy-usage-monitoring-detection\nStarting scenario: energy-usage-monitoring-detection..\n</code></pre> </li> <li> <p>You should be able to see the events generated on your console window with the detections of energy usage monitoring within the camera field of view.</p> </li> </ul> <p>[TODO]</p> <p>VisionAI app is available at a Azure Market place, one can download and use it by following steps mentioned here</p>"},{"location":"scenarios/environment-energy-usage/#features","title":"Features","text":"<p>The VisionAI solution is the most efficient way of implementing energy usage monitoring scenario, as evidenced by the following features:                           </p> <ul> <li> <p>Accurate measurement: VisionAI energy usage monitoring systems are designed to provide highly accurate measurements of energy consumption. This helps to provide reliable data for various applications.</p> </li> <li> <p>Real-time monitoring: VisionAI energy usage monitoring systems can provide real-time monitoring of energy consumption. </p> </li> <li> <p>Data analytics: VisionAI systems can use advanced data analytics to identify energy usage patterns and trends. </p> </li> <li> <p>Automated insights: VisionAI usage monitoring systems can automatically provide insights and recommendations for energy savings based on the data analysis. </p> </li> <li> <p>Integration with other systems: VisionAI usage monitoring systems can be integrated with other systems such as smart buildings, HVAC systems, and lighting systems. This helps to improve the overall efficiency and effectiveness of the systems.</p> </li> </ul>"},{"location":"scenarios/environment-energy-usage/#training-with-custom-data","title":"Training with custom data","text":"<p>The scenario is provided as part of our GPL-v3 package for VisionAI. If you wish to train this with custom datasets, please contact us and we can provide you with the training code. You can do custom training with your own datasets for free, as long as it complies with GPLv3 license (you give back the code to the community). If you are interested in a custom license, please contact us.</p>"},{"location":"scenarios/environment-energy-usage/#contact-us","title":"Contact Us","text":"<ul> <li>For technical issues, you can open a Github issue here.</li> <li>For business inquiries, you can contact us through our website.</li> </ul>"},{"location":"scenarios/environment-humidity/","title":"Humidity Monitoring","text":"<p>Ideal tool to monitor humidity in the surrounding environment.</p>"},{"location":"scenarios/environment-humidity/#overview","title":"Overview","text":"<p>Uncontrolled humidity has several implications in day-to-day life. Firstly, high humidity makes the air thicker and the surroundings uncomfortable for people. Secondly, uncontrolled humidity can build up electrostatic charges or cause corrosion. High humidity also leads to throat, eye, and skin irritation. Finally, the manufacturing process also needs to improve due to unmaintained humidity leading to compromised products. </p> <p>To understand why monitoring humidity is essential, one should first know what humidity is. It is the amount of water vapor in the air (also called absolute humidity). Relative humidity, however, is the percentage of moisture in the air compared to how much moisture the air can hold at that temperature. Therefore, both AH and RH are essential components in humidity monitoring.</p> <p>Adopting a humidity monitoring solution can help industries prevent health and monetary losses and ensure seamless workflow. However, this can be challenging, to begin with. Visionify offers a suitable solution to all sorts of humidity challenges.</p>"},{"location":"scenarios/environment-humidity/#vision-ai-based-monitoring","title":"Vision AI based monitoring","text":"<p>Sensor-Based Solution for Humidity Monitoring</p> <p>VisionAI\u2019s humidity monitoring solution helps businesses control humidity in various environments. Our sensor-based solution can monitor the humidity level and triggers an alarm/signal when levels break a threshold value. Our solution can be used for storage management, buildings and facilities management, HVAC (Heating Ventilation Air Conditioning), comfort optimization, asset condition tracking, and remote monitoring.</p>"},{"location":"scenarios/environment-humidity/#model-details","title":"Model Details","text":""},{"location":"scenarios/environment-humidity/#dataset","title":"Dataset","text":"<ul> <li> <p>Custom humidity: The model allows you to set humidity levels (absolute and relative). The alarm can be triggered when humidity in your facility reaches a high/low threshold value.</p> </li> <li> <p>Temperature: Temperature data is also required for humidity monitoring. Integrate your temperature monitoring devices with a humidity monitoring solution for a connected experience.</p> </li> <li> <p>Different distances from the camera: The dataset contains images and videos captured from different distances from the camera. This will help in capturing humidity from varied areas within the facility.</p> </li> <li> <p>Various lighting conditions: The dataset contains images and videos under different lighting conditions. The model may need to adjust its settings to continue detecting temperature accurately.</p> </li> <li> <p>Multiple camera angles and resolutions The dataset includes images taken from different angles, such as top-down, side view, or angled view, and available in varied resolutions.</p> </li> <li> <p>Using security camera feeds: The dataset includes images and videos collected from your existing security camera installations to make monitoring more accurate.</p> </li> </ul>"},{"location":"scenarios/environment-humidity/#model","title":"Model","text":"<p>The model to monitor humidity in the surrounding environment is in progress and it will be released soon.</p>"},{"location":"scenarios/environment-humidity/#scenario-details","title":"Scenario details","text":"<ul> <li>When the humidity crosses a high/low threshold value, the model can ring an alarm for a manual check.</li> <li>Share notifications with detailed insight on humidity data.</li> <li>Integrate existing temperature or pressure sensing devices with our humidity sensing solution for centralized data intelligence.</li> </ul> Test now with online Web-CamWith RTSP Camera - PipelinesWith Azure Setup <p>To test this model &amp; scenario, you can use the following steps:</p> <ul> <li> <p>Install the visionai package from PyPI</p> <pre><code>$ pip install visionai\n</code></pre> </li> <li> <p>Test the scenario from your local web-cam</p> <pre><code>$ visionai scenario test humidity-monitoring\n\nDownloading models for scenario: humidity-monitoring\nModel: humidity-monitoring: https://workplaceos.blob.core.windows.net/models/yolov5s-humidity-monitoring/yolov5s-humidity-monitoring-0.0.1.zip\nStarting scenario: humidity-monitoring..\n</code></pre> </li> <li> <p>You should be able to see the events generated on your console window when humidity crosses a high/low threshold value.</p> </li> </ul> <p>[TODO]</p> <p>VisionAI app is available at a Azure Market place, one can download and use it by following steps mentioned here</p>"},{"location":"scenarios/environment-humidity/#features","title":"Features","text":"<ul> <li>Easy to use: The model is easy to use and can be deployed in a few minutes.</li> <li>Customizable: The model can be customized to suit your needs.</li> <li>Real-time: The model can be deployed in real-time to monitor humidity in the surrounding environment.</li> <li>Scalable: The model can be scaled to monitor humidity in multiple facilities.</li> </ul>"},{"location":"scenarios/environment-humidity/#training-with-custom-data","title":"Training with custom data","text":"<p>The scenario is provided as part of our GPL-v3 package for VisionAI. If you wish to train this with custom datasets, please contact us and we can provide you with the training code. You can do custom training with your own datasets for free, as long as it complies with GPLv3 license (you give back the code to the community). If you are interested in a custom license, please contact us.</p>"},{"location":"scenarios/environment-humidity/#contact-us","title":"Contact Us","text":"<ul> <li>For technical issues, you can open a Github issue here.</li> <li>For business inquiries, you can contact us through our website.</li> </ul>"},{"location":"scenarios/environment-monitoring/","title":"Environment Monitoring","text":"<p>Temperature monitoring, Air quality, Humidity, CO2 levels, Noise levels through AI.</p>"},{"location":"scenarios/environment-monitoring/#overview","title":"Overview","text":"<p>AI-based environment monitoring models can provide real-time insights into the environmental conditions of the workplace. This can be especially useful in industries such as manufacturing, retail, and healthcare, in which the environment must be closely monitored for safety and efficiency. AI-based environment monitoring models can detect changes in temperature, humidity, and other factors, allowing employers to take action quickly to correct any issues that may arise. Additionally, AI-based models can help employers identify areas of improvement and save energy by reducing the need for manual monitoring.</p> <p>VisionAI toolkit can help with environment monitoring at a workplace by providing insights into environmental data such as temperature, humidity, CO2 levels, noise levels, air quality, and more. Our models trained to detect anomalies in environmental data, enabing employers to take proactive steps to ensure the safety of their employees. </p>"},{"location":"scenarios/environment-monitoring/#vision-ai-based-monitoring-using-sensors","title":"Vision AI based monitoring using Sensors","text":"<p>Vision AI based monitors can be used to analyze weather patterns and predict future environmental conditions, allowing employers to plan ahead and make changes to their workplace environment accordingly. Additionally, our models can be used to develop automated systems for monitoring and controlling the environment, allowing employers to maintain a safe and healthy work environment with minimal effort.</p> <p>Several sensors can be used to monitor different parameters, such as:</p> <ul> <li>Temperature sensors: Used to measure the temperature of a room or equipment.</li> <li>Humidity sensors: Used to measure the amount of moisture in the air.</li> <li>CO2 sensors: Used to detect the concentration of carbon dioxide in the air.</li> </ul> <p>The parameters that are monitored depend on the needs of the user. For example, a homeowner may want to monitor the temperature and humidity levels in their home, while a business may want to monitor the air quality in their workspace to ensure the health and productivity of their employees.</p> <p>Thresholds can be set up for each parameter, which can trigger an alert if the value goes above or below the specified limit. For example, if the temperature in a server room rises above a certain temperature, it can trigger an alert to prevent the equipment from overheating and causing damage.</p> <p>If the sensors go out of battery, they will stop transmitting data to the monitoring system. Therefore, it's important to have a backup power source or replace the batteries as needed.</p>"},{"location":"scenarios/environment-monitoring/#events","title":"Events","text":"<p>Event thresholds can be set up to determine how often alerts should be sent. For example, if the temperature in a room fluctuates frequently, it may be useful to set up an alert to be sent every 30 minutes. However, if the temperature remains relatively stable, an alert may only need to be sent once a day.</p> <p>In summary, the Environment Monitoring Model is a valuable tool for monitoring and managing environmental conditions. With the use of sensors and threshold settings, users can be alerted to potential issues and take corrective actions to maintain a safe and productive environment.</p>"},{"location":"scenarios/environment-monitoring/#configuration","title":"Configuration","text":"<p>Environment Monitoring Model is based on sensor data. However, if a camera is desired or necessary for a specific monitoring application, a configuration that includes a high-resolution camera with infrared capabilities would be recommended. This type of camera would be able to capture clear images in low light conditions, making it useful for monitoring spaces that may not have adequate lighting. Additionally, an IP (Internet Protocol) camera can be used to allow for remote viewing and monitoring, enabling users to access live video feeds or recorded footage from anywhere with an internet connection. The specific camera configuration will depend on the needs of the user and the environment being monitored.</p>"},{"location":"scenarios/environment-monitoring/#model-details","title":"Model Details","text":""},{"location":"scenarios/environment-monitoring/#dataset","title":"Dataset","text":"<p>Environment monitoring dataset collection is in progress.</p>"},{"location":"scenarios/environment-monitoring/#model","title":"Model","text":"<p>The model to detect environment monitoring events is in progress and it will be released soon.</p>"},{"location":"scenarios/environment-monitoring/#scenario-details","title":"Scenario details","text":"<p>TODO: Enforcement scenarios. How to configure &amp; use this scenario.</p> Test now with online Web-CamWith RTSP Camera - PipelinesWith Azure Setup <p>To test this model &amp; scenario, you can use the following steps:</p> <ul> <li> <p>Install the visionai package from PyPI</p> <pre><code>$ pip install visionai\n</code></pre> </li> <li> <p>Test the scenario from your local web-cam</p> <pre><code>$ visionai scenario test environment-monitoring\n\nDownloading models for scenario: environment-monitoring\nModel: environment-monitoring: https://workplaceos.blob.core.windows.net/models/yolov5s-environment-monitoring/yolov5s-environment-monitoring-0.0.1.zip\nStarting scenario: environment-monitoring..\n</code></pre> </li> <li> <p>You should be able to see the events generated on your console window with environment monitoring events within the camera field of view.</p> </li> </ul> <p>[TODO]</p> <p>VisionAI app is available at a Azure Market place, one can download and use it by following steps mentioned here</p>"},{"location":"scenarios/environment-monitoring/#features","title":"Features","text":"<ul> <li> <p>Improved safety: By continuously monitoring environmental conditions, potential hazards can be identified and addressed before they become a safety concern.</p> </li> <li> <p>Increased productivity: Monitoring the environment in a workplace can help identify factors that may impact productivity, such as poor air quality or uncomfortable temperatures.</p> </li> <li> <p>Remote monitoring: With an environment monitoring model, users can remotely access data and receive alerts, allowing for quick response times and reducing the need for on-site inspections.</p> </li> <li> <p>Compliance: Certain industries and applications may have environmental regulations that require monitoring and reporting. An environment monitoring model can help ensure compliance with these regulations.</p> </li> <li> <p>Customization: Users can select specific sensors and configure thresholds to meet their unique monitoring needs, making the system customizable and flexible.</p> </li> </ul> <p>Overall, an environment monitoring model can help ensure the safety, comfort, and efficiency of monitored spaces, while also reducing costs and environmental impact.</p>"},{"location":"scenarios/environment-monitoring/#training-with-custom-data","title":"Training with custom data","text":"<p>The scenario is provided as part of our GPL-v3 package for VisionAI. If you wish to train this with custom datasets, please contact us and we can provide you with the training code. You can do custom training with your own datasets for free, as long as it complies with GPLv3 license (you give back the code to the community). If you are interested in a custom license, please contact us</p>"},{"location":"scenarios/environment-monitoring/#contact-us","title":"Contact Us","text":"<ul> <li>For technical issues, you can open a Github issue here.</li> <li>For business inquiries, you can contact us through our website.</li> </ul>"},{"location":"scenarios/environment-pressure/","title":"Pressure Monitoring","text":"<p>Measure pressure in real-time, ensuring safe equipment and work conditions.</p>"},{"location":"scenarios/environment-pressure/#overview","title":"Overview","text":"<p>Pressure sensors are essential for smooth equipment functioning and safe work conditions within the facility. Pressure sensors are used for various purposes, from automotive to medical, industrial to consumer, and building devices. A pressure sensor simply monitors the pressure of liquids, air, and gas and displays it in several units, such as Pascal, Bar, and PSI.</p> <p>Various types of pressure sensors are used across industries to measure three common pressure types, i.e., Gauge Pressure, Absolute Pressure, and Differential Pressure. Submersible pressure sensors, for instance, are placed at the bottom of tanks for alert signals when the tank level falls below or rises above a safe limit. </p> <p>There are several other uses of pressure sensors in industries. However, the core mechanism remains the same, which we will discuss in the next section.</p>"},{"location":"scenarios/environment-pressure/#vision-ai-based-monitoring","title":"Vision AI based monitoring","text":"<p>Visionify\u2019s pressure sensors empower industry players to maintain and monitor the pressure in real time. Our intelligent sensors can measure the air, gas, or liquid pressure and alert you for preventive care of your infrastructure. </p> <p>When the surrounding environment\u2019s pressure exceeds or falls a threshold value, an alarm signal gets triggered for your proactive action. You can use a suite of sensors (temperature, humidity, gas sensors) for more effective environmental monitoring.</p>"},{"location":"scenarios/environment-pressure/#model-details","title":"Model Details","text":""},{"location":"scenarios/environment-pressure/#dataset","title":"Dataset","text":"<ul> <li> <p>Measuring safe pressure levels: The dataset includes safe pressure, voltage, and current output values. It needs to be adequately calibrated.</p> </li> <li> <p>Pressure difference: The dataset includes values of pressure difference (i.e., a difference between upstream pressure and downstream pressure). This helps in keeping the pressure difference minimum.</p> </li> <li> <p>Temperature and humidity: Temperature and pressure data are also helpful in pressure monitoring. Integrate your monitoring devices via IoT for a connected experience and better decision-making.</p> </li> <li> <p>Indoor vs. Outdoor environments: The dataset includes images and videos of indoor and outdoor environments. The temperature sensors in IR cameras can differentiate between the two settings and present relevant data and best actions.</p> </li> <li> <p>Different distances from the camera: The dataset contains images and videos captured from different distances from the pressure sensor. This will help in better pressure monitoring from varied distances.</p> </li> <li> <p>Multiple camera angles and resolutions The dataset includes images taken from different angles, such as top-down, side view, or angled view, and available in varied resolutions.</p> </li> <li> <p>Using security camera feeds: The dataset includes images and videos collected from your existing security camera installations to make monitoring more accurate.</p> </li> </ul>"},{"location":"scenarios/environment-pressure/#model","title":"Model","text":"<p>The model to monitor pressure in the surrounding environment is in progress and it will be released soon.</p>"},{"location":"scenarios/environment-pressure/#scenario-details","title":"Scenario details","text":"<ul> <li>When the pressure crosses a high/low threshold value, the sensors can ring an alarm for immediate inspection.</li> <li>Share notifications with detailed insight on pressure data.</li> <li>Integrate existing temperature, humidity, and gas sensors for centralized data intelligence.</li> </ul> Test now with online Web-CamWith RTSP Camera - PipelinesWith Azure Setup <p>To test this model &amp; scenario, you can use the following steps:</p> <ul> <li> <p>Install the visionai package from PyPI</p> <pre><code>$ pip install visionai\n</code></pre> </li> <li> <p>Test the scenario from your local web-cam</p> <pre><code>$ visionai scenario test pressure-monitoring\n\nDownloading models for scenario: pressure-monitoring\nModel: pressure-monitoring: https://workplaceos.blob.core.windows.net/models/yolov5s-pressure-monitoring/yolov5s-pressure-monitoring-0.0.1.zip\nStarting scenario: pressure-monitoring..\n</code></pre> </li> <li> <p>You should be able to see the events generated on your console window when pressure crosses a high/low threshold value.</p> </li> </ul> <p>[TODO]</p> <p>VisionAI app is available at a Azure Market place, one can download and use it by following steps mentioned here</p>"},{"location":"scenarios/environment-pressure/#features","title":"Features","text":"<ul> <li> <p>Alerting system: The model is able to send alerts and notifications to relevant stakeholders if environmental parameters exceed predefined thresholds or if there is a sudden change in the environment.</p> </li> <li> <p>Customization: The model is customizable and can be trained with custom datasets to suit your specific needs.</p> </li> <li> <p>Scalability and Integration: The model is scalable and easily integrated with other systems, allowing for seamless data exchange and collaboration across multiple departments or organizations.</p> </li> </ul>"},{"location":"scenarios/environment-pressure/#training-with-custom-data","title":"Training with custom data","text":"<p>The scenario is provided as part of our GPL-v3 package for VisionAI. If you wish to train this with custom datasets, please contact us and we can provide you with the training code. You can do custom training with your own datasets for free, as long as it complies with GPLv3 license (you give back the code to the community). If you are interested in a custom license, please contact us.</p>"},{"location":"scenarios/environment-pressure/#contact-us","title":"Contact Us","text":"<ul> <li>For technical issues, you can open a Github issue here.</li> <li>For business inquiries, you can contact us through our website.</li> </ul>"},{"location":"scenarios/environment-radiation/","title":"Radiation Monitoring","text":"<p>Keeping Radiation Under Control: Innovations in Radiation Monitoring and Safety Detection with Vision AI.</p>"},{"location":"scenarios/environment-radiation/#overview","title":"Overview","text":"<p>Radiation monitoring refers to the process of measuring and analyzing levels of ionizing radiation in a particular area, in order to ensure safety and compliance with regulatory standards. Ionizing radiation is a type of energy that can cause damage to living cells and genetic material, and can potentially cause cancer and other health problems if exposure levels are too high.</p> <p>Radiation monitoring is essential in a variety of settings, including nuclear power plants, medical facilities, industrial sites, and areas affected by natural disasters or other environmental hazards. Monitoring radiation levels can help identify potential sources of radiation exposure, track changes in radiation levels over time, and inform decisions about safety procedures and protective measures.</p> <p>There are several different types of radiation monitoring devices and techniques, including:</p> <ol> <li> <p>Geiger counters and other handheld radiation detectors, which measure levels of radiation in a specific area.</p> </li> <li> <p>Personal dosimeters, which are worn by workers in radiation-exposed environments to monitor their exposure levels over time.</p> </li> <li> <p>Environmental radiation monitors, which are used to monitor radiation levels in the surrounding environment and track changes over time.</p> </li> <li> <p>Remote sensing technologies, such as satellite-based sensors and unmanned aerial vehicles (UAVs), which can provide real-time data on radiation levels in hard-to-reach areas.</p> </li> </ol> <p>Advanced technologies, such as artificial intelligence (AI), machine learning, and big data analytics, are increasingly being used to support radiation monitoring efforts. These technologies can help process and analyze large amounts of radiation data, identify potential sources of radiation exposure, and inform decisions about safety procedures and protective measures.</p> <p>Overall, radiation monitoring is essential for ensuring safety and compliance with regulatory standards in a variety of settings, and can play a critical role in protecting public health and the environment.</p>"},{"location":"scenarios/environment-radiation/#vision-ai-based-monitoring","title":"Vision AI based monitoring","text":"<p>Vision AI based monitors can be used for the detection of radiation monitoring events by providing real-time video feeds of the factory area. The cameras scan every frame and raise an event whenever radiation level is detected to be above configured limit.</p>"},{"location":"scenarios/environment-radiation/#model-details","title":"Model Details","text":""},{"location":"scenarios/environment-radiation/#dataset","title":"Dataset","text":"<p>The dataset for this scenario is based on real-world radiation monitoring detection events. The dataset consists of images and videos collected from various sources. </p>"},{"location":"scenarios/environment-radiation/#model","title":"Model","text":"<p>The model to detect radiation monitoring is in progress and it will be released soon.</p>"},{"location":"scenarios/environment-radiation/#scenario-details","title":"Scenario details","text":"<p>The business logic for this scenario is as follows:</p> <ul> <li>We use existing camera feeds from the premises for raising radiation monitoring detection events.</li> <li>VisionAI system is able to run on edge devices. It uses camera feeds for processing.</li> <li>We use geiger counter sensors and analyse the camera feed and monitor the radiation level.</li> <li>If radiation level is detected to be above configured limit, an alert is raised.</li> </ul> Test now with online Web-CamWith RTSP Camera - PipelinesWith Azure Setup <p>To test this model &amp; scenario, you can use the following steps:</p> <ul> <li> <p>Install the visionai package from PyPI</p> <pre><code>$ pip install visionai\n</code></pre> </li> <li> <p>Test the scenario from your local web-cam</p> <pre><code>$ visionai scenario test radiation-monitoring-detection\n\nDownloading models for scenario: radiation-monitoring-detection\nStarting scenario: radiation-monitoring-detection..\n</code></pre> </li> <li> <p>You should be able to see the events generated on your console window with the detections of radiation monitoring within the camera field of view.</p> </li> </ul> <p>[TODO]</p> <p>VisionAI app is available at a Azure Market place, one can download and use it by following steps mentioned here</p>"},{"location":"scenarios/environment-radiation/#features","title":"Features","text":"<p>The VisionAI solution is the most efficient way of implementing this scenario, as evidenced by the following features:</p> <ul> <li> <p>Unmatched accuracy: Trained and Tested to give the best results. Our systems are trained to detect radiation monitoring events with an accuracy of 99%</p> </li> <li> <p>Lightning Fast and Response Time: Our Ultra-fast Processing provides real-time inference results and feedback (~30 frames per second processing). </p> </li> <li> <p>Minimizing false-positives/negatives: Our systems create a fail-proof system by ensuring there are no false-positives or false-negatives. </p> </li> <li> <p>Scalability and Deployment: Our pre-trained/custom models can be deployed instantly and are camera independent which means they can be pre-installed with existing cameras on site. We also offer cameras, IoT sensors and edge devices with strategic placement that helps scale a large workplace area with minimum installations. </p> </li> <li> <p>Custom Integrations: Our detection system can be integrated with other safety systems, such as building management systems or alarm systems, allowing for a coordinated response to emergencies.</p> </li> </ul>"},{"location":"scenarios/environment-radiation/#training-with-custom-data","title":"Training with custom data","text":"<p>The scenario is provided as part of our GPL-v3 package for VisionAI. If you wish to train this with custom datasets, please contact us and we can provide you with the training code. You can do custom training with your own datasets for free, as long as it complies with GPLv3 license (you give back the code to the community). If you are interested in a custom license, please contact us.</p>"},{"location":"scenarios/environment-radiation/#contact-us","title":"Contact Us","text":"<ul> <li>For technical issues, you can open a Github issue here.</li> <li>For business inquiries, you can contact us through our website.</li> </ul>"},{"location":"scenarios/environment-waste-management/","title":"Waste Management Detection","text":"<p>Innovative Technologies for Efficient Waste Management Detection with Vision AI.</p> <p> </p> Waste Management Detection event"},{"location":"scenarios/environment-waste-management/#overview","title":"Overview","text":"<p>Waste management is the process of collecting, transporting, processing, and disposing of waste materials in an environmentally responsible manner. Effective waste management is essential for protecting the environment and public health, and it is becoming an increasingly important issue in the context of global sustainability.</p> <p>Waste management includes a range of activities, from waste reduction and recycling to the safe disposal of hazardous materials. One of the key challenges in waste management is the need to balance environmental and economic concerns, as well as the health and safety of workers and the general public.</p>"},{"location":"scenarios/environment-waste-management/#vision-ai-based-monitoring","title":"Vision AI based monitoring","text":"<p>Vision AI based monitors can be used to for the detection of waste management events by providing real-time video feeds of the factory area. The cameras scan every frame and help identify opportunities for waste reduction, recycling, and cost savings.</p>"},{"location":"scenarios/environment-waste-management/#events","title":"Events","text":"<p>VisionAI model's generated events would be:</p> <ul> <li>Waste level exceeds limit</li> </ul> <p>It is recommended that any instance of waste level's exceeding should be reported. This is because it is a sign of inefficiency and can be used to optimize processes and reduce waste costs.</p> <p>An event data for this may include the following information: - Date and time of waste collection: This is important for tracking the frequency of waste collection and ensuring that the waste is collected on time.</p> <ul> <li> <p>Type of waste: This refers to the category of waste, such as organic waste, paper waste, plastic waste, hazardous waste, etc. It helps in sorting and recycling waste efficiently.</p> </li> <li> <p>Quantity of waste collected: This information helps in tracking the amount of waste generated and collected, which can help in planning waste management strategies.</p> </li> <li> <p>Location where waste is exceeding level: This refers to the address or area where the waste was collected, which helps in identifying areas with high or low waste generation rates.</p> </li> <li> <p>Identification of the waste collector: This refers to the name or ID number of the person or company responsible for collecting the waste, which helps in tracking the accountability and reliability of waste collection services.</p> </li> </ul> <p>Note</p> <p>By collecting and analyzing this event data, waste management organizations can develop effective waste management strategies, reduce waste generation, and promote recycling and sustainable waste management practices.</p>"},{"location":"scenarios/environment-waste-management/#configuration","title":"Configuration","text":"<p>To set up a camera system for waste monitoring,You will need a camera capable of capturing footage of waste collection at the factory. The camera should be able to capture footage of the factory area and the waste collection process. </p>"},{"location":"scenarios/environment-waste-management/#model-details","title":"Model Details","text":""},{"location":"scenarios/environment-waste-management/#dataset","title":"Dataset","text":"<p>The dataset for this scenario is based on real-world waste management detection events. The dataset consists of images and videos collected from various sources. </p>"},{"location":"scenarios/environment-waste-management/#model","title":"Model","text":"<p>The model to detect waste management is in progress and it will be released soon.</p>"},{"location":"scenarios/environment-waste-management/#scenario-details","title":"Scenario details","text":"<p>The business logic for this scenario is as follows:</p> <ul> <li>We use existing camera feeds from the premises for raising waste management events.</li> <li>VisionAI system is able to run on edge devices. It uses camera feeds for processing.</li> <li>From the camera feed we identify opportunities for waste reduction, recycling, and cost savings.</li> <li>If waste management event is detected, an alert is raised.</li> </ul> Test now with online Web-CamWith RTSP Camera - PipelinesWith Azure Setup <p>To test this model &amp; scenario, you can use the following steps:</p> <ul> <li> <p>Install the visionai package from PyPI</p> <pre><code>$ pip install visionai\n</code></pre> </li> <li> <p>Test the scenario from your local web-cam</p> <pre><code>$ visionai scenario test waste-management-detection\n\nDownloading models for scenario: waste-management-detection\nStarting scenario: waste-management-detection..\n</code></pre> </li> <li> <p>You should be able to see the events generated on your console window with the detections of waste management within the camera field of view.</p> </li> </ul> <p>[TODO]</p> <p>VisionAI app is available at a Azure Market place, one can download and use it by following steps mentioned here</p>"},{"location":"scenarios/environment-waste-management/#features","title":"Features","text":"<p>The VisionAI solution is the most efficient way of implementing this scenario, as evidenced by the following features:</p> <ul> <li> <p>Accurate measurement: VisionAI's system is designed to provide highly accurate measurements of waste production, disposal, and recycling. </p> </li> <li> <p>Real-time monitoring: VisionAI's systems can provide real-time monitoring of waste production, disposal, and recycling. </p> </li> <li> <p>Automated insights: Our solution can automatically provide insights and recommendations for waste reduction and recycling based on the data analysis. </p> </li> <li> <p>Integration with other systems: VisionAI solution can be integrated with other systems such as smart buildings, waste management systems, and recycling systems. </p> </li> <li> <p>User-friendly interface: Our user-friendly interfaces that allow users to easily access and analyze the data. </p> </li> </ul>"},{"location":"scenarios/environment-waste-management/#training-with-custom-data","title":"Training with custom data","text":"<p>The scenario is provided as part of our GPL-v3  package for VisionAI. If you wish to train this with custom datasets, please contact us and we can provide you with the training code. You can do custom training with your own datasets for free, as long as it complies with GPLv3 license (you give back the code to the community). If you are interested in a custom license, please contact us.</p>"},{"location":"scenarios/environment-waste-management/#contact-us","title":"Contact Us","text":"<ul> <li>For technical issues, you can open a Github issue here.</li> <li>For business inquiries, you can contact us through our website.</li> </ul>"},{"location":"scenarios/environment-water-level/","title":"Water Level Monitoring","text":"<p>Advanced Technologies for Effective Water Management and Conservation with Vision AI.</p> <p> </p> Water Level Detection event"},{"location":"scenarios/environment-water-level/#overview","title":"Overview","text":"<p>Water level monitoring is the process of measuring and tracking the water levels in rivers, lakes, reservoirs, and other bodies of water. It is an important aspect of water resource management, as it provides information about water availability, flood risk, and water quality.</p>"},{"location":"scenarios/environment-water-level/#vision-ai-based-monitoring","title":"Vision AI based monitoring","text":"<p>Vision AI based monitors can be used to for the detection of water level management events by providing real-time video feeds of the factory area. The cameras scan every frame and help identify water leaks, reduce water waste, and comply with environmental regulations.</p>"},{"location":"scenarios/environment-water-level/#events","title":"Events","text":"<p>VisionAI model's generated events would be:</p> <ul> <li>Water level exceeds limit</li> </ul> <p>It is recommended that any instance of water level's exceeding should be reported. This is because it is a sign of inefficiency and can be used to optimize processes and reduce water costs. </p> <p>An event data for this may include the following information:</p> <ul> <li> <p>Date and time of the event: This is important for tracking the duration and frequency of the event, which can help in identifying patterns and potential causes.</p> </li> <li> <p>Location of the event: This refers to the area or facility where the water levels exceeded the normal range. This information helps in identifying areas that are prone to flooding or other water-related problems.</p> </li> <li> <p>Type of water source: This refers to the type of water body or source, such as a river, lake, reservoir, or groundwater. </p> </li> <li> <p>Level of water exceeding normal range: This refers to the degree to which the water level exceeded the normal range, which can help in assessing the severity of the event and the potential damage caused.</p> </li> <li> <p>Water level: This refers to the actual water level at the time of the event. This information can be used to determine the cause of the event and the extent of the damage caused.</p> </li> </ul> <p>Note</p> <p>By collecting and analyzing this event data, water management organizations can develop effective waste management strategies, reduce waste generation, and promote recycling and sustainable waste management practices.</p>"},{"location":"scenarios/environment-water-level/#configuration","title":"Configuration","text":"<p>To set up a camera system for energy usage monitoring,You will need a camera capable of capturing footage of your energy meter or other relevant devices that you want to monitor. </p>"},{"location":"scenarios/environment-water-level/#model-details","title":"Model Details","text":""},{"location":"scenarios/environment-water-level/#dataset","title":"Dataset","text":"<p>The dataset for this scenario is based on real-world water management detection events. The dataset consists of images and videos collected from various sources. </p>"},{"location":"scenarios/environment-water-level/#model","title":"Model","text":"<p>The model to detect water management event is in progress and it will be released soon.</p>"},{"location":"scenarios/environment-water-level/#scenario-details","title":"Scenario details","text":"<p>The business logic for this scenario is as follows:</p> <ul> <li>We use existing camera feeds from the premises for raising water management events.</li> <li>VisionAI system is able to run on edge devices. It uses camera feeds for processing.</li> <li>From the camera feed we identify opportunities for water leaks, reduce water waste, and comply with environmental regulations.</li> <li>If water management event is detected, an alert is raised.</li> </ul> Test now with online Web-CamWith RTSP Camera - PipelinesWith Azure Setup <p>To test this model &amp; scenario, you can use the following steps:</p> <ul> <li> <p>Install the visionai package from PyPI</p> <pre><code>$ pip install visionai\n</code></pre> </li> <li> <p>Test the scenario from your local web-cam</p> <pre><code>$ visionai scenario test water-level-detection\n\nDownloading models for scenario: water-level-detection\nStarting scenario: water-level-detection..\n</code></pre> </li> <li> <p>You should be able to see the events generated on your console window with the detections of water management within the camera field of view.</p> </li> </ul> <p>[TODO]</p> <p>VisionAI app is available at a Azure Market place, one can download and use it by following steps mentioned here</p>"},{"location":"scenarios/environment-water-level/#features","title":"Features","text":"<p>The VisionAI solution is the most efficient way of implementing this scenario, as evidenced by the following features:</p> <ul> <li> <p>Accurate measurement: VisionAI's Water level monitoring solution is designed to provide accurate measurements of water levels. This helps to provide reliable data for various applications.</p> </li> <li> <p>Real-time monitoring: VisionAI can provide real-time monitoring of water levels. </p> </li> <li> <p>Data logging: Our solution helps to track water level changes over time and analyze the data.</p> </li> <li> <p>Remote monitoring: Our solution helps to access data from anywhere, allowing for remote monitoring and management.</p> </li> <li> <p>Integration with other systems: Our solution can be integrated with other systems such as weather stations, irrigation systems, and flood warning systems. This helps to improve the overall efficiency and effectiveness of the systems.</p> </li> </ul>"},{"location":"scenarios/environment-water-level/#training-with-custom-data","title":"Training with custom data","text":"<p>The scenario is provided as part of our GPL-v3 package for VisionAI. If you wish to train this with custom datasets, please contact us and we can provide you with the training code. You can do custom training with your own datasets for free, as long as it complies with GPLv3 license (you give back the code to the community). If you are interested in a custom license, please contact us.</p>"},{"location":"scenarios/environment-water-level/#contact-us","title":"Contact Us","text":"<ul> <li>For technical issues, you can open a Github issue here.</li> <li>For business inquiries, you can contact us through our website.</li> </ul>"},{"location":"scenarios/environment-water-quality/","title":"Water Quality Monitoring","text":"<p>Advanced Technologies for Effective Water Management and Conservation with Vision AI.</p> <p> </p> Water Quality Detection event"},{"location":"scenarios/environment-water-quality/#overview","title":"Overview","text":"<p>Water quality monitoring is the process of testing and analyzing water samples to determine if the water meets certain quality standards or if it contains harmful substances that could pose a risk to human health or the environment. Water quality monitoring is important for ensuring that water is safe to drink, swim in, or use for other purposes.</p> <p>There are many different factors that can affect water quality, including the presence of bacteria, viruses, heavy metals, pesticides, and other pollutants. Water quality monitoring typically involves collecting water samples from various locations, such as rivers, lakes, and groundwater sources, and testing them for various contaminants.</p> <p>Overall, water quality monitoring is a critical part of ensuring that our water resources remain safe and healthy for both people and the environment.</p>"},{"location":"scenarios/environment-water-quality/#vision-ai-based-monitoring","title":"Vision AI based monitoring","text":"<p>Water quality monitoring by VisionAI's camera based approach involves using cameras to capture images or videos of water bodies and then analyzing them to determine the water quality. This approach can provide a cost-effective and efficient way to monitor water quality over a large area, as cameras can cover a wide range of locations and can operate continuously.</p> <p>There are several ways in which cameras can be used for water quality monitoring. For example, cameras can be used to capture images of water bodies to detect changes in color or turbidity, which can indicate the presence of pollutants. They can also be used to monitor the growth of algae or other organisms, which can affect the water quality.</p> <p>In addition, cameras can be equipped with sensors to measure various water quality parameters such as pH, temperature, dissolved oxygen, and conductivity. These sensors can provide real-time data on water quality, which can be used to identify trends and to alert authorities to potential problems.</p>"},{"location":"scenarios/environment-water-quality/#events","title":"Events","text":"<p>VisionAI model's generated events would be:</p> <ul> <li>Water quality level exceeds limit</li> </ul> <p>It is recommended that any instance of water quality level's exceeding should be reported.  </p> <p>Event data for water quality management refers to information collected during specific events or incidents that can impact the water quality of a particular area. This data can help to identify potential sources of pollution and develop strategies to mitigate their effects.</p>"},{"location":"scenarios/environment-water-quality/#configuration","title":"Configuration","text":"<p>When setting up a camera system for water quality monitoring using VisionAI solution, there are several factors to consider to ensure that the system is effective and provides accurate data. Here are some important considerations:</p> <ul> <li> <p>Camera placement: Choose locations that are representative of the water body being monitored, such as areas with known sources of pollution or areas where water quality is known to vary. </p> </li> <li> <p>Camera type: Select a camera that is suitable for the monitoring objectives, such as a color camera for visual inspection or an infrared camera for monitoring temperature. </p> </li> </ul> <p>By considering these factors, one can set up an effective camera system for monitoring water quality and identifying potential sources of pollution. </p>"},{"location":"scenarios/environment-water-quality/#model-details","title":"Model Details","text":""},{"location":"scenarios/environment-water-quality/#dataset","title":"Dataset","text":"<p>The dataset for this scenario is based on real-world water quality management detection events. The dataset consists of images and videos collected from various sources. </p>"},{"location":"scenarios/environment-water-quality/#model","title":"Model","text":"<p>The model to detect water quality management event is in progress and it will be released soon.</p>"},{"location":"scenarios/environment-water-quality/#scenario-details","title":"Scenario details","text":"<p>The business logic for this scenario is as follows:</p> <ul> <li>We use existing camera feeds from the premises for raising water management events.</li> <li>VisionAI system is able to run on edge devices. It uses camera feeds for processing.</li> <li>From the camera feed we identify opportunities for water leaks, reduce water waste, and comply with environmental regulations.</li> <li>If water quality below threshold level event is detected, an alert is raised.</li> </ul> Test now with online Web-CamWith RTSP Camera - PipelinesWith Azure Setup <p>To test this model &amp; scenario, you can use the following steps:</p> <ul> <li> <p>Install the visionai package from PyPI</p> <pre><code>$ pip install visionai\n</code></pre> </li> <li> <p>Test the scenario from your local web-cam</p> <pre><code>$ visionai scenario test water-quality-detection\n\nDownloading models for scenario: water-quality-detection\nStarting scenario: water-quality-detection..\n</code></pre> </li> <li> <p>You should be able to see the events generated on your console window with the detections of water management within the camera field of view.</p> </li> </ul> <p>[TODO]</p> <p>VisionAI app is available at a Azure Market place, one can download and use it by following steps mentioned here</p>"},{"location":"scenarios/environment-water-quality/#features","title":"Features","text":"<p>The VisionAI solution is the most efficient way of implementing this scenario, as evidenced by the following features:</p> <ul> <li> <p>Water quality monitoring by camera can provide a valuable tool for managing and protecting water resources. However, it is important to ensure that the cameras are installed and operated in a way that does not harm the environment or interfere with wildlife.</p> </li> <li> <p>Cameras can be used to monitor water quality over a wide area, which can be useful for identifying areas that are prone to flooding or other water-related problems.</p> </li> <li> <p>Cameras can be equipped with sensors to measure various water quality parameters, which can provide real-time data on water quality. This data can be used to identify trends and to alert authorities to potential problems.</p> </li> </ul>"},{"location":"scenarios/environment-water-quality/#training-with-custom-data","title":"Training with custom data","text":"<p>The scenario is provided as part of our GPL-v3 package for VisionAI. If you wish to train this with custom datasets, please contact us and we can provide you with the training code. You can do custom training with your own datasets for free, as long as it complies with GPLv3 license (you give back the code to the community). If you are interested in a custom license, please contact us.</p>"},{"location":"scenarios/environment-water-quality/#contact-us","title":"Contact Us","text":"<ul> <li>For technical issues, you can open a Github issue here.</li> <li>For business inquiries, you can contact us through our website.</li> </ul>"},{"location":"scenarios/environment-water-usage/","title":"Water Usage Monitoring","text":"<p>Advanced Technologies for Effective Water Management and Conservation with Vision AI.</p> <p> </p> Water Usage Detection event"},{"location":"scenarios/environment-water-usage/#overview","title":"Overview","text":"<p>Water management involves the planning, development, distribution, and conservation of water resources for various purposes, including drinking, irrigation, industrial processes, and environmental conservation. Effective water management is essential for ensuring the availability of clean water, protecting natural ecosystems, and promoting sustainable economic development.</p> <p>Water management can be a complex and multifaceted process, involving a range of technologies, policies, and stakeholders. </p> <p>Advanced technologies, such as remote sensing, geographic information systems (GIS), and data analytics, are increasingly being used to support water management efforts. These technologies can provide valuable insights into water availability, usage, and quality, helping to inform decision-making and improve the efficiency and effectiveness of water management practices.</p>"},{"location":"scenarios/environment-water-usage/#vision-ai-based-monitoring","title":"Vision AI based monitoring","text":"<p>Vision AI based monitors can be used to for the detection of water management events by providing real-time video feeds of the factory area. The cameras scan every frame and help identify water leaks, reduce water waste, and comply with environmental regulations.</p>"},{"location":"scenarios/environment-water-usage/#events","title":"Events","text":"<p>VisionAI model's generated events would be:</p> <ul> <li>Water level exceeds limit</li> </ul> <p>It is recommended that any instance of water level's exceeding should be reported. This is because it is a sign of inefficiency and can be used to optimize processes and reduce water costs. </p> <p>An event data for this may include the following information:</p> <ul> <li> <p>Date and time of the event: This is important for tracking the duration and frequency of the event, which can help in identifying patterns and potential causes.</p> </li> <li> <p>Location of the event: This refers to the area or facility where the water levels exceeded the normal range. This information helps in identifying areas that are prone to flooding or other water-related problems.</p> </li> <li> <p>Type of water source: This refers to the type of water body or source, such as a river, lake, reservoir, or groundwater. </p> </li> <li> <p>Level of water exceeding normal range: This refers to the degree to which the water level exceeded the normal range, which can help in assessing the severity of the event and the potential damage caused.</p> </li> <li> <p>Water level: This refers to the actual water level at the time of the event. This information can be used to determine the cause of the event and the extent of the damage caused.</p> </li> </ul> <p>Note</p> <p>By collecting and analyzing this event data, water management organizations can develop effective waste management strategies, reduce waste generation, and promote recycling and sustainable waste management practices.</p>"},{"location":"scenarios/environment-water-usage/#configuration","title":"Configuration","text":"<p>To set up a camera system for water usage monitoring,You will need a camera capable of capturing footage of water bodies you want to monitor. </p>"},{"location":"scenarios/environment-water-usage/#model-details","title":"Model Details","text":""},{"location":"scenarios/environment-water-usage/#dataset","title":"Dataset","text":"<p>The dataset for this scenario is based on real-world water management detection events. The dataset consists of images and videos collected from various sources. </p>"},{"location":"scenarios/environment-water-usage/#model","title":"Model","text":"<p>The model to detect water management event is in progress and it will be released soon.</p>"},{"location":"scenarios/environment-water-usage/#scenario-details","title":"Scenario details","text":"<p>The business logic for this scenario is as follows:</p> <ul> <li>We use existing camera feeds from the premises for raising water management events.</li> <li>VisionAI system is able to run on edge devices. It uses camera feeds for processing.</li> <li>From the camera feed we identify opportunities for water leaks, reduce water waste, and comply with environmental regulations.</li> <li>If water management event is detected, an alert is raised.</li> </ul> Test now with online Web-CamWith RTSP Camera - PipelinesWith Azure Setup <p>To test this model &amp; scenario, you can use the following steps:</p> <ul> <li> <p>Install the visionai package from PyPI</p> <pre><code>$ pip install visionai\n</code></pre> </li> <li> <p>Test the scenario from your local web-cam</p> <pre><code>$ visionai scenario test water-management-detection\n\nDownloading models for scenario: water-management-detection\nStarting scenario: water-management-detection..\n</code></pre> </li> <li> <p>You should be able to see the events generated on your console window with the detections of water management within the camera field of view.</p> </li> </ul> <p>[TODO]</p> <p>VisionAI app is available at a Azure Market place, one can download and use it by following steps mentioned here</p>"},{"location":"scenarios/environment-water-usage/#features","title":"Features","text":"<p>The VisionAI solution is the most efficient way of implementing this scenario, as evidenced by the following features:</p> <ul> <li> <p>Accurate measurement: VisionAI solution is designed to provide accurate measurements of water consumption. </p> </li> <li> <p>Real-time monitoring: Our solution can provide real-time monitoring of water usage. This helps to provide up-to-date information about the water usage patterns.</p> </li> <li> <p>Data logging: Our solution helps to track water usage patterns over time and analyze the data.</p> </li> <li> <p>Remote monitoring: Our solution can be accessed remotely. This means that the data can be accessed from anywhere, allowing for remote monitoring and management.</p> </li> <li> <p>Integration with other systems: Our solution can be integrated with other systems such as smart homes, irrigation systems, and water management systems. This helps to improve the overall efficiency and effectiveness of the systems.</p> </li> </ul>"},{"location":"scenarios/environment-water-usage/#training-with-custom-data","title":"Training with custom data","text":"<p>The scenario is provided as part of our GPL-v3 package for VisionAI. If you wish to train this with custom datasets, please contact us and we can provide you with the training code. You can do custom training with your own datasets for free, as long as it complies with GPLv3 license (you give back the code to the community). If you are interested in a custom license, please contact us.</p>"},{"location":"scenarios/environment-water-usage/#contact-us","title":"Contact Us","text":"<ul> <li>For technical issues, you can open a Github issue here.</li> <li>For business inquiries, you can contact us through our website.</li> </ul>"},{"location":"scenarios/equipment-monitoring/","title":"Equipment Monitoring","text":"<p>Equipment Monitoring deals with monitoring of equipment in a factory or warehouse to ensure that the equipment is working properly and is not being misused.                    </p> <p>VisionAI can be used to monitor the equipment and alert the user if the equipment is not working properly or is being misused.</p> <p>There are many events that could trigger an alert for events related to monitoring of equipments. Here are a few use cases:</p> <ul> <li>Equipment temperature monitoring</li> <li>Equipment rust and corrosion</li> <li>Equipment vibration monitoring</li> <li>Equipment noise monitoring</li> <li>Read analog dials</li> <li>Tools check-in/out</li> <li>Spill &amp; leak</li> </ul>"},{"location":"scenarios/equipment-spills-and-leaks/","title":"Spills &amp; Leaks detection through VisionAI","text":"<p>Spills and Leaks detection through Vision AI.</p>"},{"location":"scenarios/equipment-spills-and-leaks/#overview","title":"Overview","text":"<p>Spills and leaks in industries can have significant health impacts on both humans and wildlife. The severity of the health impact depends on the type of substance that is spilled or leaked, the duration and extent of the exposure, and the vulnerability of the exposed population. Some potential health impacts of spills and leaks are Respiratory problems, skin irritation, Neurological effects, Cancer, Reproductive problems and Environmental impact.</p> <p>Preventing and mitigating spills and leaks is crucial for protecting the environment and human health. Existing solitions could be regular inspections and maintenance of equipment. Manual inspection is not foolproof and can be prone to errors and oversights. Human inspectors may miss small leaks or spills that may go undetected until they become larger and more severe.</p>"},{"location":"scenarios/equipment-spills-and-leaks/#vision-ai-based-monitoring","title":"Vision AI based monitoring","text":"<p>Manual inspections can be time-consuming and labor-intensive, which can make them impractical for large or complex industrial facilities.</p> <p>Vision AI-based model is designed to detect spills and leaks including water puddles, water leaks and slippery surfaces. The model can analyze images and video footage to identify visual anomalies, such as the appearance of a spill or leak, which can be missed by human inspectors.</p> Oil leak Water leak in pipes"},{"location":"scenarios/equipment-spills-and-leaks/#events","title":"Events","text":"<p>VisionAI model's generated events would be:</p> <ul> <li>Water puddle detected</li> <li>Water leak from equipment detected</li> <li>Spill event detected</li> <li>Slippery sign detected</li> </ul>"},{"location":"scenarios/equipment-spills-and-leaks/#model-details","title":"Model Details","text":""},{"location":"scenarios/equipment-spills-and-leaks/#dataset","title":"Dataset","text":"<p>Dataset for spills/leakages is properly curated and validated to ensure that the models are accurate and reliable. </p> <p>Some of the sources used to take images are:</p> <ul> <li>CAMEO Chemicals dataset</li> <li>The NOAA Hazardous Material Incident database</li> <li> <p>The Oil Spill Dataset</p> </li> <li> <p>The Pipeline and Hazardous Materials Safety Administration (PHMSA) dataset</p> </li> <li> <p>The Spill Impact Mitigation Assessment (SIMA) dataset</p> </li> </ul>"},{"location":"scenarios/equipment-spills-and-leaks/#model","title":"Model","text":"<p>The model to detect leak/spill event is in progress and it will be released soon.</p>"},{"location":"scenarios/equipment-spills-and-leaks/#scenario-details","title":"Scenario details","text":"<p>The business logic for this scenario is as follows: </p> <ul> <li>We use existing camera feeds from the premises to monitor the signs of leakage, spills in the workplace to ensure the safety of human lives in the workplace. </li> <li>VisionAI system is able to run on edge devices. It uses camera feeds for processing. </li> <li>We detect any kind of leakage in the camera feed.</li> <li>An alarming system is inplace as part of solution.</li> </ul> Test now with online Web-CamWith RTSP Camera - PipelinesWith Azure Setup <p>To test this model &amp; scenario, you can use the following steps:</p> <ul> <li> <p>Install the visionai package from PyPI</p> <pre><code>$ pip install visionai\n</code></pre> </li> <li> <p>Test the scenario from your local web-cam</p> <pre><code>$ visionai scenario test no-leak-detection\n\nDownloading models for scenario: no-smoking-detection\nModel: no-leak-detection: https://workplaceos.blob.core.windows.net/models/yolov5s-people/yolov5s-people-0.0.4.zip\nStarting scenario: no-leak-detection..\n</code></pre> </li> <li> <p>You should be able to see the events generated on your console window with the detections of spills and leak within the camera field of view.</p> </li> </ul> <p>[TODO]</p> <p>VisionAI app is available at a Azure Market place, one can download and use it by following steps mentioned here</p>"},{"location":"scenarios/equipment-spills-and-leaks/#features","title":"Features","text":"<p>VisionAI's Spill and leak detection  identifies and classifies spills and leaks in real-time. Here are some features of spill and leak detection:</p> <ul> <li> <p>Real-time monitoring: AI-based spill and leak detection systems can continuously monitor facilities and pipelines in real-time, allowing for quick detection and response times.</p> </li> <li> <p>Automated detection and alerts: AI-based systems can detect spills and leaks automatically and issue alerts to relevant personnel or systems, allowing for quick response and mitigation of the issue.</p> </li> <li> <p>Increased accuracy and reliability: VisionAI models can analyze large amounts of data quickly and accurately, allowing for the identification of even small leaks or spills that may be missed by human inspectors.</p> </li> <li> <p>Integration with other systems: VisionAI solution can be integrated with other systems such as alarm systems and spill response plans, allowing for a more comprehensive and effective response to spills and leaks.</p> </li> <li> <p>Predictive analytics: VisionAI models  can analyze historical data and patterns to identify potential risks and prevent future spills and leaks.</p> </li> <li> <p>Remote monitoring:  The system allows continuous monitoring of facilities and pipelines in remote or hard-to-reach areas.</p> </li> </ul> <p>Note</p> <p>Overall, spill and leak detection using our VisionAI's solution provides a powerful tool for industries to improve the accuracy, speed, and efficiency of spill and leak detection and response. The use of AI can also help to reduce the risk of human exposure to hazardous materials and prevent environmental damage caused by spills and leaks.</p>"},{"location":"scenarios/equipment-spills-and-leaks/#training-with-custom-data","title":"Training with custom data","text":"<p>The scenario is provided as part of our GPL-v3 package for VisionAI. If you wish to train this with custom datasets, please contact us and we can provide you with the training code. You can do custom training with your own datasets for free, as long as it complies with GPLv3 license (you give back the code to the community). If you are interested in a custom license, please (contact us)[contact.md].</p>"},{"location":"scenarios/equipment-spills-and-leaks/#contact-us","title":"Contact Us","text":"<ul> <li>For technical issues, you can open a Github issue here.</li> <li>For business inquiries, you can contact us through our website.</li> </ul>"},{"location":"scenarios/equipment-temperature-hazard/","title":"Equipment temperature monitoring","text":"<p>Monitor equipment temperature. Get an alert when temperature exceeds a limit or below a threshold.</p>"},{"location":"scenarios/equipment-temperature-hazard/#overview","title":"Overview","text":"<p>Industry equipment works best under a fixed temperature range. However, these are susceptible to malfunction or failure if equipment temperature exceeds or falls a threshold value. It could halt service for more extended periods and cause fatal accidents.</p> <p>Organizations need a sensor-based temperature monitoring device to ensure smooth equipment functioning, safe exits, and quick preventive maintenance. VisionAI-powered solutions monitor equipment temperature in real-time and alert signals if temperature surpasses threshold values.</p>"},{"location":"scenarios/equipment-temperature-hazard/#ir-camera-based-monitoring-for-a-temperature-of-an-equipment","title":"IR camera based monitoring for a temperature of an equipment","text":"<p>Visionify\u2019s thermal monitoring solution integrated with an IR camera monitors equipment temperature and functioning under various conditions and surroundings. Our solution detects equipment temperature and performance by processing real-time images and videos from different sources. In addition, it can trigger an alarm (as soon as the temperature reaches the threshold margins or faulty equipment is detected) and identify areas where exits are present. </p> <p>By installing this in your facility, you can ensure the following:</p> <ul> <li>Long-term equipment health</li> <li>Quick equipment maintenance </li> <li>Safe evacuation in case of an emergency</li> </ul> <p> </p> Detection of equipment temperature event"},{"location":"scenarios/equipment-temperature-hazard/#events","title":"Events","text":"<p>VisionAI model's generated events would be:</p> <ul> <li>Temperature exceeds limit</li> <li>Temperature subceeds limit</li> </ul> <p>It is recommended that any instance of equipment temperature not maintaining its limit should be reported, especially if it is related to a fire safety system.</p> <p>An event data for a temperature not maintaining its limit may include the following information:</p> <ul> <li> <p>Date and time: The date and time when the temperature reading was taken and when the issue was first detected.</p> </li> <li> <p>Location: The specific location where the temperature reading was taken, such as a room or area within a factory.</p> </li> <li> <p>Threshold limits: The threshold limits for temperature, which can help to determine whether the temperature is outside of the acceptable range.</p> </li> </ul> <p>Having this information recorded in an event data can help to identify the cause of the temperature issue, determine the appropriate response and follow-up actions, and track the status of the issue over time.</p>"},{"location":"scenarios/equipment-temperature-hazard/#configuration","title":"Configuration","text":"<p>Setting up IR Camera sensors in critical areas such as electrical rooms, server rooms, storage areas and kitchens is especially important, as these areas are often more prone to fire hazards due to the presence of heat-generating equipment or flammable materials. </p> <p>Regularly monitoring temperature variations can help to identify potential fire hazards before they escalate into a full-blown fire, allowing for prompt corrective action to be taken to prevent damage to property and ensure the safety of occupants.</p> <p>In addition to installing temperature sensors, it's also important to ensure that the sensors are calibrated properly and that their readings are regularly checked and maintained. This can help to ensure that the temperature sensors are accurately detecting any temperature variations and triggering appropriate fire safety measures when necessary.</p>"},{"location":"scenarios/equipment-temperature-hazard/#model-details","title":"Model Details","text":"<p>The model to detect temperature exceeds or subceeds event is in progress and it will be released soon.</p>"},{"location":"scenarios/equipment-temperature-hazard/#scenario-details","title":"Scenario details","text":"<p>When equipment temperature changes rapidly, the model can detect heat or cooling loss. If equipment temperature crosses a threshold value (both high and low), an alarm can be triggered for preventive maintenance.  When the equipment is overheated, and there is a high risk of an accident, the IR camera sensors can mark safe exits. The model allows you to upload the most recent version of the baseline image onto your camera before the inspection.</p> Test now with online Web-CamWith RTSP Camera - PipelinesWith Azure Setup <p>To test this model &amp; scenario, you can use the following steps:</p> <ul> <li> <p>Install the visionai package from PyPI</p> <pre><code>$ pip install visionai\n</code></pre> </li> <li> <p>Test the scenario from your local web-cam</p> <pre><code>$ visionai scenario test temp-detection\n\nDownloading models for scenario: temp-detection\nModel: temp-detection: https://workplaceos.blob.core.windows.net/models/yolov5s-people/yolov5s-people-0.0.4.zip\nStarting scenario: temp-detection..\n</code></pre> </li> <li> <p>You should be able to see the information generated on your console window with the detections of smoking/vaping event within the camera field of view.</p> </li> </ul> <p>[TODO]</p> <p>VisionAI app is available at a Azure Market place, one can download and use it by following steps mentioned here</p>"},{"location":"scenarios/equipment-temperature-hazard/#features","title":"Features","text":"<p>VisionAI's thermal camera based solution is suitable for equipment monitoring as they can provide real-time temperature readings and help detect temperature variations that may indicate potential issues or problems with the equipment.</p>"},{"location":"scenarios/equipment-temperature-hazard/#training-with-custom-data","title":"Training with custom data","text":"<p>The scenario is provided as part of our GPL-v3 package for VisionAI. If you wish to train this with custom datasets, please contact us and we can provide you with the training code. You can do custom training with your own datasets for free, as long as it complies with GPLv3 license (you give back the code to the community). If you are interested in a custom license, please contact us.</p>"},{"location":"scenarios/equipment-temperature-hazard/#contact-us","title":"Contact Us","text":"<ul> <li>For technical issues, you can open a Github issue here.</li> <li>For business inquiries, you can contact us through our website.</li> </ul>"},{"location":"scenarios/equipment-temperature/","title":"Equipment temperature monitoring","text":"<p>Monitor equipment temperature. Get an alert when temperature exceeds a limit or below a threshold.</p>"},{"location":"scenarios/equipment-temperature/#overview","title":"Overview","text":"<p>Industry equipment works best under a fixed temperature range. However, these are susceptible to malfunction or failure if equipment temperature exceeds or falls a threshold value. It could halt service for more extended periods and cause fatal accidents.</p> <p>Organizations need a sensor-based temperature monitoring device to ensure smooth equipment functioning, safe exits, and quick preventive maintenance. VisionAI-powered solutions monitor equipment temperature in real-time and alert signals if temperature surpasses threshold values.</p>"},{"location":"scenarios/equipment-temperature/#ir-camera-based-monitoring-for-a-temperature-of-an-equipment","title":"IR camera based monitoring for a temperature of an equipment","text":"<p>Visionify\u2019s thermal monitoring solution integrated with an IR camera monitors equipment temperature and functioning under various conditions and surroundings. Our solution detects equipment temperature and performance by processing real-time images and videos from different sources. In addition, it can trigger an alarm (as soon as the temperature reaches the threshold margins or faulty equipment is detected) and identify areas where exits are present. </p> <p>By installing this in your facility, you can ensure the following:</p> <ul> <li>Long-term equipment health</li> <li>Quick equipment maintenance </li> <li>Safe evacuation in case of an emergency</li> </ul> <p> </p> Detection of equipment temperature event"},{"location":"scenarios/equipment-temperature/#events","title":"Events","text":"<p>VisionAI model's generated events would be:</p> <ul> <li>Temperature exceeds limit</li> <li>Temperature subceeds limit</li> </ul> <p>It is recommended that any instance of equipment temperature not maintaining its limit should be reported, especially if it is related to a fire safety system.</p> <p>An event data for a temperature not maintaining its limit may include the following information:</p> <ul> <li> <p>Date and time: The date and time when the temperature reading was taken and when the issue was first detected.</p> </li> <li> <p>Location: The specific location where the temperature reading was taken, such as a room or area within a factory.</p> </li> <li> <p>Threshold limits: The threshold limits for temperature, which can help to determine whether the temperature is outside of the acceptable range.</p> </li> </ul> <p>Having this information recorded in an event data can help to identify the cause of the temperature issue, determine the appropriate response and follow-up actions, and track the status of the issue over time.</p>"},{"location":"scenarios/equipment-temperature/#configuration","title":"Configuration","text":"<p>Setting up IR Camera sensors in critical areas such as electrical rooms, server rooms, storage areas and kitchens is especially important, as these areas are often more prone to fire hazards due to the presence of heat-generating equipment or flammable materials. </p> <p>Regularly monitoring temperature variations can help to identify potential fire hazards before they escalate into a full-blown fire, allowing for prompt corrective action to be taken to prevent damage to property and ensure the safety of occupants.</p> <p>In addition to installing temperature sensors, it's also important to ensure that the sensors are calibrated properly and that their readings are regularly checked and maintained. This can help to ensure that the temperature sensors are accurately detecting any temperature variations and triggering appropriate fire safety measures when necessary.</p>"},{"location":"scenarios/equipment-temperature/#model-details","title":"Model Details","text":"<p>The model to detect temperature exceeds or subceeds event is in progress and it will be released soon.</p>"},{"location":"scenarios/equipment-temperature/#scenario-details","title":"Scenario details","text":"<p>When equipment temperature changes rapidly, the model can detect heat or cooling loss. If equipment temperature crosses a threshold value (both high and low), an alarm can be triggered for preventive maintenance.  When the equipment is overheated, and there is a high risk of an accident, the IR camera sensors can mark safe exits. The model allows you to upload the most recent version of the baseline image onto your camera before the inspection.</p> Test now with online Web-CamWith RTSP Camera - PipelinesWith Azure Setup <p>To test this model &amp; scenario, you can use the following steps:</p> <ul> <li> <p>Install the visionai package from PyPI</p> <pre><code>$ pip install visionai\n</code></pre> </li> <li> <p>Test the scenario from your local web-cam</p> <pre><code>$ visionai scenario test temp-detection\n\nDownloading models for scenario: temp-detection\nModel: temp-detection: https://workplaceos.blob.core.windows.net/models/yolov5s-people/yolov5s-people-0.0.4.zip\nStarting scenario: temp-detection..\n</code></pre> </li> <li> <p>You should be able to see the information generated on your console window with the detections of smoking/vaping event within the camera field of view.</p> </li> </ul> <p>[TODO]</p> <p>VisionAI app is available at a Azure Market place, one can download and use it by following steps mentioned here</p>"},{"location":"scenarios/equipment-temperature/#features","title":"Features","text":"<p>VisionAI's thermal camera based solution is suitable for equipment monitoring as they can provide real-time temperature readings and help detect temperature variations that may indicate potential issues or problems with the equipment.</p>"},{"location":"scenarios/equipment-temperature/#training-with-custom-data","title":"Training with custom data","text":"<p>The scenario is provided as part of our GPL-v3 package for VisionAI. If you wish to train this with custom datasets, please contact us and we can provide you with the training code. You can do custom training with your own datasets for free, as long as it complies with GPLv3 license (you give back the code to the community). If you are interested in a custom license, please contact us.</p>"},{"location":"scenarios/equipment-temperature/#contact-us","title":"Contact Us","text":"<ul> <li>For technical issues, you can open a Github issue here.</li> <li>For business inquiries, you can contact us through our website.</li> </ul>"},{"location":"scenarios/ergonomics/","title":"Ergonomics Monitoring","text":"<p>Ensure safety and comfort of employees by monitoring ergonomics. </p>"},{"location":"scenarios/ergonomics/#overview","title":"Overview","text":"<p>Ergonomics is the study of designing and arranging products, systems, and environments to fit the capabilities and limitations of people, with the goal of improving efficiency, safety, and comfort. The primary focus of ergonomics is to create environments that optimize human performance and well-being.</p>"},{"location":"scenarios/ergonomics/#vision-ai-based-monitoring","title":"Vision AI based monitoring","text":"<p>Vision AI based monitors can be used to monitor productivity and workers health by providing real-time video feeds of different areas. These cameras can be used to monitor and track number of objects moved by persons from one place to the other, as well as the bending information of workers while performing this work. </p> <p>Companies can put compliance policies in place to ensure that workers are made aware of work-related injuries and illnesses\u202fdue to unnecessary bending.  </p> <p>It is important to note that these camera-based monitoring provides should be supplanted by strong compliance processes to ensure their accuracy and reliability. In addition, workers working in companies should always be trained on ergonomics and its significance for safety and its impact on human health.</p>"},{"location":"scenarios/ergonomics/#events","title":"Events","text":"<p>VisionAI model's generated events would be:</p> <ul> <li>Bend count event per individual</li> </ul>"},{"location":"scenarios/ergonomics/#configuration","title":"Configuration","text":"<p>Camera placement for detecting ergonomics can help to identify workplace design issues that may be contributing to other ergonomic-related injuries. Here are some general guidelines for camera placement in this context:</p> <ul> <li> <p>Workstation cameras: Cameras can be placed at individual workstations to monitor employee movements and postures. This can help to identify ergonomic issues such as poor posture, repetitive motions.</p> </li> <li> <p>Assembly line cameras: Cameras can also be placed along assembly lines to monitor employee movements and postures during repetitive tasks.</p> </li> <li> <p>Multiple angles: Multiple cameras may be necessary to ensure complete coverage of the area being monitored. Cameras should be placed at different angles to ensure complete coverage, with overlapping views to ensure no blind spots.</p> </li> </ul> <p>Note</p> <p>It's important to ensure that any data collected through camera monitoring is stored and used in compliance with relevant regulations and privacy considerations.</p>"},{"location":"scenarios/ergonomics/#model-details","title":"Model Details","text":""},{"location":"scenarios/ergonomics/#dataset","title":"Dataset","text":"<p>Model training is carried out with Microsoft COCO: Common Objects in Context dataset. Person and Book classes are considered for model building. Person class is considered here because the problem of ergonomics is related to pose estimation and the object used to show movement is the book. It is the object carried by a person from one location to the other. Other objects can be considered as per the requirement.  </p> <p>The dataset is made up of a large number of images and it is curated to ensure a true  representation of the real world for: </p> <ul> <li> <p>Indoor vs Outdoor environments </p> </li> <li> <p>Variations in time  </p> </li> <li> <p>Different types of clothing for persons </p> </li> <li> <p>Different distances from the camera </p> </li> <li> <p>Various lighting conditions </p> </li> <li> <p>Various camera angles, resolutions and calibrations </p> </li> <li> <p>Using security camera feeds </p> </li> </ul>"},{"location":"scenarios/ergonomics/#model","title":"Model","text":"<p>The Yolov5 pre-trained model for detecting person and book (an example of an item) classes are used to build the model. DenseNet is employed to estimate each person's landmarks. These landmarks are used to estimate poses. This is mostly used to track a person's bending motion. To guarantee the counting of boxes, object tracking using a strong sort algorithm is also built. </p> <p>This provides ergonomics data that may be utilised for a variety of tasks, such as alarm generation when the number of bends exceeds predetermined levels and productivity counting to determine how many objects were transported from one location to another. </p> <p>The DenseNet Model for Landmark detection provides the following metrics: </p> Precision Recall mAP  <p>The model is lightweight enough to be run on any edge device. </p>"},{"location":"scenarios/ergonomics/#scenario-details","title":"Scenario details","text":"<p>The business logic for this scenario is as follows: </p> <ul> <li> <p>We use existing camera feeds from the premises to monitor the presence of workers. </p> </li> <li> <p>VisionAI system is able to run on edge devices. It uses camera feeds for processing. </p> </li> <li> <p>We detect and track the number of objects transported and we monitor the total number of bending motions of a person while working.</p> </li> </ul> <p>=== \"Test now with online Web-Cam\"      To test this model &amp; scenario, you can use the following steps:</p> <pre><code> - Install the visionai package from PyPI\n\n    ```console\n    $ pip install visionai\n\n    ```\n\n - Test the scenario from your local web-cam\n\n\n    ```console\n    $ visionai scenario test ergonomics\n\n    ```\n\n    Downloading models for scenario: ergonomics\n\n\n\n    Starting scenario: ergonomics..\n\n    ```\n- You should be able to see the events generated on your console window with the detections of firearms and knives event within the camera field of view.\n</code></pre> With RTSP Camera - PipelinesWith Azure Setup <p>[TODO]</p> <p>VisionAI app is available at a Azure Market place, one can download and use it by following steps mentioned here</p>"},{"location":"scenarios/ergonomics/#training-with-custom-data","title":"Training with custom data","text":"<p>The scenario is provided as part of our GPL-v3 package for VisionAI. If you wish to train this with custom datasets, please contact us and we can provide you with the training code. You can do custom training with your own datasets for free, as long as it complies with GPLv3 license (you give back the code to the community). If you are interested in a custom license, please [contact us] (../company/contact.md).</p>"},{"location":"scenarios/ergonomics/#contact-us","title":"Contact Us","text":"<ul> <li>For technical issues, you can open a Github issue here.</li> <li>For business inquiries, you can contact us through our website.</li> </ul>"},{"location":"scenarios/exclusion-zones/","title":"Restricted areas/times","text":"<p>Secure restricted areas with our powerful AI-based monitoring system that detects and prevents unauthorized access in real-time</p>"},{"location":"scenarios/exclusion-zones/#overview","title":"Overview","text":"<p>Unauthorized access to restricted zones at the workplace can lead to theft, accidents, and other security breaches. Be it valuable assets, sensitive information or providing employee safety, maintaining high-security and controlled access for restricted zones at the workplace is essential for all organizations. However, monitoring and controlling access to these areas is often an expensive and error-prone process, requiring continuous manual surveillance by security personnel.</p> <p>Another major problem with existing systems cannot detect intrusion after an unauthorized access has been made.This renders biometric, sensors and security personnels ineffective after an unauthorized access has been already made.</p>"},{"location":"scenarios/exclusion-zones/#vision-ai-based-monitoring","title":"Vision AI based monitoring","text":"<p>With our Vision AI monitoring you can authorize access as well as continuous monitor live feeds inside a restricted area for real-time detection of unauthorized personnel. Our fully automated detection models are not only more powerful and accurate than existing systems but also more affordable and easy to integrate into existing infrastructure allowing users to scale the power of i-based real-time detection with a few simple clicks.</p> <p> </p> Detection of unauthorized entry event"},{"location":"scenarios/exclusion-zones/#events","title":"Events","text":"<p>VisionAI model's generated events would be:</p> <ul> <li>Person detected in restricted area</li> <li>Movement detected in restricted area</li> <li>Person detected after hours</li> <li>Movement detected after hours\"</li> </ul> <p>It is recommended that any instance of unauthorized entry be reported to the appropriate authority. An event data for a unauthozrized entry in exclusion zones may include information such as:</p> <ul> <li>Date and time of the event</li> <li>Location of the event</li> <li>Image of the event</li> </ul>"},{"location":"scenarios/exclusion-zones/#configuration","title":"Configuration","text":"<p>It is recommended to set up camera in ceiling view to detect unauthorized entry events. </p>"},{"location":"scenarios/exclusion-zones/#model-details","title":"Model Details","text":""},{"location":"scenarios/exclusion-zones/#dataset","title":"Dataset","text":"<p>The dataset consists of images and videos collected from diverse sources and is designed to reflect real-world scenarios. It is evenly distributed with:</p> <ul> <li> <p>Different environments: Both indoor and outdoor with varying/contrasting surrounding and infrastructure details</p> </li> <li> <p>Different angles and perspectives: The dataset includes images captured from different angles and perspectives, such as from above, below, or from the side of subjects</p> </li> <li> <p>Different modes of unauthorized access: The dataset includes images of individuals attempting to gain unauthorized access in different ways, such as climbing over fences, breaking locks, using counterfeit credentials, or attempting to sneak past security personnel.</p> </li> <li> <p>Diversity of individuals: The dataset includes images of individuals from different genders, ages, and ethnicities, to ensure that the AI model is able to accurately detect unauthorized access attempts regardless of the individual's appearance.</p> </li> </ul>"},{"location":"scenarios/exclusion-zones/#model","title":"Model","text":"<p>The model to detect unauthorized entry event is in progress and it will be released soon.</p>"},{"location":"scenarios/exclusion-zones/#scenario-details","title":"Scenario details","text":"<p>Real-time detection and alerts for different kinds unauthorized access which includes but are not limited to:</p> <ul> <li>When an unauthorized person follows an authorized person through a secure area without proper authorization</li> <li>When an individual lingering around restricted areas without proper authorization</li> <li>Forceful entry </li> <li>Use of counterfeit access credentials</li> <li>Unauthorized access attempts during off-hours</li> </ul> Test now with online Web-CamWith RTSP Camera - PipelinesWith Azure Setup <p>To test this model &amp; scenario, you can use the following steps:</p> <ul> <li> <p>Install the visionai package from PyPI</p> <pre><code>$ pip install visionai\n</code></pre> </li> <li> <p>Test the scenario from your local web-cam</p> <pre><code>$ visionai scenario test exclusion-detection\n\nDownloading models for scenario: exclusion-detection\nModel: miss-fire-exting-detection: https://workplaceos.blob.core.windows.net/models/yolov5s-people/yolov5s-people-0.0.4.zip\nStarting scenario: exclusion-detection..\n</code></pre> </li> <li> <p>You should be able to see the events generated on your console window with the detections of unauthorized access or forceful entry within the camera field of view.</p> </li> </ul> <p>[TODO]</p> <p>VisionAI app is available at a Azure Market place, one can download and use it by following steps mentioned here</p>"},{"location":"scenarios/exclusion-zones/#features","title":"Features","text":"<p>Some potential features of VisionAI for detecting missing fire extinguishers could include:</p> <ul> <li> <p>Lightning Fast and Response Time: Ultra-fast Processing for real-time inference results and feedback (~30 frames per second processing) with customizable telemetry and inference results for your requirements.</p> </li> <li> <p>Scalability and Instant Deployment: Our pre-trained/custom models can be deployed instantly and are camera independent which means they can be pre-installed with existing cameras on site. </p> </li> <li> <p>Custom Integrations: Our custom smart dashboards and real-time alert/notification systems can be tailored to fit your specific needs be it simple dashboards or complex ERP integrations.</p> </li> <li> <p>Multiple channels for notifications: Employee Role-based notifications and alerts through different omni channels like emails, messages, custom alert systems, etc.</p> </li> <li> <p>Pre-Processing and Privacy by design: Our Pre-processing enhances Image quality before further analysis  While  maintaining data privacy by blurring out faces and other sensitive information present in a frame.</p> </li> </ul>"},{"location":"scenarios/exclusion-zones/#training-with-custom-data","title":"Training with custom data","text":"<p>The scenario is provided as part of our GPL-v3 package for VisionAI. If you wish to train this with custom datasets, please contact us and we can provide you with the training code. You can do custom training with your own datasets for free, as long as it complies with GPLv3 license (you give back the code to the community). If you are interested in a custom license, please contact us.</p>"},{"location":"scenarios/exclusion-zones/#contact-us","title":"Contact Us","text":"<ul> <li>For technical issues, you can open a Github issue here.</li> <li>For business inquiries, you can contact us through our website.</li> </ul>"},{"location":"scenarios/fall-and-accident-detection/","title":"Fall &amp; Accident detection","text":"<p>Detect potential collision/accident and wet floor, inspect slip and fall instances with VisionAI.</p>"},{"location":"scenarios/fall-and-accident-detection/#overview","title":"Overview","text":"<p>Fall &amp; Accident computer vision based detection system is designed to detect potential safety hazards in a given environment. The system uses video data from cameras placed in the area to identify a range of potential hazards, including person slip &amp; fall, potential collision/accident, wet floor, debris on the floor, and wet/slippery signs.</p> <p>To detect these hazards, the system uses deep learning-based algorithms to analyze the video data and identify specific patterns and features that correspond to each type of hazard. For example, to detect a person slip &amp; fall, the system may look for sudden changes in movement, unusual body positions, or signs of distress.</p> <p>Similarly, to detect potential collisions or accidents, the system may analyze the movement of people or objects in the environment and identify situations where there is a high likelihood of a collision or other accident occurring.</p>"},{"location":"scenarios/fall-and-accident-detection/#vision-ai-based-monitoring","title":"Vision AI based monitoring","text":"<p>Vision AI-based system can be used to detect slip and fall with high accuracy. Additionally, our model trained on real-world images minimizes false-positives or false-negatives.  </p> <p>The cameras scan every frame to ensure there are no accidents related to slip and fall cases. </p> <p>To detect a wet floor, the system may look for areas where there is a significant change in reflectance or texture, which could indicate the presence of moisture.</p> <p>To detect debris on the floor, the system may analyze the texture and shape of objects in the environment and identify items that are out of place or could potentially cause a tripping hazard.</p> <p>Finally, to detect wet/slippery signs, the system may analyze the shape and color of signs in the environment and identify those that indicate a wet or slippery floor.</p> <p>Overall, the system is designed to help improve safety in a range of environments, from factories and warehouses to retail stores and public spaces. By detecting potential hazards in real-time, the system can alert workers or visitors to potential dangers and help prevent accidents and injuries.</p>"},{"location":"scenarios/fall-and-accident-detection/#model-details","title":"Model Details","text":""},{"location":"scenarios/fall-and-accident-detection/#dataset","title":"Dataset","text":"<p>The dataset for this scenario is based on real-world data from different workplaces. The dataset consists of images and videos collected from various sources. </p>"},{"location":"scenarios/fall-and-accident-detection/#model","title":"Model","text":"<p>The model to detect fall and accident events is in progress and it will be released soon. </p>"},{"location":"scenarios/fall-and-accident-detection/#scenario-details","title":"Scenario details","text":"<p>The business logic for this scenario is as follows:</p> <ul> <li> <p>We use existing camera feeds from the premises to detect potential collision/accident and wet floor, monitor and detect occurrences of slip and fall incidents.  </p> </li> <li> <p>VisionAI system is able to run on edge devices. It uses camera feeds for processing.</p> </li> <li> <p>An alert will be raised, when a potential collision/accident and wet floor is detected and/or occurrence slip and fall instance.</p> </li> </ul> Test now with online Web-CamWith RTSP Camera - PipelinesWith Azure Setup <p>To test this model &amp; scenario, you can use the following steps:</p> <ul> <li> <p>Install the visionai package from PyPI</p> <pre><code>$ pip install visionai\n</code></pre> </li> <li> <p>Test the scenario from your local web-cam</p> <pre><code>$ visionai scenario test fall-and-accident-detection\n\nDownloading models for scenario: fall-and-accident-detection\nModel: fall-and-accident-detection: https://workplaceos.blob.core.windows.net/models/yolov5s-fall-and-accident-detection/yolov5s-fall-and-accident-detection-0.0.1.zip\nStarting scenario: fall-and-accident-detection..\n</code></pre> </li> <li> <p>You should be able to see the events generated on your console window with the detections of potential collision/accident and wet floor, slip and fall instances within the camera field of view.</p> </li> </ul> <p>[TODO]</p> <p>VisionAI app is available at a Azure Market place, one can download and use it by following steps mentioned here</p>"},{"location":"scenarios/fall-and-accident-detection/#features","title":"Features","text":"<ul> <li> <p>Continuous monitoring: The model continuously monitors the user movements to ensure that they are safe and alert the user or emergency services if necessary. This includes monitoring the user's heart rate, breathing, and other vital signs to detect any signs of distress or injury.</p> </li> <li> <p>Alerting system: The model is able to alert supervisors or managers and/or emergency services when it identifies a range of potential hazards, including person slip &amp; fall, potential collision/accident, wet floor, debris on the floor, and wet/slippery signs.</p> </li> <li> <p>Customization: The model must be customizable to fit the needs of different users. This includes settings for sensitivity, activity recognition, and user-specific parameters such as age, weight, and height.</p> </li> </ul>"},{"location":"scenarios/fall-and-accident-detection/#training-with-custom-data","title":"Training with custom data","text":"<p>The scenario is provided as part of our GPL-v3 package for VisionAI. If you wish to train this with custom datasets, please contact us and we can provide you with the training code. You can do custom training with your own datasets for free, as long as it complies with GPLv3 license (you give back the code to the community). If you are interested in a custom license, please contact us.</p>"},{"location":"scenarios/fall-and-accident-detection/#contact-us","title":"Contact Us","text":"<ul> <li>For technical issues, you can open a Github issue here.</li> <li>For business inquiries, you can contact us through our website.</li> </ul>"},{"location":"scenarios/firearms-and-knives/","title":"Firearms and knives detection","text":"<p>New Technology aims to improve Firearm Detection and Save Lives with VisionAI.</p> <p> </p> Detection of Firearms and knives event"},{"location":"scenarios/firearms-and-knives/#overview","title":"Overview","text":"<p>Firearms detection refers to the use of technology and methods to identify the presence of firearms in a particular location or setting. The goal of firearms detection is to prevent violence and ensure public safety by detecting and responding to the presence of firearms including knives, guns and other weapons.</p> <p>There are various methods and technologies used for firearms detection, including metal detectors, X-ray machines, and millimeter-wave scanners. All these solutions are invasive. </p> <p>With the advent in technology, our VisionAI solution for fire-arms detection is non-invasive in nature and it works by analyzing video footage to detect the presence of firearms or other weapons.</p>"},{"location":"scenarios/firearms-and-knives/#vision-ai-based-monitoring","title":"Vision AI based monitoring","text":"<p>Vision AI based monitors can be used to detect Firearms and knives events by providing real-time video feeds of the factory area. The cameras scan every frame to ensure there is no sign of firearms and knives.</p>"},{"location":"scenarios/firearms-and-knives/#events","title":"Events","text":"<p>VisionAI model's generated events would be:</p> <ul> <li>Person brandishing firearm</li> <li>Person brandishing knives</li> </ul>"},{"location":"scenarios/firearms-and-knives/#configuration","title":"Configuration","text":"<p>It is recommended to set up camera in ceiling view to detect weapons including guns and knives.</p>"},{"location":"scenarios/firearms-and-knives/#model-details","title":"Model Details","text":""},{"location":"scenarios/firearms-and-knives/#dataset","title":"Dataset","text":"<p>The dataset for this scenario is based on real-world firearms and knives events. The dataset consists of images and videos collected from various sources including UC Berkeley Anomaly Detection Dataset, UCF Crime Dataset etc.</p>"},{"location":"scenarios/firearms-and-knives/#model","title":"Model","text":"<p>The model to detect firearms and knives event is in progress and it will be released soon.</p>"},{"location":"scenarios/firearms-and-knives/#scenario-details","title":"Scenario details","text":"<p>The business logic for this scenario is as follows:</p> <ul> <li>We use existing camera feeds from the premises to detect firearms and knives events.</li> <li>VisionAI system is able to run on edge devices. It uses camera feeds for processing.</li> <li>We detect people in the camera feed and we monitor whether the person is carrying any firearms and knives.</li> <li>If the person is detected with this event, an alert is raised.</li> </ul> Test now with online Web-CamWith RTSP Camera - PipelinesWith Azure Setup <p>To test this model &amp; scenario, you can use the following steps:</p> <ul> <li> <p>Install the visionai package from PyPI</p> <pre><code>$ pip install visionai\n</code></pre> </li> <li> <p>Test the scenario from your local web-cam</p> <pre><code>$ visionai scenario test firearms-knives-detection\n\nDownloading models for scenario: firearms-knives-detection\nStarting scenario: firearms-knives-detection..\n</code></pre> </li> <li> <p>You should be able to see the events generated on your console window with the detections of firearms and knives within the camera field of view.</p> </li> </ul> <p>[TODO]</p> <p>VisionAI app is available at a Azure Market place, one can download and use it by following steps mentioned here</p>"},{"location":"scenarios/firearms-and-knives/#features","title":"Features","text":"<p>The VisionAI solution is the most efficient way of implementing this scenario, as evidenced by the following features:</p> <ul> <li> <p>Unmatched accuracy</p> <p>Trained and Tested to give the best results. Our systems are trained to detect firearms and knives at the earliest detection with an accuracy of 99%</p> </li> <li> <p>Lightning Fast and Response Time</p> <p>Our Ultra-fast Processing provides real-time inference results and feedback (~30 frames per second processing). </p> </li> <li> <p>Minimizing false-positives/negatives</p> <p>Our systems create a fail-proof system by ensuring there are no false-positives or false-negatives. </p> </li> <li> <p>Scalability and Deployment </p> <p>Our pre-trained/custom models can be deployed instantly and are camera independent which means they can be pre-installed with existing cameras on site. We also offer cameras, IoT sensors and edge devices with strategic placement that helps scale a large workplace area with minimum installations. </p> </li> <li> <p>Custom Integrations</p> <p>Our detection system can be integrated with other safety systems, such as building management systems or alarm systems, allowing for a coordinated response to emergencies.</p> </li> </ul>"},{"location":"scenarios/firearms-and-knives/#training-with-custom-data","title":"Training with custom data","text":"<p>The scenario is provided as part of our GPL-v3 package for VisionAI. If you wish to train this with custom datasets, please contact us and we can provide you with the training code. You can do custom training with your own datasets for free, as long as it complies with GPLv3 license (you give back the code to the community). If you are interested in a custom license, please (contact us)[contact.md].</p>"},{"location":"scenarios/firearms-and-knives/#contact-us","title":"Contact Us","text":"<ul> <li>For technical issues, you can open a Github issue here.</li> <li>For business inquiries, you can contact us through our website.</li> </ul>"},{"location":"scenarios/forklift-zone-breach/","title":"Forklift Zone Monitoring","text":"<p>An intelligent alarm system that could be used to detect forklifts entering restricted zones</p>"},{"location":"scenarios/forklift-zone-breach/#overview","title":"Overview","text":"<p>Vehicle forklift zone monitoring is a system that uses cameras to monitor the forklifts entering restricted zones. It is used to enforce safety rules and calculate fine amounts, as well as to manage the flow of traffic. </p>"},{"location":"scenarios/forklift-zone-breach/#vision-ai-based-monitoring","title":"Vision AI based monitoring","text":"<p>VisionAI based forklift zone monitoring is an advanced technology that can be used to monitor and enforce forklifts entering restricted zones in the workplace. </p> <p>With our Vision AI monitoring you can authorize access as well as continuous monitor live feeds inside a restricted area for real-time detection of unauthorized personnel. Our fully automated detection models are not only more powerful and accurate than existing systems but also more affordable and easy to integrate into existing infrastructure allowing users to scale real-time detection with a few simple clicks.                       </p>"},{"location":"scenarios/forklift-zone-breach/#events","title":"Events","text":"<p>VisionAI model's generated events would be:</p> <ul> <li>Forklift observed outside of configured zone</li> <li>Pedestrian observed in forklift zone</li> </ul> <p>It is recommended that any instance of such event be reported to the appropriate authority. An event data may include information such as:</p> <ul> <li>Date and time of the event</li> <li>Location of the event</li> <li>license plate number</li> <li>Image of the event</li> <li>Video of the event</li> </ul>"},{"location":"scenarios/forklift-zone-breach/#configuration","title":"Configuration","text":"<p>To set up a camera system to detect forklifting of vehicles, you will need to consider the following:</p> <ul> <li> <p>Camera Placement: Cameras should be placed in locations where they can capture clear images of license plates, such as at entrances and exits to parking lots, toll booths, or intersections. Cameras should be mounted at an appropriate height and angle to capture the entire license plate.</p> </li> <li> <p>Camera Type: High-resolution cameras with a minimum resolution of 1080p are recommended for license plate detection. Cameras with a wide field of view (FOV) are also recommended to capture license plates from a distance.</p> </li> <li> <p>Lighting: Adequate lighting is essential for license plate detection. The lighting should be bright and evenly distributed to minimize shadows and glare.</p> </li> </ul>"},{"location":"scenarios/forklift-zone-breach/#model-details","title":"Model Details","text":""},{"location":"scenarios/forklift-zone-breach/#dataset","title":"Dataset","text":"<p>The dataset consists of images and videos collected from diverse sources and is designed to reflect real-world scenarios. It is evenly distributed with;</p> <ul> <li>Different environments: Both indoor and outdoor with varying/contrasting surrounding and infrastructure details</li> <li>Different lighting conditions: Day and night with varying light intensities</li> <li>Different camera angles: Front, side, and rear views</li> <li>Different vehicle types: Cars, trucks, buses, and motorcycles</li> <li>Different vehicle colors etc.</li> </ul>"},{"location":"scenarios/forklift-zone-breach/#model","title":"Model","text":"<p>The model to monitor enforcement of vehicle forklifting event is in progress and it will be released soon.</p>"},{"location":"scenarios/forklift-zone-breach/#scenario-details","title":"Scenario details","text":"<p>The business logic for this scenario is as follows: </p> <ul> <li> <p>We use existing camera feeds from the premises to monitor occurrences of vehicle forklifting events. </p> </li> <li> <p>VisionAI system is able to run on edge devices. It uses camera feeds for processing. </p> </li> <li> <p>We detect vehicle forklifting event in the camera feed, an alert is raised.</p> </li> </ul> Test now with online Web-CamWith RTSP Camera - PipelinesWith Azure Setup <p>To test this model &amp; scenario, you can use the following steps:</p> <ul> <li> <p>Install the visionai package from PyPI</p> <pre><code>$ pip install visionai\n</code></pre> </li> <li> <p>Test the scenario from your local web-cam</p> <pre><code>$ visionai scenario test vehicle-forklift-detection\n\nDownloading models for scenario: vehicle-forklift-detection\nModel: vehicle-forklift-detection: https://workplaceos.blob.core.windows.net/models/yolov5s-people/yolov5s-people-0.0.4.zip\nStarting scenario: vehicle-forklift-detection..\n</code></pre> </li> <li> <p>You should be able to see the events generated on your console window with the detections of vehicle cargo  monitoring event within the camera field of view.</p> </li> </ul> <p>[TODO]</p> <p>VisionAI app is available at a Azure Market place, one can download and use it by following steps mentioned here</p>"},{"location":"scenarios/forklift-zone-breach/#features","title":"Features","text":"<p>VisionAI based vehicle forklifting monitoring system exhibits following features:</p> <ul> <li> <p>Real-time detection: VisionAI based vehicle forklifting monitoring system can detect vehicle forklifting event in real-time.</p> </li> <li> <p>Scalable: VisionAI based vehicle forklifting monitoring system can be scaled to monitor multiple cameras at the same time.</p> </li> <li> <p>Easy to use: VisionAI based vehicle forklifting monitoring system is easy to use and can be integrated with existing infrastructure with a few simple clicks.</p> </li> </ul>"},{"location":"scenarios/forklift-zone-breach/#training-with-custom-data","title":"Training with custom data","text":"<p>The scenario is provided as part of our GPL-v3 package for VisionAI. If you wish to train this with custom datasets, please contact us and we can provide you with the training code. You can do custom training with your own datasets for free, as long as it complies with GPLv3 license (you give back the code to the community). If you are interested in a custom license, please contact us.</p>"},{"location":"scenarios/forklift-zone-breach/#contact-us","title":"Contact Us","text":"<ul> <li>For technical issues, you can open a Github issue here.</li> <li>For business inquiries, you can contact us through our website.</li> </ul>"},{"location":"scenarios/gas-leaks-detection/","title":"Gas Leak Detection","text":"<p>Gas leaks detection through VisionAI.</p>"},{"location":"scenarios/gas-leaks-detection/#overview","title":"Overview","text":"<p>Gas leak detection is important in industries to ensure the safety of workers, prevent damage to equipment, and minimize the risk of fires and explosions. There are several methods for gas leak detection in industries.</p> <p>VisionAI Infrared Camera (IR) based solution can be used to detect gas leaks in industrial settings. They can detect gases that are invisible to the naked eye and provide real-time images of gas leaks. They can be used for leak detection in large areas, such as pipelines and storage tanks.</p>"},{"location":"scenarios/gas-leaks-detection/#vision-ai-based-monitoring-using-ir-camera","title":"Vision AI based monitoring using IR Camera","text":"<p>Manual inspections based on installation of fixed gas detectors and portable gas detectors can be time-consuming and labor-intensive, which can make them impractical for large or complex industrial facilities.</p> <p>VisionAI's Infrared cameras detect the infrared radiation emitted by objects and can be used to visualize gas leaks that are invisible to the naked eye. The cameras can be used for leak detection in large areas, such as pipelines and storage tanks.</p> Gas leak Example Gas leak Example"},{"location":"scenarios/gas-leaks-detection/#gas-leak-detection-using-ir-cameras","title":"Gas leak Detection using IR cameras","text":""},{"location":"scenarios/gas-leaks-detection/#events","title":"Events","text":"<p>VisionAI model's generated events would be: - gas leak event detected</p>"},{"location":"scenarios/gas-leaks-detection/#configuration","title":"Configuration","text":"<p>It is recommended to set up Infrared cameras for gas leak detection.  In addition, high success rate is possible if the camera is operated during periods of low wind, warm weather, clear skies and leaks are imaged from distances of about 30 feet.</p>"},{"location":"scenarios/gas-leaks-detection/#model-details","title":"Model Details","text":""},{"location":"scenarios/gas-leaks-detection/#dataset","title":"Dataset","text":"<p>IR Image dataset for gas leak detection is in progress.</p>"},{"location":"scenarios/gas-leaks-detection/#model","title":"Model","text":"<p>The model to detect gas leak event is in progress and it will be released soon.</p>"},{"location":"scenarios/gas-leaks-detection/#scenario-details","title":"Scenario details","text":"<p>The business logic for this scenario is as follows: </p> <ul> <li>We use IR camera feeds from the premises to monitor the signs of leakage, spills in the workplace to ensure the safety of human lives in the workplace. </li> <li>VisionAI system is able to run on edge devices. It uses camera feeds for processing. </li> <li>We detect any kind of leakage in the camera feed.</li> <li>An alarming system is inplace as part of solution.</li> </ul> Test now with online Web-CamWith RTSP Camera - PipelinesWith Azure Setup <p>To test this model &amp; scenario, you can use the following steps:</p> <ul> <li> <p>Install the visionai package from PyPI</p> <pre><code>$ pip install visionai\n</code></pre> </li> <li> <p>Test the scenario from your local web-cam</p> <pre><code>$ visionai scenario test gas-leak-detection\n\nDownloading models for scenario: gas-leak-detection\nModel: gas-leak-detection: https://workplaceos.blob.core.windows.net/models/yolov5s-people/yolov5s-people-0.0.4.zip\nStarting scenario: gas-leak-detection..\n</code></pre> </li> <li> <p>You should be able to see the events generated on your console window with the detections of spills and leak event within the camera field of view.</p> </li> </ul> <p>[TODO]</p> <p>VisionAI app is available at a Azure Market place, one can download and use it by following steps mentioned here</p>"},{"location":"scenarios/gas-leaks-detection/#features","title":"Features","text":"<p>Salient features of IR based gas leak detection are: - Non-contact imaging is possible with VisionAI's infrared spectral imaging technology for gas leak detection.\u00a0</p> <ul> <li> <p>It can visualize the distribution of gas in addition to obtaining its spectral information. </p> </li> <li> <p>The type of gas can also\u00a0be determined with the aid of information.\u00a0</p> </li> </ul> <p>Note</p> <p>Different gases have varying infrared absorption properties, passive infrared detection equipment can detect a range of gases. As a result, it is very important to analyse gas leaks using passive infrared imaging.</p>"},{"location":"scenarios/gas-leaks-detection/#training-with-custom-data","title":"Training with custom data","text":"<p>If you wish to train this scenario with custom datasets, please contact us and we can provide you with the training code. If you are interested in a custom license, please (contact us)[contact.md].</p>"},{"location":"scenarios/gas-leaks-detection/#contact-us","title":"Contact Us","text":"<ul> <li>For technical issues, you can open a Github issue here.</li> <li>For business inquiries, you can contact us through our website.</li> </ul>"},{"location":"scenarios/hand-wash/","title":"Hand Wash/Hand Sanitizer","text":"<p>Enhance hygiene compliance with our cutting-edge hand wash detection model, designed to accurately monitor and promote effective hand washing practices.</p> <p> </p> Detection of hand-wash"},{"location":"scenarios/hand-wash/#overview","title":"Overview","text":"<p>Hand hygiene is critical to preventing the spread of infectious diseases. However, ensuring that individuals properly wash their hands at appropriate times can be challenging, particularly in high-traffic areas. </p> <p>A hand wash detection model can help address this challenge by automatically detecting and monitoring hand washing behaviors, providing real-time feedback and alerts to individuals who may need to improve their hygiene practices. This can enhance overall hygiene compliance, reduce the spread of germs and diseases, and promote a safer and healthier environment for all.</p>"},{"location":"scenarios/hand-wash/#vision-ai-based-monitoring","title":"Vision AI based monitoring","text":"<p>Vision AI-based hand-wash system is designed to detect and ensure no one misses hand wash and/or using hand sanitizer. The system uses image processing and machine learning algorithms to analyze the hand region in images or videos and identify the presence of hand sanitizer or hand wash based on specific features.</p> <p>Overall, the hand sanitizer/hand wash detection model is an important tool for promoting hygiene and preventing the spread of disease in a range of environments, from hospitals and schools to offices and public spaces. By detecting whether people have used hand sanitizer or hand wash, the system can help encourage good hygiene practices and reduce the risk of infection.</p>"},{"location":"scenarios/hand-wash/#model-details","title":"Model Details","text":""},{"location":"scenarios/hand-wash/#dataset","title":"Dataset","text":"<p>The dataset consists of images and videos collected from various sources. </p>"},{"location":"scenarios/hand-wash/#model","title":"Model","text":"<p>The model to detect hand wash event is in progress and it will be released soon. </p>"},{"location":"scenarios/hand-wash/#scenario-details","title":"Scenario details","text":"<p>The business logic for this scenario is as follows: </p> <ul> <li> <p>We use existing camera feeds from the premises to monitor whether people are using hand sanitizers or not. We detect the instances of missing hand wash.</p> </li> <li> <p>VisionAI system is able to run on edge devices. It uses camera feeds for processing. </p> </li> <li> <p>An alarming system is in place as part of an hand wash detection solution. </p> </li> </ul> Test now with online Web-CamWith RTSP Camera - PipelinesWith Azure Setup <p>To test this model &amp; scenario, you can use the following steps:</p> <ul> <li> <p>Install the visionai package from PyPI</p> <pre><code>$ pip install visionai\n</code></pre> </li> <li> <p>Test the scenario from your local web-cam</p> <pre><code>$ visionai scenario test hand-wash-detection\n\nDownloading models for scenario: hand-wash-detection\nModel: hand-wash-detection: https://workplaceos.blob.core.windows.net/models/yolov5s-hand-wash-detection/yolov5s-hand-wash-detection-0.0.4.zip\nStarting scenario: hand-wash-detection..\n</code></pre> </li> <li> <p>You should be able to see the information generated on your console window with the detections of missing hand-wash/hand-sanitizer event within the camera field of view.</p> </li> </ul> <p>[TODO]</p> <p>VisionAI app is available at a Azure Market place, one can download and use it by following steps mentioned here</p>"},{"location":"scenarios/hand-wash/#features","title":"Features","text":"<ul> <li> <p>Hand region detection: The system should be able to accurately detect the hand region in an image or video, which can be done using skin color segmentation or hand detection algorithms.</p> </li> <li> <p>Real-time performance: The system should be able to operate in real-time, analyzing images or videos quickly and accurately to detect whether a person has used hand sanitizer or hand wash.</p> </li> <li> <p>Robustness: The system should be able to perform well under varying conditions, such as different lighting conditions, hand positions, or hand appearances due to age, skin color, or skin conditions.</p> </li> </ul>"},{"location":"scenarios/hand-wash/#training-with-custom-data","title":"Training with custom data","text":"<p>The scenario is provided as part of our GPL-v3 package for VisionAI. If you wish to train this with custom datasets, please contact us and we can provide you with the training code. You can do custom training with your own datasets for free, as long as it complies with GPLv3 license (you give back the code to the community). If you are interested in a custom license, please contact us.</p>"},{"location":"scenarios/hand-wash/#contact-us","title":"Contact Us","text":"<ul> <li>For technical issues, you can open a Github issue here.</li> <li>For business inquiries, you can contact us through our website.</li> </ul>"},{"location":"scenarios/hazard-warning-suite/","title":"Hazard warning Suite","text":"<p>A \"hazard warning suite\" could refer to a set of scenarios that are designed to monitor, detect, and alert individuals or systems of potential hazards in a given environment. The suite could include various types of sensors and technologies, such as cameras, microphones, and environmental sensors, that can detect hazards like fires, toxic gas leaks, chemical spills, and other potential threats.</p> <p>The suite also include artificial intelligence techniques that can analyze sensor data and detect patterns or anomalies that could indicate the presence of a hazard. Once a hazard is detected, the suite could automatically trigger alarms, notifications, or alerts to relevant personnel, emergency services, or other systems to take appropriate action.</p> <p>In addition to real-time hazard detection and alerts, a hazard warning suite also include provisions for post-event analysis and reporting. This could help organizations identify patterns of hazards and develop strategies to prevent or mitigate future incidents.</p> <p>Overall, a hazard warning suite can be an important tool for ensuring the safety of workers, visitors, and the environment in a variety of settings, such as factories, warehouses, research facilities, and other high-risk environments.</p> <ul> <li>Smoke and Fire Detection</li> <li>No smoking/no vaping</li> <li>Spills &amp; leaks detection</li> <li>Gas leak detection</li> <li>Missing fire extinguisher</li> <li>Blocked exit monitoring</li> <li>Equipment temperature monitoring</li> <li>Equipment rust and corrosion</li> </ul>"},{"location":"scenarios/intrusion-detection/","title":"** Intrusion Detection**","text":"<p>An advanced physical intrusion detection system powered with Computer Vision and AI.</p>"},{"location":"scenarios/intrusion-detection/#overview","title":"Overview","text":"<p>Intrusion detection is a system to monitor suspicious activities indicating an intrusion. Intrusion detection is important to detect and prevent unauthorized access to the facility, production area and other restricted areas within a building or manufacturing setup. An intrusion detection system helps prevent theft and vandalism, protecting the facility from malicious activities that can disrupt production, damage equipment, and compromise sensitive information. In addition, an effective intrusion detection system helps to ensure compliance with regulations and standards related to security and safety in the industry.</p> <p>The current physical intrusion detection systems have a limited detection range and a number of other problems.  Existing solutions: - Do not provide comprehensive coverage across a large sprawling area. - Can be affected by environmental factors in the outdoors and raise false alarms. - Are vulnerable to tampering. - Do not offer much flexibility in terms of getting integrated well with other security measures like video surveillance not cost-effective. </p>"},{"location":"scenarios/intrusion-detection/#vision-ai-based-monitoring","title":"Vision AI based monitoring","text":"<p>Expand your security capabilities with VisionAI, a modern AI and ML solution to detect intrusions and protect your premises against all potential physical intrusion threats. Our model can accurately identify suspicious behaviors that may indicate a physical intrusion and instantly alerts the security personnel, allowing them to take appropriate and timely action against it to avoid associated dangers.</p> <p>Our smart solution seamlessly integrates with the existing camera infrastructure and analyzes the real-time video feed. It can help in different ways, offers a comprehensive solution to detect all forms of physical intrusion, and ensures compliance with all security and safety regulations.</p>"},{"location":"scenarios/intrusion-detection/#model-details","title":"Model Details","text":""},{"location":"scenarios/intrusion-detection/#dataset","title":"Dataset","text":"<p>The dataset consists of high-quality images and videos collected from diverse sources and is designed to reflect real-world scenarios. The dataset is representative of all types of environments that should be taken care of while detecting physical intrusion.  </p> <ul> <li> <p>Different locations/environments \u2013 The dataset includes images/videos from a variety of indoor and outdoor environments, locations, weather conditions and building layouts.</p> </li> <li> <p>Diversity of intruders \u2013 The dataset considers images and videos of intruders from different backgrounds trying to intrude in various poses such as standing, walking, running, crawling etc. </p> </li> <li> <p>Balanced - The dataset is evenly distributed and balanced between intrusion and non-intrusion examples to prevent bias towards one class of data.</p> </li> <li> <p>Different angles and perspectives - The dataset includes images and videos captured from different angles and lighting conditions to ensure the model can detect intrusion in various real-world scenarios.</p> </li> </ul>"},{"location":"scenarios/intrusion-detection/#model","title":"Model","text":"<p>The model to detection intrusion is in progress and will be released soon.</p>"},{"location":"scenarios/intrusion-detection/#scenario-details","title":"Scenario details","text":"<p>Our VisionAI solution for intrusion detection can be deployed to a physical perimeter integrated with the existing camera infrastructure and works in different scenarios to detect physical intrusion. The model is equipped to detect the following;</p> <ul> <li>Any person with suspicious behavior</li> <li>Anybody trying to intrude in a building or perimeter</li> <li>Anybody trying to trespass a restricted area without permission</li> <li>Anybody loitering around a restricted area for an extended period </li> </ul> Test now with online Web-CamWith RTSP Camera - PipelinesWith Azure Setup <p>To test this model &amp; scenario, you can use the following steps:</p> <ul> <li> <p>Install the visionai package from PyPI</p> <pre><code>$ pip install visionai\n</code></pre> </li> <li> <p>Test the scenario from your local web-cam</p> <pre><code>$ visionai scenario test intrusion-detection\n\nDownloading models for scenario: intrusion-detection\nModel: intrusion-detection: https://workplaceos.blob.core.windows.net/models/yolov5s-intrusion-detection/yolov5s-intrusion-detection-0.0.1.zip\nStarting scenario: intrusion-detection..\n</code></pre> </li> <li> <p>You should be able to see the events generated on your console window with detection of intrusions within the camera field of view.</p> </li> </ul> <p>[TODO]</p> <p>VisionAI app is available at a Azure Market place, one can download and use it by following steps mentioned here</p>"},{"location":"scenarios/intrusion-detection/#features","title":"Features","text":"<ul> <li> <p>Real-time monitoring: The model can be deployed to a physical perimeter integrated with the existing camera infrastructure and works in real-time to detect physical intrusion.</p> </li> <li> <p>Comprehensive coverage: The model can detect intrusion in a variety of environments, locations, weather conditions and building layouts.</p> </li> <li> <p>Customizable: The model can be customized to detect intrusion in a specific area or location based on user requirements.</p> </li> </ul> <p>-Integration: The model can be integrated with other security measures like video surveillance to provide a comprehensive solution to detect all forms of physical intrusion.</p> <ul> <li>Alert system: The solution has an alert system to notify the security personnel in case of an intrusion.</li> </ul>"},{"location":"scenarios/intrusion-detection/#training-with-custom-data","title":"Training with custom data","text":"<p>The scenario is provided as part of our GPL-v3 package for VisionAI. If you wish to train this with custom datasets, please contact us and we can provide you with the training code. You can do custom training with your own datasets for free, as long as it complies with GPLv3 license (you give back the code to the community). If you are interested in a custom license, please contact us.</p>"},{"location":"scenarios/intrusion-detection/#contact-us","title":"Contact Us","text":"<ul> <li>For technical issues, you can open a Github issue here.</li> <li>For business inquiries, you can contact us through our website.</li> </ul>"},{"location":"scenarios/light-sensor-monitoring/","title":"Light Sensor Monitoring","text":"<p>Monitor ambient light in real-time with our light-sensing solution</p>"},{"location":"scenarios/light-sensor-monitoring/#overview","title":"Overview","text":"<p>Energy is valuable, hence efforts should be made for optimum energy usage. As carbon footprints increase, enterprises must adopt innovative methods to optimize energy consumption and reduce spendings. Innovative lighting solutions can help minimize service disruption, allow prompt servicing, and ensure centralized governance.</p> <p>Light monitoring solutions are an innovation that is benefiting organizations across industries. These solutions use sensors to regulate and optimize lighting in the workplace and homes. There are various types of light sensors, namely ambient light sensors, infrared light sensor, sunlight sensors, and ultraviolet light sensors. Ambient light sensors are the most common sensing devices used in smartphones, tablets, smartwatches, LCDs, room lights, conference rooms, etc.</p>"},{"location":"scenarios/light-sensor-monitoring/#vision-ai-sensor-based-solution-for-ambient-light-monitoring","title":"Vision AI Sensor-Based Solution for Ambient Light Monitoring","text":"<p>Ambient light sensors can sense the surrounding light conditions and tell the processing chip to automatically adjust the display's brightness to reduce the product's power consumption. When the ambient illumination is high, the device using the ambient light sensor will automatically adjust to high brightness. When the external environment is dark, the display will be adjusted to low brightness to achieve automatic brightness adjustment.</p> <p>Visionify\u2019s light monitoring solution can monitor the ambient light of your electronic devices in real-time. When the ambient light of your equipment/devices sub-seeds a specific limit, it can be increased to the desired level. You can regulate illumination with our solution to cut bills and make best energy use. </p>"},{"location":"scenarios/light-sensor-monitoring/#model-details","title":"Model Details","text":""},{"location":"scenarios/light-sensor-monitoring/#dataset","title":"Dataset","text":"<ul> <li> <p>Outdoor environments: The dataset contains images and videos of the outside environment. The lighting devices can be turned off (lights, for instance) in sunlight with the dataset's help and turned on in the evening.</p> </li> <li> <p>Daylighting conditions: The dataset contains images and videos under different lighting conditions. The model may need to adjust its settings as per the lighting needs.</p> </li> <li> <p>Distance from light source: The dataset contains images of ambient light sources captured from different distances. </p> </li> </ul>"},{"location":"scenarios/light-sensor-monitoring/#model","title":"Model","text":""},{"location":"scenarios/light-sensor-monitoring/#scenario-details","title":"Scenario details","text":"<ul> <li>The model automatically resets the brightness when the ambient light subseeds a threshold limit.</li> <li>The model allows you to automatically turn on/off light sources at particular times of the day or for a specific period. </li> <li>When the equipment is faulty, and there is a high risk of an accident, the camera sensors can ring an alarm.</li> <li>Best suited for offices, factories, and homes with multiple intelligent appliances.</li> </ul> Test now with online Web-CamWith RTSP Camera - PipelinesWith Azure Setup <p>To test this model &amp; scenario, you can use the following steps:</p> <ul> <li> <p>Install the visionai package from PyPI</p> <pre><code>$ pip install visionai\n</code></pre> </li> <li> <p>Test the scenario from your local web-cam</p> <pre><code>$ visionai scenario test light-sensor-monitoring\n\nDownloading models for scenario: light-sensor-monitoring\nModel: light-sensor-monitoring: https://workplaceos.blob.core.windows.net/models/yolov5s-light-sensor-monitoring/yolov5s-light-sensor-monitoring-0.0.1.zip\nStarting scenario: light-sensor-monitoring..\n</code></pre> </li> <li> <p>You should be able to see the events generated on your console window with the detections of light sensor monitoring events within the camera field of view.</p> </li> </ul> <p>[TODO]</p> <p>VisionAI app is available at a Azure Market place, one can download and use it by following steps mentioned here</p>"},{"location":"scenarios/light-sensor-monitoring/#features","title":"Features","text":"<ul> <li> <p>Alert system: The model is programmed to send alerts when the light levels fall outside of a predetermined range.</p> </li> <li> <p>Integration with other systems: The model can be integrated with other systems, such as HVAC or lighting systems, to automatically adjust the environment based on the light levels.</p> </li> <li> <p>Remote monitoring: The model can be remotely monitored, allowing users to check the light levels from anywhere at any time.</p> </li> <li> <p>Energy efficiency: The model can help to promote energy efficiency by ensuring that lighting systems are only used when necessary, based on the detected light levels.</p> </li> </ul>"},{"location":"scenarios/light-sensor-monitoring/#training-with-custom-data","title":"Training with custom data","text":"<p>The scenario is provided as part of our GPL-v3 package for VisionAI. If you wish to train this with custom datasets, please contact us and we can provide you with the training code. You can do custom training with your own datasets for free, as long as it complies with GPLv3 license (you give back the code to the community). If you are interested in a custom license, please contact us.</p>"},{"location":"scenarios/light-sensor-monitoring/#contact-us","title":"Contact Us","text":"<ul> <li>For technical issues, you can open a Github issue here.</li> <li>For business inquiries, you can contact us through our website.</li> </ul>"},{"location":"scenarios/loitering/","title":"Loitering Detection","text":"<p>Keeping Public Spaces Safe: Innovations in Loitering Detection and Prevention with Vision AI.</p>"},{"location":"scenarios/loitering/#overview","title":"Overview","text":"<p>Loitering detection refers to the use of technology to identify and monitor individuals who are loitering in public spaces, with the aim of preventing crime, reducing security risks, and maintaining public safety. Loitering is typically defined as lingering or remaining in a particular location for an extended period of time, without a legitimate reason to be there.</p> <p>Loitering detection technologies may include sensors, cameras, and other monitoring systems that can detect and track individuals in public spaces. Some of these technologies can be integrated with machine learning and artificial intelligence (AI) algorithms to analyze data and identify patterns of behavior that may be indicative of loitering.</p> <p>Loitering detection technologies can be used in a variety of settings, including transportation hubs, shopping centers, and other public areas where large groups of people may congregate. These technologies can help identify potential security threats, such as individuals who may be carrying weapons or engaging in suspicious activities.</p> <p>However, there are also concerns about privacy and civil liberties when it comes to the use of loitering detection technologies. Critics argue that these technologies can be used to target marginalized communities, and may contribute to a climate of suspicion and discrimination.</p> <p>Overall, the use of loitering detection technologies is a complex issue that requires careful consideration of both security and privacy concerns. While these technologies can play an important role in maintaining public safety, it is important to ensure that their use is balanced with respect for individual rights and freedoms.</p>"},{"location":"scenarios/loitering/#vision-ai-based-monitoring","title":"Vision AI based monitoring","text":"<p>Vision AI based monitors can be used to for the detection of loitering events by providing real-time video feeds of the factory area. The cameras scan every frame and raise an event when a person enters from an usually closed location, person detected during off-hours, person detected for extended duration of time.</p>"},{"location":"scenarios/loitering/#model-details","title":"Model Details","text":""},{"location":"scenarios/loitering/#dataset","title":"Dataset","text":"<p>The dataset for this scenario is based on real-world loitering detection events. The dataset consists of images and videos collected from various sources. </p>"},{"location":"scenarios/loitering/#model","title":"Model","text":"<p>The model to detect loitering is in progress and it will be released soon.</p>"},{"location":"scenarios/loitering/#scenario-details","title":"Scenario details","text":"<p>The business logic for this scenario is as follows:</p> <ul> <li>We use existing camera feeds from the premises for raising loitering events.</li> <li>VisionAI system is able to run on edge devices. It uses camera feeds for processing.</li> <li>From the camera feed we monitor if a person enters from an usually closed location, person detected during off-hours, person detected for extended duration of time.</li> <li>If loitering event is detected, an alert is raised.</li> </ul> Test now with online Web-CamWith RTSP Camera - PipelinesWith Azure Setup <p>To test this model &amp; scenario, you can use the following steps:</p> <ul> <li> <p>Install the visionai package from PyPI</p> <pre><code>$ pip install visionai\n</code></pre> </li> <li> <p>Test the scenario from your local web-cam</p> <pre><code>$ visionai scenario test loitering-detection\n\nDownloading models for scenario: loitering-detection\nStarting scenario: loitering-detection..\n</code></pre> </li> <li> <p>You should be able to see the events generated on your console window with the detections of loitering within the camera field of view.</p> </li> </ul> <p>[TODO]</p> <p>VisionAI app is available at a Azure Market place, one can download and use it by following steps mentioned here</p>"},{"location":"scenarios/loitering/#features","title":"Features","text":"<p>The VisionAI solution is the most efficient way of implementing this scenario, as evidenced by the following features:</p> <ul> <li> <p>Customization: Our systems are customizable to fit your needs. We can train our models with your own data and provide you with a custom solution. We also provide a custom license for our software if you wish to use it in a closed environment.</p> </li> <li> <p>Unmatched accuracy: Trained and Tested to give the best results. Our systems are trained to detect loitering events with an accuracy of 99%</p> </li> <li> <p>Lightning Fast and Response Time: Our Ultra-fast Processing provides real-time inference results and feedback (~30 frames per second processing). </p> </li> <li> <p>Scalability and Deployment: Our pre-trained/custom models can be deployed instantly and are camera independent which means they can be pre-installed with existing cameras on site. We also offer cameras, IoT sensors and edge devices with strategic placement that helps scale a large workplace area with minimum installations. </p> </li> </ul>"},{"location":"scenarios/loitering/#training-with-custom-data","title":"Training with custom data","text":"<p>The scenario is provided as part of our GPL-v3 package for VisionAI. If you wish to train this with custom datasets, please contact us and we can provide you with the training code. You can do custom training with your own datasets for free, as long as it complies with GPLv3 license (you give back the code to the community). If you are interested in a custom license, please contact us.</p>"},{"location":"scenarios/loitering/#contact-us","title":"Contact Us","text":"<ul> <li>For technical issues, you can open a Github issue here.</li> <li>For business inquiries, you can contact us through our website.</li> </ul>"},{"location":"scenarios/max-occupancy-count/","title":"Maximum Occupancy","text":"<p>Transform the way you manage occupancy in real-time with our cutting-edge Computer Vision Occupancy Monitoring Solution.</p>"},{"location":"scenarios/max-occupancy-count/#overview","title":"Overview","text":"<p>Effective crowd management is critical for many workplaces like airports, hospitals, factories, and retail shops, among others. One key aspect is maintaining compliance with maximum occupancy limits which is  crucial for maintaining safety, mitigating potential injuries, and legal issues. </p> <p>Existing solutions for tracking maximum occupancy typically rely on manual monitoring, which can be labor-intensive, prone to errors, and time-consuming while other systems such as sensors and RFID (Radio-Frequency Identification) tags produce a lot of false readings, have limited range and incur significant expenses.</p> <p>As such, there is a need for more efficient and reliable methods for monitoring and managing maximum occupancy. A promising solution lies in the use of computer vision technology, which can accurately detect and track individuals in real-time, providing an automated and seamless approach to crowd management.</p>"},{"location":"scenarios/max-occupancy-count/#vision-ai-based-monitoring","title":"Vision AI based monitoring","text":"<p>Maintain workplace occupancy limits flawlessly by leveraging computer vision-powered occupancy monitoring. Monitor Occupancy levels in Real-Time, get instant alerts and warnings whenever count exceeds threshold limit. With our ready-to-deploy models, businesses can effortlessly adhere to regulations and maintain a safe environment without the need for manual monitoring or complex sensor installations. </p> <p>A single camera can cover a wide area, allowing businesses to leverage our AI-based technology with minimal effort. You can easily augment your existing infrastructure and get started with our models with just a few clicks.</p> <p> </p> monitoring maximum occupancy"},{"location":"scenarios/max-occupancy-count/#events","title":"Events","text":"<p>VisionAI model's generated events would be:</p> <ul> <li>Person count exceeds limit (Max occupancy exceeded)</li> <li>Person count is below limit (Max occupancy not exceeded)</li> </ul>"},{"location":"scenarios/max-occupancy-count/#event-data","title":"Event Data","text":"<p>An event data for maximum occupancy scenario may include information such as:</p> <ul> <li>Date and time of the event</li> <li>Location of the event</li> <li>type of event (Max occupancy exceeded, etc.)</li> <li>Image of the event</li> </ul>"},{"location":"scenarios/max-occupancy-count/#configuration","title":"Configuration","text":"<p>It is recommended to set up camera setups to monitor maximum occupancy in the workplace. The location of cameras to monitor maximum occupancy will depend on the specific policies being enforced and the nature of the work environment.</p>"},{"location":"scenarios/max-occupancy-count/#model-details","title":"Model Details","text":""},{"location":"scenarios/max-occupancy-count/#dataset","title":"Dataset","text":"<p>The dataset consists of images and videos of people in different scenarios.    </p>"},{"location":"scenarios/max-occupancy-count/#model","title":"Model","text":"<p>The model to monitor maximum occupancy is in progress and it will be released soon.</p>"},{"location":"scenarios/max-occupancy-count/#scenario-details","title":"Scenario details","text":"<p>Real-time detection and alerts for different scenarios includes but are not limited to:</p> <ul> <li>When person count exceeds the predefined limit</li> <li>When the threshold is about to be reached as a safety warning</li> <li>Warnings based on population flow</li> </ul> Test now with online Web-CamWith RTSP Camera - PipelinesWith Azure Setup <p>To test this model &amp; scenario, you can use the following steps:</p> <ul> <li> <p>Install the visionai package from PyPI</p> <pre><code>$ pip install visionai\n</code></pre> </li> <li> <p>Test the scenario from your local web-cam</p> <pre><code>$ visionai scenario test max-occupancy\n</code></pre> <p>Downloading models for scenario: max-occupancy Model: max-occupancy: https://workplaceos.blob.core.windows.net/models/yolov5s-people/yolov5s-people-0.0.4.zip</p> <p>Starting scenario: max-occupancy..</p> <p>```</p> </li> <li> <p>You should be able to see the events generated on your console window with the detections of maximum occupancy event within the camera field of view.</p> </li> </ul> <p>[TODO]</p> <p>VisionAI app is available at a Azure Market place, one can download and use it by following steps mentioned here</p>"},{"location":"scenarios/max-occupancy-count/#features","title":"Features","text":"<p>Some potential features of VisionAI for monitoring maximum occupancy include:</p> <ul> <li> <p>Real-time monitoring of maximum occupancy: VisionAI can monitor maximum occupancy in real-time, providing an automated and seamless approach to crowd management.</p> </li> <li> <p>Instant alerts and warnings: VisionAI can send instant alerts and warnings whenever count exceeds threshold limit.</p> </li> <li> <p>Easy to deploy: VisionAI can be easily deployed with minimal effort, allowing businesses to leverage our AI-based technology with minimal effort.</p> </li> </ul>"},{"location":"scenarios/max-occupancy-count/#training-with-custom-data","title":"Training with custom data","text":"<p>The scenario is provided as part of our GPL-v3 package for VisionAI. If you wish to train this with custom datasets, please contact us and we can provide you with the training code. You can do custom training with your own datasets for free, as long as it complies with GPLv3 license (you give back the code to the community). If you are interested in a custom license, please contact us.</p>"},{"location":"scenarios/max-occupancy-count/#contact-us","title":"Contact Us","text":"<ul> <li>For technical issues, you can open a Github issue here.</li> <li>For business inquiries, you can contact us through our website.</li> </ul>"},{"location":"scenarios/missing-fire-extinguisher/","title":"Missing Fire Extinguisher","text":"<p>Strengthen your smoke &amp; fire detection compliance - through adding custom logic for checking for missing fire extinguisher from required places.</p>"},{"location":"scenarios/missing-fire-extinguisher/#overview","title":"Overview","text":"<p>Fire Extinguishers prove to be a crucial preventive measure against unexpected fires. These are essential components of safety features that can help contain early fires before they escalate into large ones. Adequately installed fire extinguishers in the building offer round-the-clock protection against unexpected fires, and a majority of fires can be put out using handy fire extinguishers.</p> <p>However, if a fire breaks out, missing fire extinguishers can increase the risk of injury and damage and can also have legal and regulatory obligations. Failure to comply with regulations can incur fines or legal issues. It also raises a question about the business\u2019s reputation and leads to a loss of trust from customers. The existing fire warning and safety systems also cannot identify early fire signs, and none of them offer detection of missing fire extinguishers.</p>"},{"location":"scenarios/missing-fire-extinguisher/#vision-ai-based-monitoring","title":"Vision AI based monitoring","text":"<p>Make your workplace safer with our VisionAI monitoring, a computer vision and deep learning-based solution that helps you detect missing fire extinguishers by analyzing visual data, making it easier for businesses to ensure that they have the necessary safety equipment in place.</p> <p>Our fully automated system guards your facility 24/7. It sends instant alerts whenever a missing fire extinguisher is detected, allowing businesses to achieve improved fire safety, compliance with regulations, cost savings, and peace of mind. </p> Missing Fire extinguisher at office Missing Fire extinguisher at factory"},{"location":"scenarios/missing-fire-extinguisher/#events","title":"Events","text":"<p>VisionAI model's generated events would be:</p> <ul> <li>Missing fire extinguisher</li> </ul> <p>It is recommended that any instance of a missing fire extinguisher be reported to the appropriate authority. An event data for a missing fire extinguisher may include information such as:</p> <ul> <li>Date and time the missing fire extinguisher was discovered</li> <li>Location of the missing fire extinguisher, including the building, floor, and room number</li> <li>Type of fire extinguisher that is missing</li> </ul>"},{"location":"scenarios/missing-fire-extinguisher/#configuration","title":"Configuration","text":"<p>It is recommended to set up camera in ceiling view to detect missing fire extinguisher event. Cameras can see an area, mark areas where a fire extinguisher should be present. Any time it is removed or used or not seen, we will generate this event.</p>"},{"location":"scenarios/missing-fire-extinguisher/#model-details","title":"Model Details","text":""},{"location":"scenarios/missing-fire-extinguisher/#dataset","title":"Dataset","text":"<p>The dataset consists of images and videos collected from diverse sources and is designed to reflect real-world scenarios. It is evenly distributed with;</p> <ul> <li> <p>Different locations: Different locations within an industrial setting where fire extinguishers are usually installed, like emergency exits, heavy machinery, near combustible material etc., all have been considered within the dataset.</p> </li> <li> <p>Different angles and perspectives: The dataset includes images captured from different angles and perspectives, such as from above, below, or from the side, in a crowded space or fire extinguishers obscured behind other objects in different locations.</p> </li> <li> <p>Different lighting conditions: The dataset includes images in different lighting conditions, like where the fire extinguisher is clearly visible, partially visible or obstructed.</p> </li> <li> <p>Different classes: The dataset is balanced between the two classes, present and missing fire extinguishers, to avoid bias in the model.  </p> </li> </ul>"},{"location":"scenarios/missing-fire-extinguisher/#model","title":"Model","text":"<p>The model to detect missing fire extinguisher event is in progress and it will be released soon.</p>"},{"location":"scenarios/missing-fire-extinguisher/#scenario-details","title":"Scenario details","text":"<p>Our VisionAI solution detects missing fire extinguishers in different scenarios within an industrial setting where the presence of fire extinguishers is expected. These scenarios can be;</p> <ul> <li> <p>Fire extinguishers are generally wall or pillar mounted. Our model is trained to detect the missing fire extinguishers in these locations.</p> </li> <li> <p>Our state-of-the-art models can detect missing fire extinguishers near hazardous/combustible material inside a manufacturing plant.</p> </li> <li> <p>There are specific equipment or machinery that require the availability of fire extinguishers in close proximity for safety reasons. Our model can identify any such space if a fire extinguisher is missing.</p> </li> <li> <p>Also, the model can detect missing fire extinguishers near emergency exits, where they are installed for quick access in case of fire.  </p> </li> </ul> Test now with online Web-CamWith RTSP Camera - PipelinesWith Azure Setup <p>To test this model &amp; scenario, you can use the following steps:</p> <ul> <li> <p>Install the visionai package from PyPI</p> <pre><code>$ pip install visionai\n</code></pre> </li> <li> <p>Test the scenario from your local web-cam</p> <pre><code>$ visionai scenario test miss-fire-exting-detection\n\nDownloading models for scenario: miss-fire-exting-detection\nModel: miss-fire-exting-detection: https://workplaceos.blob.core.windows.net/models/yolov5s-people/yolov5s-people-0.0.4.zip\nStarting scenario: miss-fire-exting-detection..\n</code></pre> </li> <li> <p>You should be able to see the events generated on your console window with the detections of smoking/vaping event within the camera field of view.</p> </li> </ul> <p>[TODO]</p> <p>VisionAI app is available at a Azure Market place, one can download and use it by following steps mentioned here</p>"},{"location":"scenarios/missing-fire-extinguisher/#features","title":"Features","text":"<p>Some potential features of VisionAI for detecting missing fire extinguishers could include:</p> <ul> <li> <p>Continuous monitoring: An system could continuously monitor fire extinguisher locations and track any changes in real-time, enabling prompt detection of missing fire extinguishers.</p> </li> <li> <p>Location tracking: The system could use sensors or other location tracking devices to monitor the precise location of fire extinguishers and track any movements or changes in their location.</p> </li> <li> <p>Alerts and notifications: When a missing fire extinguisher is detected, the system could automatically generate an alert or notification to the appropriate personnel or authorities, enabling prompt corrective action.</p> </li> <li> <p>Historical data analysis: Over time, the system could collect and analyze historical data on fire extinguisher locations, enabling identification of trends or patterns that may indicate underlying fire safety issues.</p> </li> </ul> <p>Note</p> <p>Overall, an AI-based system for detecting missing fire extinguishers could enable more proactive and efficient fire safety monitoring and management, helping to prevent fires and ensure the safety of occupants and property.</p>"},{"location":"scenarios/missing-fire-extinguisher/#training-with-custom-data","title":"Training with custom data","text":"<p>The scenario is provided as part of our GPL-v3 package for VisionAI. If you wish to train this with custom datasets, please contact us and we can provide you with the training code. You can do custom training with your own datasets for free, as long as it complies with GPLv3 license (you give back the code to the community). If you are interested in a custom license, please contact us.</p>"},{"location":"scenarios/missing-fire-extinguisher/#contact-us","title":"Contact Us","text":"<ul> <li>For technical issues, you can open a Github issue here.</li> <li>For business inquiries, you can contact us through our website.</li> </ul>"},{"location":"scenarios/no-food-or-drinks/","title":"No Food, No Drinks","text":"<p>An easier, smarter way to enforce policies with VisionAI</p> <p> </p> Event: No food, No drinks"},{"location":"scenarios/no-food-or-drinks/#overview","title":"Overview","text":"<p>Implementing a \u2018No Food, No Drinks\u2019 policy can be challenging. However, for some industries like healthcare, manufacturing, textiles, laboratories and pharmaceuticals, it is imperative to have an effective \u2018no food, no drinks\u2019 policy to maintain strict hygiene and safety standards essential to prevent product contamination. Unfortunately, the current mechanisms rely on manual inspections, are highly human-oriented, and are difficult to automate, depending on enforcement by supervisors and security personnel.</p> <p>Manual inspections can be inconsistent and subjective. Also, humans are prone to errors; they may miss food or drink items that are not easily visible. Furthermore, manual inspections may not be able to cover all areas of the workplace, and they can create privacy concerns for employees. All these factors can compromise the effectiveness of the policy and may lead to non-compliance.</p>"},{"location":"scenarios/no-food-or-drinks/#vision-ai-based-monitoring","title":"Vision AI based monitoring","text":"<p>Implement VisionAI solution to address these problems by providing consistent, objective, and cost-effective enforcement while minimizing privacy concerns. Our smart solution seamlessly integrates with your existing camera infrastructure to capture all areas where food and drinks may be present. The model works by detecting and identifying individuals carrying or consuming foods or beverages. In addition, the model is trained to recognize different types of foods and drinks and the actions associated with consuming them, such as holding a cup or bottle and lifting it to the mouth, chewing, swallowing etc.</p> <p>The model analyzes the video feed in real-time and works with the greatest accuracy. It instantly alerts the appropriate personnel to take action and proves to be an effective tool for enforcing a \u2018no food and drinks\u2019 policy in an organizational setting, improving hygiene and safety measures, and ensuring compliance with regulations.</p>"},{"location":"scenarios/no-food-or-drinks/#model-details","title":"Model Details","text":""},{"location":"scenarios/no-food-or-drinks/#dataset","title":"Dataset","text":"<p>The dataset consists of images and videos collected from diverse sources and is designed to reflect real-world scenarios. The dataset is representative of the types of people, settings, and situations where the policy will be enforced. It is evenly distributed with:</p> <ul> <li> <p>Different locations: Different locations within an industrial setting where food and drinks are not allowed, like production areas, offices or workstations, and storage areas, have been considered within the dataset.</p> </li> <li> <p>Different angles and perspectives: The dataset includes images captured from different angles and perspectives, such as a front-facing view to have a clear view of the person for any food item they may be carrying, a top-down view for when the person is seated or when food or drinks are on a table, side view, low/high angle view and oblique view to detect from a diagonal or slanted perspective. An oblique view is useful when carrying food or drinks in a bag or container.</p> </li> <li> <p>Different lighting conditions: The dataset includes images of lighting conditions, like where the food items are partially visible or obstructed.</p> </li> <li> <p>Different versions: The images in the dataset have variations in the appearance of people, food, and drinks, so the model can learn to recognize them in different contexts.</p> </li> </ul>"},{"location":"scenarios/no-food-or-drinks/#model","title":"Model","text":"<p>The model for detecting food items and drinks is in progress and it will be released soon.</p>"},{"location":"scenarios/no-food-or-drinks/#scenario-details","title":"Scenario details","text":"<p>Our VisionAI solution for detecting food items and drinks works in different scenarios within an industrial setting. Our model can be deployed at the entrance/exit points or inside to monitor and see whether employees or visitors carry food items or beverages. The model is equipped to detect the following: - Person carrying a food item - Person carrying any beverage - Any spill event taking place within the specified area</p> Test now with online Web-CamWith RTSP Camera - PipelinesWith Azure Setup <p>To test this model &amp; scenario, you can use the following steps:</p> <ul> <li> <p>Install the visionai package from PyPI</p> <pre><code>$ pip install visionai\n</code></pre> </li> <li> <p>Test the scenario from your local web-cam</p> <pre><code>$ visionai scenario test no-food-or-drinks\n\nDownloading models for scenario: no-food-or-drinks\nModel: no-food-or-drinks: https://workplaceos.blob.core.windows.net/models/yolov5s-no-food-or-drinks/yolov5s-no-food-or-drinks-0.0.1.zip\nStarting scenario: no-food-or-drinks..\n</code></pre> </li> <li> <p>You should be able to see the events generated on your console window with detection of food items and drinks within the camera field of view.</p> </li> </ul> <p>[TODO]</p> <p>VisionAI app is available at a Azure Market place, one can download and use it by following steps mentioned here</p>"},{"location":"scenarios/no-food-or-drinks/#features","title":"Features","text":"<ul> <li> <p>Real-time monitoring: The model analyzes the video feed in real-time and works with the greatest accuracy. It is an effective tool for enforcing a \u2018no food and drinks\u2019 policy in an organizational setting, improving hygiene and safety measures, and ensuring compliance with regulations.</p> </li> <li> <p>Cost-effective: The model is cost-effective and does not require any additional hardware or software. It can be deployed with existing camera infrastructure.</p> </li> <li> <p>Privacy: The model is designed to protect the privacy of employees and visitors. It does not capture or store any personal information, and it does not require any personal information to be provided by the user.</p> </li> <li> <p>Customizable: The model can be customized to suit the needs of the user. It can be trained with custom data to detect and recognize different types of food items and drinks.</p> </li> </ul>"},{"location":"scenarios/no-food-or-drinks/#training-with-custom-data","title":"Training with custom data","text":"<p>The scenario is provided as part of our GPL-v3 package for VisionAI. If you wish to train this with custom datasets, please contact us and we can provide you with the training code. You can do custom training with your own datasets for free, as long as it complies with GPLv3 license (you give back the code to the community). If you are interested in a custom license, please contact us.</p>"},{"location":"scenarios/no-food-or-drinks/#contact-us","title":"Contact Us","text":"<ul> <li>For technical issues, you can open a Github issue here.</li> <li>For business inquiries, you can contact us through our website.</li> </ul>"},{"location":"scenarios/no-phone-usage/","title":"Mobile Phone Usage Detection","text":"<p>Enabling businesses to overcome digital distractions and misuse of mobile phones at workplaces.</p>"},{"location":"scenarios/no-phone-usage/#overview","title":"Overview","text":"<p>Mobile phones at workplaces are proving to be an insidious way to execute malicious purposes. Prevent industrial espionage and reinforce security measures with the most reliable mobile phone usage detection models powered by AI and Deep Learning.  Visionify's computer vision solutions are more accurate than the conventional methods, can safely detect mobile phone usage (people taking pictures, recording videos/audios, sending texts in prohibited areas). We offer instant integration with your existing camera infrastructure, and quick results. </p>"},{"location":"scenarios/no-phone-usage/#vision-ai-based-monitoring","title":"Vision AI based monitoring","text":"<p>Vision AI-based system can be used to detect mobile phone usage by providing real-time video feeds at workplaces. This system can be used to detect an event of workers using mobile phones aganist compliance policies.</p> <p>Enforce any company policies on mobile-usage in the workplace. These camera based detection processes should be supplimented by strong compliance practices. If workers are prohibited from mobile usage, ensure that they are aware of the policy and the consequences of violating it. So, if an employee is found to be using mobile phone, an appropriate action needs to be taken.</p>"},{"location":"scenarios/no-phone-usage/#model-details","title":"Model Details","text":""},{"location":"scenarios/no-phone-usage/#dataset","title":"Dataset","text":"<p>Model training is carried out with Microsoft COCO: Common Objects in Context dataset. Person class is considered for model building. </p> <p>Basically, COCO is a  large-scale dataset and it provides real-world data representation including:</p> <ul> <li>Indoor vs Outdoor environments</li> <li>Male vs Female</li> <li>Day vs Night</li> <li>Different types of clothing</li> <li>Different distances from the camera</li> <li>Various lighting conditions</li> <li>Various camera angles and resolutions</li> <li>Using seurity camera feeds</li> </ul>"},{"location":"scenarios/no-phone-usage/#model","title":"Model","text":"<p>The model is built using Yolov5 pre-trained model for person and mobile classes. The yolov5 model is used to identify the human body landmarks of the subject. </p> <p>The Yolov5 model provides the following metrics:</p> Precision  Recall  mAP  <p>and landmark detection model gives the following metrics:</p> Precision  Recall  mAP  <p>The model is light-weight enough to be run on any edge devices.</p>"},{"location":"scenarios/no-phone-usage/#scenario-details","title":"Scenario details","text":"<p>The business logic for this scenario is as follows: - We use existing camera feeds from the premises to detect mobile phone usage by employees. - VisionAI system is able to run on edge devices. It uses camera feeds for processing.  - We detect the mobile phone usage event and an alert is raised.</p> Test now with online Web-CamWith RTSP Camera - PipelinesWith Azure Setup <p>To test this model &amp; scenario, you can use the following steps:</p> <ul> <li> <p>Install the visionai package from PyPI</p> <pre><code>$ pip install visionai\n</code></pre> </li> <li> <p>Test the scenario from your local web-cam</p> <pre><code>$ visionai scenario test mobile-usage-detection\n\nDownloading models for scenario: mobile-usage-detection\nModel: mobile-usage-detection: https://workplaceos.blob.core.windows.net/models/yolov5s-mobile-usage-detection/yolov5s-mobile-usage-detection-0.0.1.zip\nStarting scenario: mobile-usage-detection..\n</code></pre> </li> <li> <p>You should be able to see the events generated on your console window with phone usage being detected within the camera field of view.</p> </li> </ul> <p>[TODO]</p> <p>VisionAI app is available at a Azure Market place, one can download and use it by following steps mentioned here</p>"},{"location":"scenarios/no-phone-usage/#features","title":"Features","text":"<ul> <li>Unparalleled Accuracy and faster detection: Our models, not only trained to detect the presence of a mobile device but detect its usage, are equipped to achieve an outstanding accuracy of up to 92%.   </li> <li>Seamless integration and Scalability: An end-to-end solution, integrates seamlessly with your existing camera network and is ready to detect. Easily expandable by adding more IP cameras to the network. </li> <li>Integrated Solution: It is an integrated system combining surveillance and mobile phone detection in one system.</li> <li>Absolute Privacy: We understand your concerns about data privacy and take a proactive approach to preserve it. Our models are privacy oriented by design.</li> <li>Automate and Grow: Leverage the precision and power of the groundbreaking computer vision technology, automate complex tasks and detect flaws sooner to achieve better performance and reduced costs.   </li> <li>Versatile Framework: We offer flexibility in deployment; the model can operate at the Edge, in the cloud, or any self-hosted environment. </li> </ul>"},{"location":"scenarios/no-phone-usage/#training-with-custom-data","title":"Training with custom data","text":"<p>The scenario is provided as part of our GPL-v3 package for VisionAI. If you wish to train this with custom datasets, please contact us and we can provide you with the training code. You can do custom training with your own datasets for free, as long as it complies with GPLv3 license (you give back the code to the community). If you are interested in a custom license, please contact us.</p>"},{"location":"scenarios/no-phone-usage/#contact-us","title":"Contact Us","text":"<ul> <li>For technical issues, you can open a Github issue here.</li> <li>For business inquiries, you can contact us through our website.</li> </ul>"},{"location":"scenarios/no-smoking-hazard/","title":"No Smoking/No Vaping","text":"<p>No smoking &amp; No vaping zone enforcements with Vision AI.</p> <p> </p> Detection of Smoking event"},{"location":"scenarios/no-smoking-hazard/#overview","title":"Overview","text":"<p>Smoking and vaping are typically banned in workplaces like manufacturing plants, construction sites, warehouses, chemical plants, etc., an ideal 100% compliance rate can be challenging to achieve. However, it is imperative for employers to ensure that their workplaces are absolutely smoke-free.</p> <p>VisionAI makes it possible to avert workplace hazards and help employers maintain 100% compliance through smart AI solutions. Our next-gen real-time detection systems make sure a fire/smoke or any sign of vaping is detected instantly. These systems are also trained to generate alerts and notifications accordingly.</p>"},{"location":"scenarios/no-smoking-hazard/#vision-ai-based-monitoring","title":"Vision AI based monitoring","text":"<p>Vision AI based monitors can be used to detect smoking/vaping events by providing real-time video feeds of the factory area. The cameras scan every frame to ensure there is no sign of smoking/vaping.</p>"},{"location":"scenarios/no-smoking-hazard/#model-details","title":"Model Details","text":""},{"location":"scenarios/no-smoking-hazard/#dataset","title":"Dataset","text":"<p>The dataset for this scenario is based on real-world smoking/vaping events. The dataset consists of images and videos collected from various sources. </p>"},{"location":"scenarios/no-smoking-hazard/#model","title":"Model","text":"<p>The model to detect smoking/vaping event is in progress and it will be released soon.</p>"},{"location":"scenarios/no-smoking-hazard/#scenario-details","title":"Scenario details","text":"<p>The business logic for this scenario is as follows:</p> <ul> <li>We use existing camera feeds from the premises to detect smoking/vaping events.</li> <li>VisionAI system is able to run on edge devices. It uses camera feeds for processing.</li> <li>We detect people in the camera feed and we monitor whether the person is involved in any smoking/vaping activity.</li> <li>If the person is detected with this event, an alert is raised.</li> </ul> Test now with online Web-CamWith RTSP Camera - PipelinesWith Azure Setup <p>To test this model &amp; scenario, you can use the following steps:</p> <ul> <li> <p>Install the visionai package from PyPI</p> <pre><code>$ pip install visionai\n</code></pre> </li> <li> <p>Test the scenario from your local web-cam</p> <pre><code>$ visionai scenario test no-smoking-detection\n\nDownloading models for scenario: no-smoking-detection\nModel: no-smoking-detection: https://workplaceos.blob.core.windows.net/models/yolov5s-people/yolov5s-people-0.0.4.zip\nStarting scenario: no-smoking-detection..\n</code></pre> </li> <li> <p>You should be able to see the events generated on your console window with the detections of smoking/vaping within the camera field of view.</p> </li> </ul> <p>[TODO]</p> <p>VisionAI app is available at a Azure Market place, one can download and use it by following steps mentioned here</p>"},{"location":"scenarios/no-smoking-hazard/#features","title":"Features","text":"<p>The VisionAI solution is the most efficient way of implementing this scenario, as evidenced by the following features:</p> <ul> <li> <p>Unmatched accuracy</p> <p>Trained and Tested to give the best results. Our systems are trained to detect Fire and Smoke at the earliest detection with an accuracy of 99%</p> </li> <li> <p>Lightning Fast and Response Time</p> <p>Our Ultra-fast Processing provides real-time inference results and feedback (~30 frames per second processing). </p> </li> <li> <p>Minimizing false-positives/negatives</p> <p>Our systems create a fail-proof system by ensuring there are no false-positives or false-negatives. </p> </li> <li> <p>Scalability and Deployment </p> <p>Our models can be deployed instantly and are camera independent which means they can be pre-installed with existing cameras on site. We also offer cameras, IoT sensors and edge devices with strategic placement that helps scale a large workplace area with minimum installations. </p> </li> <li> <p>Custom Integrations</p> <p>Our detection system can be integrated with other safety systems, such as building management systems or alarm systems, allowing for a coordinated response to emergencies.</p> </li> </ul>"},{"location":"scenarios/no-smoking-hazard/#training-with-custom-data","title":"Training with custom data","text":"<p>The scenario is provided as part of our GPL-v3 package for VisionAI. If you wish to train this with custom datasets, please contact us and we can provide you with the training code. You can do custom training with your own datasets for free, as long as it complies with GPLv3 license (you give back the code to the community). If you are interested in a custom license, please (contact us)[contact.md].</p>"},{"location":"scenarios/no-smoking-hazard/#contact-us","title":"Contact Us","text":"<ul> <li>For technical issues, you can open a Github issue here.</li> <li>For business inquiries, you can contact us through our website.</li> </ul>"},{"location":"scenarios/no-smoking/","title":"No Smoking/No Vaping","text":"<p>No smoking &amp; No vaping zone enforcements with Vision AI.</p> <p> </p> Detection of Smoking event"},{"location":"scenarios/no-smoking/#overview","title":"Overview","text":"<p>Smoking and vaping are typically banned in workplaces like manufacturing plants, construction sites, warehouses, chemical plants, etc., an ideal 100% compliance rate can be challenging to achieve. However, it is imperative for employers to ensure that their workplaces are absolutely smoke-free.</p> <p>VisionAI makes it possible to avert workplace hazards and help employers maintain 100% compliance through smart AI solutions. Our next-gen real-time detection systems make sure a fire/smoke or any sign of vaping is detected instantly. These systems are also trained to generate alerts and notifications accordingly.</p>"},{"location":"scenarios/no-smoking/#vision-ai-based-monitoring","title":"Vision AI based monitoring","text":"<p>Vision AI based monitors can be used to detect smoking/vaping events by providing real-time video feeds of the factory area. The cameras scan every frame to ensure there is no sign of smoking/vaping.</p>"},{"location":"scenarios/no-smoking/#model-details","title":"Model Details","text":""},{"location":"scenarios/no-smoking/#dataset","title":"Dataset","text":"<p>The dataset for this scenario is based on real-world smoking/vaping events. The dataset consists of images and videos collected from various sources. </p>"},{"location":"scenarios/no-smoking/#model","title":"Model","text":"<p>The model to detect smoking/vaping event is in progress and it will be released soon.</p>"},{"location":"scenarios/no-smoking/#scenario-details","title":"Scenario details","text":"<p>The business logic for this scenario is as follows:</p> <ul> <li>We use existing camera feeds from the premises to detect smoking/vaping events.</li> <li>VisionAI system is able to run on edge devices. It uses camera feeds for processing.</li> <li>We detect people in the camera feed and we monitor whether the person is involved in any smoking/vaping activity.</li> <li>If the person is detected with this event, an alert is raised.</li> </ul> Test now with online Web-CamWith RTSP Camera - PipelinesWith Azure Setup <p>To test this model &amp; scenario, you can use the following steps:</p> <ul> <li> <p>Install the visionai package from PyPI</p> <pre><code>$ pip install visionai\n</code></pre> </li> <li> <p>Test the scenario from your local web-cam</p> <pre><code>$ visionai scenario test no-smoking-detection\n\nDownloading models for scenario: no-smoking-detection\nModel: no-smoking-detection: https://workplaceos.blob.core.windows.net/models/yolov5s-people/yolov5s-people-0.0.4.zip\nStarting scenario: no-smoking-detection..\n</code></pre> </li> <li> <p>You should be able to see the events generated on your console window with the detections of smoking/vaping within the camera field of view.</p> </li> </ul> <p>[TODO]</p> <p>VisionAI app is available at a Azure Market place, one can download and use it by following steps mentioned here</p>"},{"location":"scenarios/no-smoking/#features","title":"Features","text":"<p>The VisionAI solution is the most efficient way of implementing this scenario, as evidenced by the following features:</p> <ul> <li> <p>Unmatched accuracy</p> <p>Trained and Tested to give the best results. Our systems are trained to detect Fire and Smoke at the earliest detection with an accuracy of 99%</p> </li> <li> <p>Lightning Fast and Response Time</p> <p>Our Ultra-fast Processing provides real-time inference results and feedback (~30 frames per second processing). </p> </li> <li> <p>Minimizing false-positives/negatives</p> <p>Our systems create a fail-proof system by ensuring there are no false-positives or false-negatives. </p> </li> <li> <p>Scalability and Deployment </p> <p>Our models can be deployed instantly and are camera independent which means they can be pre-installed with existing cameras on site. We also offer cameras, IoT sensors and edge devices with strategic placement that helps scale a large workplace area with minimum installations. </p> </li> <li> <p>Custom Integrations</p> <p>Our detection system can be integrated with other safety systems, such as building management systems or alarm systems, allowing for a coordinated response to emergencies.</p> </li> </ul>"},{"location":"scenarios/no-smoking/#training-with-custom-data","title":"Training with custom data","text":"<p>The scenario is provided as part of our GPL-v3 package for VisionAI. If you wish to train this with custom datasets, please contact us and we can provide you with the training code. You can do custom training with your own datasets for free, as long as it complies with GPLv3 license (you give back the code to the community). If you are interested in a custom license, please (contact us)[contact.md].</p>"},{"location":"scenarios/no-smoking/#contact-us","title":"Contact Us","text":"<ul> <li>For technical issues, you can open a Github issue here.</li> <li>For business inquiries, you can contact us through our website.</li> </ul>"},{"location":"scenarios/no-taking-pictures/","title":"People Taking Picture Detection","text":"<p>Get real-time alerts when workers are taking pictures at the workplace, a monitoring system that can detect camera usage on company-owned devices or company networks.</p> <p></p>"},{"location":"scenarios/no-taking-pictures/#overview","title":"Overview","text":"<p>The usage of mobile phones at workplaces can have serious consequences. Other than the distraction they cause, these devices prove to be a powerful and insidious way to execute malicious purposes. Their small size and sophisticated features make them an effective tool for espionage. Many companies experience the loss or theft of sensitive data due to their employees' careless or intentional use of mobile devices. Detection of mobile phone usage in organizations can help companies avoid any harmful or haphazard use cases. Manual ways to detect mobile phone usage are neither cost-effective nor scalable.</p>"},{"location":"scenarios/no-taking-pictures/#vision-ai-based-monitoring","title":"Vision AI based monitoring","text":"<p>Vision AI based monitors can be used to safely detect mobile phone usage (people taking pictures, recording or streaming videos/audios, sending texts in prohibited areas) by providing real-time video feeds of the area. These cameras can be used to detect an event of workers taking pictures in prohibited areas.</p> <p>Enforce any company policies on picture-taking in the workplace. These camera based detection processes should be supplimented by strong compliance practices. If workers are prohibited from taking pictures, ensure that they are aware of the policy and the consequences of violating it.So, if a worker is found to be taking unauthorized pictures, appropriate action can be taken.</p>"},{"location":"scenarios/no-taking-pictures/#model-details","title":"Model Details","text":""},{"location":"scenarios/no-taking-pictures/#dataset","title":"Dataset","text":"<p>Model training is carried out with Microsoft COCO: Common Objects in Context dataset. Person class is considered for model building. </p> <p>Basically, COCO is a  large-scale dataset and it provides real-world data representation including:</p> <ul> <li>Indoor vs Outdoor environments</li> <li>Male vs Female</li> <li>Day vs Night</li> <li>Different types of clothing</li> <li>Different distances from the camera</li> <li>Various lighting conditions</li> <li>Various camera angles and resolutions</li> <li>Using seurity camera feeds</li> </ul>"},{"location":"scenarios/no-taking-pictures/#model","title":"Model","text":"<p>The model is built using Yolov5 pre-trained model for person and mobile classes. The yolov5 model is used to identify the human body landmarks of the subject. By considering the angle difference in the movement of body parts, people taking picture event is detected.</p> <p>The Yolov5 model provides the following metrics:</p> Model Name Precision Recall  mAP   PERSON DETECTION 84.0%  85.1%  81.5%  <p>and landmark detection model gives the following metrics:</p> Model Name Precision Recall  mAP   LANDMARK DETECTION 84.0%  72.8%  84.9%  <p>The model is light-weight enough to be run on any edge devices.</p>"},{"location":"scenarios/no-taking-pictures/#scenario-details","title":"Scenario details","text":"<p>The business logic for this scenario is as follows: - We use existing camera feeds from the premises to detect a worker taking picture in unauthorized areas . - VisionAI system is run at the edge. It uses the camera feeds for processing. - We detect the picture taking event and an alert is raised.</p>"},{"location":"scenarios/no-taking-pictures/#try-it-now","title":"Try it now","text":""},{"location":"scenarios/no-taking-pictures/#quick-method-using-your-local-web-cam","title":"Quick method - using your local web-cam","text":"<p>To test this model &amp; scenario, you can use the following steps:</p> <ul> <li>Install the visionai package from PyPI</li> </ul> <pre><code>$ pip install visionai\n---&gt; 100%\n</code></pre> <ul> <li>Test the scenario from your local web-cam</li> </ul> <pre><code>$ visionai scenario test people-taking-picture-detection\n\nDownloading models for scenario: people-taking-picture-detection\nModel: people-taking-picture-detection: https://workplaceos.blob.core.windows.net/models/yolov5s-people/yolov5s-people-0.0.4.zip\n---&gt; 100%\nStarting scenario: people-taking-picture-detection..\n</code></pre> <ul> <li>You should be able to see the events generated on your console window with people taking picture being detected within the camera field of view.</li> </ul>"},{"location":"scenarios/no-taking-pictures/#in-an-actual-environment","title":"In an actual environment","text":"<p>To use this scenario in an actual environment, you can follow these steps:</p> <ul> <li>Install the visionai package from PyPI</li> </ul> <pre><code>$ pip install visionai\n---&gt; 100%\n</code></pre> <ul> <li>Download the scenario</li> </ul> <pre><code>$ visionai scenario download people-taking-picture-detection\n\nDownloading models for scenario: ergonomics-detection\nModel: people-taking-picture-detection\nhttps://workplaceos.blob.core.windows.net/models/yolov5s-people/yolov5s-people-0.0.4.zip\n---&gt; 100%\n</code></pre> <ul> <li>Add the camera feed to the scenario</li> </ul> <pre><code>$ visionai camera add OFFICE-01 --url rtsp://192.168.0.1/stream1\n$ visionai camera OFFICE-01 add-scenario ergonomics-detection\n$ visionai run\n\nStarting scenario: people-taking-picture-detection..\n</code></pre> <ul> <li>You should be able to see the events generated on your console window with people taking picture being detected within the camera field of view.</li> </ul> <p>For more details visit VisionAI web applicaion.</p>"},{"location":"scenarios/no-taking-pictures/#training-with-custom-data","title":"Training with custom data","text":"<p>The scenario is provided as part of our GPL-v3 package for VisionAI. If you wish to train this with custom datasets, please contact us and we can provide you with the training code. You can do custom training with your own datasets for free, as long as it complies with GPLv3 license (you give back the code to the community). If you are interested in a custom license, please (contact us)[contact.md].</p>"},{"location":"scenarios/no-taking-pictures/#contact-us","title":"Contact Us","text":"<ul> <li>For technical issues, you can open a Github issue here.</li> <li>For business inquiries, you can contact us through our website.</li> </ul>"},{"location":"scenarios/noise-level-monitoring/","title":"Noise Level Monitoring","text":"<p>Measure the sound level in the environment and identify a noise source</p>"},{"location":"scenarios/noise-level-monitoring/#overview","title":"Overview","text":"<p>Environmental noise around work facilities can be a pain to the ears. Traffic, factories, construction, and recreational activities can generate these harmful and unwanted noises. Airports, power plants, shooting ranges, rock crushing, etc., are some other noise sources that can be heard from many kilometers away. </p> <p>Identifying the source is one of the biggest challenges in dealing with environment noise. Firstly, single-point noise measurement is insufficient to accurately locate the source or measure noise levels. Secondly, this requires a considerable amount of resources as one needs to record noise levels for extended time periods. Therefore, noise measuring instruments with advanced technology sensors can be a great tool for accurate noise identification and monitoring.</p>"},{"location":"scenarios/noise-level-monitoring/#vision-ai-based-monitoring","title":"Vision AI based monitoring","text":"<p>VisionAI's noise monitoring solution is integrated with sensors that can automatically validate sound sources from a distance. Our model can classify noise sources using a classification algorithm capable of learning a sophisticated noise source classifier for an arbitrary scenario just by using relevant annotated recordings as training material. </p> <p>All the extracted sound measurements are transmitted from the smart sensor to the cloud service for detailed analysis. The cloud service stores the data in the measurement database, and audio segments marked for later inspection are stored in your disk server. End-users can access the measurement data and analysis of the measurements through a web-based portal. Implement this solution to identify noise sources and prevent noise levels from exceeding a threshold value.</p>"},{"location":"scenarios/noise-level-monitoring/#model-details","title":"Model Details","text":""},{"location":"scenarios/noise-level-monitoring/#dataset","title":"Dataset","text":"<p>The dataset contains crucial data collected from various sources over a considerable period. Here are some of the critical dataset items:</p> <ul> <li> <p>Source classification: The dataset contains audio of sound sources. These data items will help you classify noise sources faster.</p> </li> <li> <p>Distance from the source: The sensors will be installed at multiple places from the source. This is crucial to verify the accuracy of the source and measure the noise level.</p> </li> <li> <p>Sound pressure level: Measure the loudest and distortion-free sound level (SPL) with the help of a vast dataset and by setting threshold values.</p> </li> <li> <p>Data visualization: The measurement data can be visualized in multiple ways: calendar heat-maps, graphs, and report tables are just a few of the types. </p> </li> </ul>"},{"location":"scenarios/noise-level-monitoring/#model","title":"Model","text":"<p>We would be releasing the model to monitor noise level events in Q2-2023.</p>"},{"location":"scenarios/noise-level-monitoring/#contact-us","title":"Contact Us","text":"<ul> <li>For technical issues, you can open a Github issue here.</li> <li>For business inquiries, you can contact us through our website.</li> </ul>"},{"location":"scenarios/obstructed-camera-view/","title":"Obstructed Camera View","text":"<p>Keep your camera view clear with our obstructed camera detection model.</p> <p>Detection of obstructed camera event</p>"},{"location":"scenarios/obstructed-camera-view/#overview","title":"Overview","text":"<p>The obstructed camera detection model uses computer vision techniques to analyze the camera's video feed and identify if there is any obstruction present in the camera's field of view. The model utilizes deep learning techniques to learn the features of an unobstructed camera view and detects the presence of obstructions by analyzing the changes in the image features.</p> <p>The obstructed camera detection model can operate in real-time, providing continuous monitoring of the camera's field of view. This model can detect various types of obstructions, including partial obstructions, and can provide an alert when an obstruction is detected.</p>"},{"location":"scenarios/obstructed-camera-view/#vision-ai-based-monitoring","title":"Vision AI based monitoring","text":"<p>The aim of an obstructed camera detection model is to detect whether a cameras field of view is obstructed or not. This model is designed to detect various types of obstructions such as fingers, tape, post-it notes, or other physical objects that might block the camera view. The obstructed camera detection model can be useful in different settings such as surveillance systems, video conferencing, or any other applications that require a clear and unobstructed camera view.</p> <p>In summary, the obstructed camera detection model is a computer vision-based algorithm that uses deep learning techniques to detect obstructions in a camera's field of view. This model can be useful in various settings, providing real-time monitoring of camera views and alerting when obstructions are present.</p>"},{"location":"scenarios/obstructed-camera-view/#model-details","title":"Model Details","text":""},{"location":"scenarios/obstructed-camera-view/#dataset","title":"Dataset","text":"<p>The dataset consists of images and videos collected from various sources. </p>"},{"location":"scenarios/obstructed-camera-view/#model","title":"Model","text":"<p>The model to detect obstructed camera event is in progress and it will be released soon. </p>"},{"location":"scenarios/obstructed-camera-view/#scenario-details","title":"Scenario details","text":"<p>The business logic for this scenario is as follows:</p> <ul> <li> <p>We use existing camera feeds from the premises to monitor whether the camera view is obstructed or not. We detect partial obstructions that may only partially block the camera view, such as fingers partially covering the lens. </p> </li> <li> <p>VisionAI system is able to run on edge devices. It uses camera feeds for processing.</p> </li> <li> <p>When an instance of obstructed camera is detected, an alert will be raised.</p> </li> </ul> Test now with online Web-CamWith RTSP Camera - PipelinesWith Azure Setup <p>To test this model &amp; scenario, you can use the following steps:</p> <ul> <li> <p>Install the visionai package from PyPI</p> <pre><code>$ pip install visionai\n</code></pre> </li> <li> <p>Test the scenario from your local web-cam</p> <pre><code>$ visionai scenario test obstructed-camera-detection\n\nDownloading models for scenario: obstructed-camera-detection\nModel: obstructed-camera-detection: https://workplaceos.blob.core.windows.net/models/yolov5s-obstructed-camera-detection/yolov5s-obstructed-camera-detection-0.0.1.zip\nStarting scenario: obstructed-camera-detection..\n</code></pre> </li> <li> <p>You should be able to see the events generated on your console window with the detections of obstructed cameras.</p> </li> </ul> <p>[TODO]</p> <p>VisionAI app is available at a Azure Market place, one can download and use it by following steps mentioned here</p>"},{"location":"scenarios/obstructed-camera-view/#features","title":"Features","text":"<p>Here are the features of the obstructed camera detection model:</p> <ul> <li> <p>Real-time monitoring: The model can monitor the camera's field of view in real-time, providing immediate detection of any obstructions.</p> </li> <li> <p>Detection of various obstructions: The model can detect various types of obstructions such as fingers, tape, post-it notes, or other physical objects that might block the camera view.</p> </li> <li> <p>High accuracy: The model uses deep learning techniques and can achieve high accuracy in detecting obstructions.</p> </li> <li> <p>Easy integration: The model can be integrated with different applications such as video conferencing, surveillance systems, or any other system that requires an unobstructed camera view.</p> </li> </ul>"},{"location":"scenarios/obstructed-camera-view/#training-with-custom-data","title":"Training with custom data","text":"<p>The scenario is provided as part of our GPL-v3 package for VisionAI. If you wish to train this with custom datasets, please contact us and we can provide you with the training code. You can do custom training with your own datasets for free, as long as it complies with GPLv3 license (you give back the code to the community). If you are interested in a custom license, please contact us.</p>"},{"location":"scenarios/obstructed-camera-view/#contact-us","title":"Contact Us","text":"<ul> <li>For technical issues, you can open a Github issue here.</li> <li>For business inquiries, you can contact us through our website.</li> </ul>"},{"location":"scenarios/occupancy-metrics/","title":"Occupancy Metrics","text":"<p>Track workplace Occupancy Metrics effortlessly.</p> <p>Occupancy metrics</p>"},{"location":"scenarios/occupancy-metrics/#overview","title":"Overview","text":"<p>Measuring occupancy metrics is crucial for businesses because of the valuable insights it offers. Such insights help optimize various aspects of a business operation, from customer experiences and marketing strategies to operational efficiencies. By accurately measuring and analyzing occupancy metrics, businesses can make data-driven decisions to improve other aspects of their business, like optimizing the physical layout and resource management. While there are existing systems such as sensors, Wi-Fi devices, and Surveillance, they all exhibit lack of accuracy, and manual intervention.</p>"},{"location":"scenarios/occupancy-metrics/#vision-ai-based-monitoring","title":"Vision AI based monitoring","text":"<p>Track workplace Occupancy Metrics effortlessly with Vision AI monitoring that provides real-time insights and analysis. Our ready-to-deploy model is designed to provide businesses and organizations with valuable insights such as:</p> <ul> <li> <p>Occupancy count: This measures the number of people or objects within a given space, such as a room or a parking lot.</p> </li> <li> <p>Dwell time: This measures the amount of time that people spend in a particular location, such as a store or a workspace.</p> </li> <li> <p>Traffic flow: This tracks the movement of people or objects through a space, enabling businesses to optimize their layouts for maximum efficiency.</p> </li> <li> <p>Occupancy density: This measures the number of people or objects per unit of space, indicating whether a particular area is overcrowded or underutilized.</p> </li> <li> <p>Heatmaps: These visualizations show the most crowded areas in a building or a room, helping businesses to optimize their layouts and improve customer flow.</p> </li> <li> <p>Social distancing compliance: This monitors and enforces social distancing guidelines by identifying people who are too close to each other and triggering an alert.</p> </li> <li> <p>Foot traffic: This tracks the number of people entering and exiting a building, store, or event, providing insights into customer behavior and trends.</p> </li> <li> <p>Queue management: This analyzes the length of lines and identifies bottlenecks, optimizing customer flow and reducing wait times in places like retail stores and theme parks.</p> </li> <li> <p>Occupancy rate: This measures the percentage of space that is occupied at a given time, providing insights into space utilization and capacity planning.</p> </li> <li> <p>Crowd density: This measures the number of people per unit of space, providing insights into areas that may be overcrowded or unsafe.</p> </li> <li> <p>Occupancy behavior: This tracks how people move and interact within a space, providing insights into how spaces are being used and how they can be optimized.</p> </li> <li> <p>Occupancy trends: This analyzes occupancy data over time, identifying patterns and trends that can inform business decisions such as staffing, marketing, and capacity planning.</p> </li> <li> <p>Occupancy alerts: This triggers alerts when occupancy levels exceed predetermined thresholds, enabling businesses to take proactive measures to manage crowds and ensure safety.</p> </li> </ul>"},{"location":"scenarios/occupancy-metrics/#model-details","title":"Model Details","text":""},{"location":"scenarios/occupancy-metrics/#dataset","title":"Dataset","text":"<p>The dataset consists of images and videos collected from diverse sources and is designed to reflect real-world scenarios. It is evenly distributed with:</p> <ul> <li>Images with different subjects: The dataset includes images of people or objects in a variety of positions and orientations within the space being monitored.</li> <li>Images with different lighting conditions: The dataset includes images captured in different lighting conditions, such as bright sunlight or dim lighting.</li> <li>Images captured at different times of the day: The dataset includes images captured at different times of day to capture the variations in occupancy patterns over time.</li> <li>Images with different camera angles: The dataset includes images captured from different camera angles, such as top-down, side-on, or at an angle.</li> <li>Images with different camera types: The dataset includes images captured with different camera resolutions and focal lengths to simulate real-world scenarios.</li> <li>Images with a variety of occlusions, such as people partially hidden behind obstacles or objects.</li> <li>Variation in subjects: The dataset includes images of people wearing different types of clothing, such as hats, coats, or backpacks.</li> <li>Images with different camera angles: The dataset includes images that are taken from different camera angles, such as top-down, side view, or angled view.</li> <li>Images with different environments: The dataset includes images that show different types of workspace infrastructures, environments and layout, such as standing desks, shared desks, or cubicles.</li> </ul>"},{"location":"scenarios/occupancy-metrics/#model","title":"Model","text":"<p>The model to track events of occupancy metrics is in progress and it will be released soon.</p>"},{"location":"scenarios/occupancy-metrics/#scenario-and-potential-deployment-area-details","title":"Scenario and Potential Deployment Area Details","text":"<ul> <li> <p>Retail stores: Retail stores can use occupancy metrics to optimize store layouts, improve customer flow, and reduce wait times.</p> </li> <li> <p>Office buildings: Office buildings can use occupancy metrics to optimize workspace layouts, improve traffic flow, and identify underutilized areas.</p> </li> <li> <p>Transportation hubs: Transportation hubs, such as airports and train stations, can use occupancy metrics to optimize passenger flow, reduce wait times, and improve safety.</p> </li> <li> <p>Stadiums and event venues: Stadiums and event venues can use occupancy metrics to optimize seating arrangements, improve traffic flow, and enhance the overall visitor experience.</p> </li> <li> <p>Healthcare facilities: Healthcare facilities can use occupancy metrics to optimize waiting areas, improve patient flow, and ensure compliance with social distancing guidelines.</p> </li> <li> <p>Public spaces: Public spaces, such as parks and city centers, can use occupancy metrics to optimize traffic flow, identify overcrowding, and improve safety.</p> </li> <li> <p>Manufacturing plants: Manufacturing plants can use occupancy metrics to optimize production lines, improve traffic flow, and identify bottlenecks.</p> </li> <li> <p>Parking lots: Parking lots can use occupancy metrics to optimize parking arrangements, reduce wait times, and improve safety.</p> </li> </ul> Test now with online Web-CamWith RTSP Camera - PipelinesWith Azure Setup <p>To test this model &amp; scenario, you can use the following steps:</p> <ul> <li> <p>Install the visionai package from PyPI</p> <pre><code>$ pip install visionai\n</code></pre> </li> <li> <p>Test the scenario from your local web-cam</p> <pre><code>$ visionai scenario test occupancy-metrics\n\nDownloading models for scenario: occupancy-metrics\nModel: occupancy-metrics: https://workplaceos.blob.core.windows.net/models/yolov5s-occupancy-metrics/yolov5s-occupancy-metrics-0.0.1.zip\nStarting scenario: occupancy-metrics..\n</code></pre> </li> <li> <p>You should be able to see the events generated on your console window with events of occupancy metrics within the camera field of view.</p> </li> </ul> <p>[TODO]</p> <p>VisionAI app is available at a Azure Market place, one can download and use it by following steps mentioned here</p>"},{"location":"scenarios/occupancy-metrics/#features","title":"Features","text":"<ul> <li> <p>Lightning Fast and Response Time: Ultra-fast Processing for real-time inference results and feedback (~30 frames per second processing) with customizable telemetry and inference results for your requirements.</p> </li> <li> <p>Scalability and Instant Deployment: Our pre-trained/custom models can be deployed instantly and are camera independent which means they can be pre-installed with existing cameras on site. </p> </li> <li> <p>Custom Integrations: Our custom smart dashboards and real-time alert/notification systems can be tailored to fit your specific needs be it simple dashboards or complex ERP integrations.</p> </li> <li> <p>Multiple channels for notifications: Employee Role-based notifications and alerts through different omni channels like emails, messages, custom alert systems, etc.</p> </li> <li> <p>Pre-Processing and Privacy by design: Our Pre-processing enhances Image quality before further analysis  While  maintaining data privacy by blurring out faces and other sensitive information present in a frame.</p> </li> <li> <p>Intelligent Insights: Our Active Continuous Learning creates by-products in the form of intelligent insights, analytics and insightful data that helps you optimize processes and increase efficiency..</p> </li> <li> <p>Hassle-free Data Access: Clients can access and manage data/insights/analytics from anywhere using Cloud services. Further, we create role-based authentication systems for access to data.</p> </li> </ul>"},{"location":"scenarios/occupancy-metrics/#training-with-custom-data","title":"Training with custom data","text":"<p>The scenario is provided as part of our GPL-v3 package for VisionAI. If you wish to train this with custom datasets, please contact us and we can provide you with the training code. You can do custom training with your own datasets for free, as long as it complies with GPLv3 license (you give back the code to the community). If you are interested in a custom license, please contact us.</p>"},{"location":"scenarios/occupancy-metrics/#contact-us","title":"Contact Us","text":"<ul> <li>For technical issues, you can open a Github issue here.</li> <li>For business inquiries, you can contact us through our website.</li> </ul>"},{"location":"scenarios/occupancy-policies/","title":"Occupancies Policies","text":"<p>Ensuring security and controlled access at the workplace is vital for organizations, but conventional surveillance and crowd management techniques are complex, expensive, and heavily reliant on human intervention. Furthermore, these systems are often unable to provide desired, foolproof results due to limitations, inaccuracies, and the inability to provide multiple metrics.</p> <p>One of the significant challenges with current systems for workplace crowd management is the constantly evolving workplace dynamics. For instance, frequent changes in access requirements can make it difficult to reconfigure one-time installation measures and create a tedious and time-consuming task. As a result, these systems may be limited in terms of flexibility, adaptability, and scalability, making them less effective in certain situations and unable to adapt to changing needs or circumstances. This challenge highlights the need for more dynamic and adaptable solutions to accommodate the evolving nature of workplaces.</p>"},{"location":"scenarios/occupancy-policies/#visionifys-workplace-safety-suite-for-occupancy-policies","title":"Visionify\u2019s Workplace Safety Suite for Occupancy Policies","text":"<p>Enhance the safety and intelligence of your workplace with our cutting-edge VisionAI suite designed for effective crowd management. Our Crowd Management suite offers a complete solution set that helps you regulate access, enforce security policy adherence and deter intruders effectively. Our fully automated and ready-to-deploy models enable real-time monitoring for the detection of unauthorized access attempts, ensuring that your facility is guarded 24/7. Instant alerts are sent to prevent security breaches before they happen. Our system guarantees reliable detection and can be seamlessly integrated with your existing camera infrastructure, making it easy to scale your system with just a few clicks.</p> <p>What\u2019s included in this suite:</p> <ul> <li>No food or drinks</li> <li>No phone, text, pictures</li> <li>No Smoking zones</li> <li>No children/visitors</li> <li>Waste Management</li> <li>Energy Conservation</li> <li>Restricted Areas</li> </ul>"},{"location":"scenarios/perimeter-control/","title":"Perimeter Control/Fence Intruder Detection","text":""},{"location":"scenarios/perimeter-control/#overview","title":"Overview","text":"<p>Perimeter protection is the first line in the defense to detect an intruder. For premise perimeter defence, doors, windows, vents, skylights, or any other opening to a building, are the most frequently used sensing points. The majority of alarm systems offer this level of security because more than 80% of break-ins take place through these openings. The primary benefit of perimeter security is its straightforward construction. The main drawback is that it just shields the apertures. </p> <p>Constraint - Perimeter security is pointless if the intruder breaks through a wall, enters through the ventilation system, or hangs around after closing. </p>"},{"location":"scenarios/perimeter-control/#vision-ai-based-monitoring","title":"Vision AI based monitoring","text":"<p>Vision AI based perimeter control can be used to detect intruders in the premises. The model uses a detection algorithm followed by computer vision techniques to detect intruders in images and videos. The model works in a way that it ensures that intruders are fully and effectively detected so that neccesary action can be taken to  prevent unauthorized access. </p>"},{"location":"scenarios/perimeter-control/#model","title":"Model","text":"<p>We would be releasing the model for perimeter control in Q2-2023.</p>"},{"location":"scenarios/perimeter-control/#contact-us","title":"Contact Us","text":"<ul> <li>For technical issues, you can open a Github issue here.</li> <li>For business inquiries, you can contact us through our website.</li> </ul>"},{"location":"scenarios/ppe-detection/","title":"PPE Detection","text":"<p>Prevent Workplace Injuries and Occupational Hazards with Vision AI</p>"},{"location":"scenarios/ppe-detection/#overview","title":"Overview","text":"<p>Personal protective equipment, or \"PPE,\" is the clothing worn to reduce exposure to risks that might result in significant workplace diseases and injuries. Contact with chemical, radioactive, physical, electrical, mechanical, or other job hazards may cause these wounds and illnesses. Items like gloves, safety goggles, shoes, earplugs or muffs, hard hats, respirators, coveralls, vests, and full-body suits are examples of personal protection equipment. Accidents and injuries due to employees not wearing PPE hold business owners/employers legally accountable. Apart from legal damages, accidents due to PPE negligence result in loss of time, reduced productivity, costly worker compensations, etc.</p> <ul> <li> <p>Workplace Fatalities are rising, and employers cannot afford to tolerate PPE negligence. According to the Bureau of Labor Statistics, there were 5,190 fatal work injuries recorded in the United States in 2021, an 8.9-percent increase from 4,764</p> </li> <li> <p>Speaking about one of the most injury-susceptible body parts - hands, OSHA reports that almost 70 percent of hand and arm injuries could be prevented with personal protective equipment, specifically safety gloves. Yet, 70 percent of workers don\u2019t wear hand protection, and 30 percent don\u2019t wear the right kind of glove for the task.</p> </li> <li> <p>If we consider one of the leading causes of workplace fatalities - Head Injuries a study by BLO found that 84% of head injuries at a worksite were due to the absence of safety helmets. A percentage which not small considering there were 68,170 head injuries in 2021</p> </li> </ul> <p>Therefore, complete compliance is necessary since even a brief lapse in usage can prove fatal. To monitor PPE compliance, cameras can be used.</p>"},{"location":"scenarios/ppe-detection/#vision-ai-based-monitoring","title":"Vision AI based monitoring","text":"<p>Vision AI based monitors can be used to monitor PPE compliance by providing real-time video feeds of the factory unit. The cameras scan every frame to ensure there's no PPE negligence, eliminating occupational hazards and serious injuries.</p> <p>To ensure accuracy and reliability, these camera-based monitoring services should be supplemented by strong compliance processes. Furthermore, workers working in different factory units should always be made aware of PPE compliance practices.</p>"},{"location":"scenarios/ppe-detection/#dataset","title":"Dataset","text":"<p>The dataset for this scenario is based on PPE detection algorithms that are currently in practice.</p> <p>The dataset is made up of images and videos gathered from various sources. The dataset has been catalogued to ensure real-world intricacies. It has an even distribution of:</p> <ul> <li>Different(indoor/outdoor) environments</li> <li>Male vs Female</li> <li>Variations in PPE suits</li> <li>Variations in gloves, helmet, goggles, safety-vest design</li> <li>Different light settings</li> <li>Variations in camera orientations</li> <li>Using security camera feeds</li> </ul> <p>Total number of images used was 207,300</p>"},{"location":"scenarios/ppe-detection/#model","title":"Model","text":"<p>The model is based off of the YOLOv5 algorithm. The model is trained on a custom dataset of images and videos. The model is trained based on the above dataset compiled by our team.</p> <p>The model provides the following metrics:</p> Precision  Recall  mAP  <p>The model is adaptable enough to run on any edge computing device.</p>"},{"location":"scenarios/ppe-detection/#scenario-details","title":"Scenario details","text":"<p>The business logic for this scenario is as follows:</p> <ul> <li>We use existing camera feeds from the premises to monitor the compliance of PPE in the workplace.</li> <li>VisionAI system is able to run on edge devices. It uses camera feeds for processing.</li> <li>We detect people in the camera feed and we monitor whether the person is wearing safety gloves, goggles, helmet, mask, safety-shoes and vest or not.</li> <li>If the person is detected without safety gloves, goggles, helmet, mask, safety-shoes and vest, an alert is raised.</li> </ul>"},{"location":"scenarios/ppe-detection/#events-supported","title":"Events Supported","text":"<p>This scenario supports the following event:</p> <ul> <li>PPE detected: This event is generated when a PPE including goggles, gloves, helmet, mask, safety-shoes, vest, etc. are detected in the camera feed.</li> </ul>"},{"location":"scenarios/ppe-detection/#features","title":"Features","text":"<ul> <li> <p>Real-time monitoring: The solution enables you to monitor PPE compliance events in real-time, using camera feeds from the premises.</p> </li> <li> <p>Alert system: The solution provides an alert system to notify the concerned authorities in case of PPE negligence.</p> </li> <li> <p>Customizable: The solution is customizable to suit your needs. You can customize the solution to monitor PPE compliance in your workplace.</p> </li> </ul>"},{"location":"scenarios/ppe-detection/#training-with-custom-data","title":"Training with custom data","text":"<p>The scenario is provided as part of our GPL-v3 package for VisionAI. If you wish to train this with custom datasets, please contact us and we can provide you with the training code. You can do custom training with your own datasets for free, as long as it complies with GPLv3 license (you give back the code to the community). If you are interested in a custom license, please contact us.</p>"},{"location":"scenarios/ppe-detection/#contact-us","title":"Contact Us","text":"<ul> <li>For technical issues, you can open a Github issue here.</li> <li>For business inquiries, you can contact us through our website.</li> </ul>"},{"location":"scenarios/rust-and-corrosion-hazard/","title":"Equipment monitoring","text":""},{"location":"scenarios/rust-and-corrosion-hazard/#rust-and-corrosion-detection","title":"Rust and Corrosion Detection","text":"<p>Ensure the safety of employees by inspecting machine equipment for the presence of rust/corrosion. </p> <p></p>"},{"location":"scenarios/rust-and-corrosion-hazard/#overview","title":"Overview","text":"<p>Visual inspection of industrial environments is a common requirement across heavy industries, such as transportation, construction, and shipbuilding, and typically requires qualified experts to perform the inspection. Inspection locations can often be remote or in adverse environments that put humans at risk, such as bridges, skyscrapers, and offshore oil rigs. </p> <p>Many of these industries deal with huge metal surfaces and harsh environments. A common problem across these industries is metal corrosion and rust. Although corrosion and rust are used interchangeably across different industries (we also use the terms interchangeably in this post), these two phenomena are different. </p> <p>Visionify\u2019s AI Vision Model for Rust/Corrosion Detection is designed to detect instances of rust/corrosion if any in machine parts, manufacturing equipments etc. </p>"},{"location":"scenarios/rust-and-corrosion-hazard/#events","title":"Events","text":"<p>VisionAI model's generated events would be:</p> <ul> <li>Rust or corrosion event detected</li> </ul>"},{"location":"scenarios/rust-and-corrosion-hazard/#model-details","title":"Model Details","text":""},{"location":"scenarios/rust-and-corrosion-hazard/#dataset","title":"Dataset","text":"<p>The dataset for this scenario is based on rust/corrosion detection algorithms. The dataset is made up of images and videos gathered from various sources where instances of rust were found. The dataset has been catalogued to ensure real-world situations. It has an even distribution of:</p> <ul> <li>Variations of pieces of equipment</li> <li>Different(indoor/outdoor) environments</li> <li>Different rust severity</li> <li>Variations in camera orientations</li> <li>Using security camera feeds</li> </ul> <p>Total number of images used was 5572.</p>"},{"location":"scenarios/rust-and-corrosion-hazard/#model","title":"Model","text":"<p>The model is based off of the YOLOv5 algorithm. The model is trained on a custom dataset of images and videos. The model is trained based on the above dataset compiled by our team.The model provides the following metrics:- </p> Precision Recall mAP  <p>The model is adaptable enough to run on any edge computing device.</p>"},{"location":"scenarios/rust-and-corrosion-hazard/#scenario-details","title":"Scenario details","text":"<p>The business logic for this scenario is as follows: </p> <ul> <li>We use existing camera feeds from the premises to monitor the equipments in the workplace. </li> <li>VisionAI system is able to run on edge devices. It uses camera feeds for processing. </li> <li>We detect instances of rust/corrosion if any in machine parts, manufacturing equipments.</li> </ul> Test now with online Web-CamWith RTSP Camera - PipelinesWith Azure Setup <p>To test this model &amp; scenario, you can use the following steps:</p> <ul> <li> <p>Install the visionai package from PyPI</p> <pre><code>$ pip install visionai\n</code></pre> </li> <li> <p>Test the scenario from your local web-cam</p> <pre><code>$ visionai scenario test rust-detection\n\nDownloading models for scenario: rust-detection\nModel: rust-detection: https://workplaceos.blob.core.windows.net/models/yolov5s-people/yolov5s-people-0.0.4.zip\nStarting scenario: rust-detection..\n</code></pre> </li> <li> <p>You should be able to see the events generated on your console window with the detections of smoking/vaping event within the camera field of view.</p> </li> </ul> <p>[TODO]</p> <p>VisionAI app is available at a Azure Market place, one can download and use it by following steps mentioned here</p>"},{"location":"scenarios/rust-and-corrosion-hazard/#training-with-custom-data","title":"Training with custom data","text":"<p>The scenario is provided as part of our GPL-v3 package for VisionAI. If you wish to train this with custom datasets, please contact us and we can provide you with the training code. You can do custom training with your own datasets for free, as long as it complies with GPLv3 license (you give back the code to the community). If you are interested in a custom license, please contact us.</p>"},{"location":"scenarios/rust-and-corrosion-hazard/#contact-us","title":"Contact Us","text":"<ul> <li>For technical issues, you can open a Github issue here.</li> <li>For business inquiries, you can contact us through our website.</li> </ul>"},{"location":"scenarios/rust-and-corrosion/","title":"Equipment monitoring","text":""},{"location":"scenarios/rust-and-corrosion/#rust-and-corrosion-detection","title":"Rust and Corrosion Detection","text":"<p>Ensure the safety of employees by inspecting machine equipment for the presence of rust/corrosion. </p> <p></p>"},{"location":"scenarios/rust-and-corrosion/#overview","title":"Overview","text":"<p>Visual inspection of industrial environments is a common requirement across heavy industries, such as transportation, construction, and shipbuilding, and typically requires qualified experts to perform the inspection. Inspection locations can often be remote or in adverse environments that put humans at risk, such as bridges, skyscrapers, and offshore oil rigs. </p> <p>Many of these industries deal with huge metal surfaces and harsh environments. A common problem across these industries is metal corrosion and rust. Although corrosion and rust are used interchangeably across different industries (we also use the terms interchangeably in this post), these two phenomena are different. </p> <p>Visionify\u2019s AI Vision Model for Rust/Corrosion Detection is designed to detect instances of rust/corrosion if any in machine parts, manufacturing equipments etc. </p>"},{"location":"scenarios/rust-and-corrosion/#events","title":"Events","text":"<p>VisionAI model's generated events would be:</p> <ul> <li>Rust or corrosion event detected</li> </ul>"},{"location":"scenarios/rust-and-corrosion/#model-details","title":"Model Details","text":""},{"location":"scenarios/rust-and-corrosion/#dataset","title":"Dataset","text":"<p>The dataset for this scenario is based on rust/corrosion detection algorithms. The dataset is made up of images and videos gathered from various sources where instances of rust were found. The dataset has been catalogued to ensure real-world situations. It has an even distribution of:</p> <ul> <li>Variations of pieces of equipment</li> <li>Different(indoor/outdoor) environments</li> <li>Different rust severity</li> <li>Variations in camera orientations</li> <li>Using security camera feeds</li> </ul> <p>Total number of images used was 5572.</p>"},{"location":"scenarios/rust-and-corrosion/#model","title":"Model","text":"<p>The model is based off of the YOLOv5 algorithm. The model is trained on a custom dataset of images and videos. The model is trained based on the above dataset compiled by our team.The model provides the following metrics:- </p> Precision Recall mAP  <p>The model is adaptable enough to run on any edge computing device.</p>"},{"location":"scenarios/rust-and-corrosion/#scenario-details","title":"Scenario details","text":"<p>The business logic for this scenario is as follows: </p> <ul> <li>We use existing camera feeds from the premises to monitor the equipments in the workplace. </li> <li>VisionAI system is able to run on edge devices. It uses camera feeds for processing. </li> <li>We detect instances of rust/corrosion if any in machine parts, manufacturing equipments.</li> </ul> Test now with online Web-CamWith RTSP Camera - PipelinesWith Azure Setup <p>To test this model &amp; scenario, you can use the following steps:</p> <ul> <li> <p>Install the visionai package from PyPI</p> <pre><code>$ pip install visionai\n</code></pre> </li> <li> <p>Test the scenario from your local web-cam</p> <pre><code>$ visionai scenario test rust-detection\n\nDownloading models for scenario: rust-detection\nModel: rust-detection: https://workplaceos.blob.core.windows.net/models/yolov5s-people/yolov5s-people-0.0.4.zip\nStarting scenario: rust-detection..\n</code></pre> </li> <li> <p>You should be able to see the events generated on your console window with the detections of smoking/vaping within the camera field of view.</p> </li> </ul> <p>[TODO]</p> <p>VisionAI app is available at a Azure Market place, one can download and use it by following steps mentioned here</p>"},{"location":"scenarios/rust-and-corrosion/#training-with-custom-data","title":"Training with custom data","text":"<p>The scenario is provided as part of our GPL-v3 package for VisionAI. If you wish to train this with custom datasets, please contact us and we can provide you with the training code. You can do custom training with your own datasets for free, as long as it complies with GPLv3 license (you give back the code to the community). If you are interested in a custom license, please contact us.</p>"},{"location":"scenarios/rust-and-corrosion/#contact-us","title":"Contact Us","text":"<ul> <li>For technical issues, you can open a Github issue here.</li> <li>For business inquiries, you can contact us through our website.</li> </ul>"},{"location":"scenarios/shipping-activity/","title":"Shipping Activity Detection","text":"<p>Stay vigilant even after hours with our advanced suspicious shipping activity solution.</p>"},{"location":"scenarios/shipping-activity/#overview","title":"Overview","text":"<p>Shipping activity detection refers to the use of technology to identify and monitor shipping activity that may be indicative of illicit activity. Shipping activity detection technologies may include sensors, cameras, and other monitoring systems that can detect and track shipping activity. Some of these technologies can be integrated with machine learning and artificial intelligence (AI) algorithms to analyze data and identify patterns of behavior that may be indicative of suspicious shipping activity.</p> <p>Shipping activity detection technologies can be used in a variety of settings, including ports, harbors, and other areas where shipping activity may occur. These technologies can help identify potential security threats, such as vessels that may be carrying weapons or engaging in suspicious activities.</p> <p>The Suspicious shipping activity detected from non-designated area and during after-hours model is an important tool to identify potential threats and take appropriate action to mitigate risks.</p>"},{"location":"scenarios/shipping-activity/#vision-ai-based-monitoring","title":"Vision AI based monitoring","text":"<p>VisionAI's shipping activity detection solutions can be used to for the detection of suspicious shipping activity events by providing real-time video feeds of the shipping area. The cameras scan every frame and raise an event when a suspicious entry detected from an usually closed location or during off-hours and/or for extended duration of time.</p> <p>Suspicious Shipping Activity Detection model is an important tool for helping to prevent fraudulent or criminal activity in the shipping industry, and it works in real time to help ensure that potentially suspicious activity is identified and addressed as quickly as possible.</p>"},{"location":"scenarios/shipping-activity/#model-details","title":"Model Details","text":""},{"location":"scenarios/shipping-activity/#dataset","title":"Dataset","text":"<p>The dataset of Suspicious shipping activity detected from non-designated area and during after-hours is a collection of data points that provide insights into potential illicit activities taking place in the shipping industry.  One key feature of this dataset is the inclusion of information on shipping activity outside of designated areas and during after-hours. These factors are often indicators of suspicious behavior, as they suggest that the vessel is attempting to avoid detection and operate outside of normal shipping patterns. By analyzing this data, security personnel can identify potential threats and take appropriate action to prevent harm.</p>"},{"location":"scenarios/shipping-activity/#model","title":"Model","text":"<p>The model to detect suspicious shipping activity events is in progress and it will be released soon.</p>"},{"location":"scenarios/shipping-activity/#scenario-details","title":"Scenario details","text":"<p>Our VisionAI solution for Suspicious shipping activity detection works in different scenarios.</p> <ul> <li> <p>The model works by continuously monitoring shipping data from various sources and then analyzes the data to identify patterns and anomalies that may be indicative of suspicious activity.</p> </li> <li> <p>The model may flag a shipment as suspicious if it originates from a non-designated area or if it is being shipped during after-hours.</p> </li> <li> <p>Once the model identifies a potentially suspicious shipment, it can trigger an alert to notify relevant personnel or authorities, who can then  investigate further and take appropriate action as needed.</p> </li> </ul> Test now with online Web-CamWith RTSP Camera - PipelinesWith Azure Setup <p>To test this model &amp; scenario, you can use the following steps:</p> <ul> <li> <p>Install the visionai package from PyPI</p> <pre><code>$ pip install visionai\n</code></pre> </li> <li> <p>Test the scenario from your local web-cam</p> <pre><code>$ visionai scenario test shipping-activity-detection\n\nDownloading models for scenario: shipping-activity-detection\nModel: shipping-activity-detection: https://workplaceos.blob.core.windows.net/models/yolov5s-shipping-activity-detection/yolov5s-shipping-activity-detection-0.0.1.zip\nStarting scenario: shipping-activity-detection..\n</code></pre> </li> <li> <p>You should be able to see the information generated on your console window with suspicious shipping activity detection events within the camera field of view.</p> </li> </ul> <p>[TODO]</p> <p>VisionAI app is available at a Azure Market place, one can download and use it by following steps mentioned here</p>"},{"location":"scenarios/shipping-activity/#features","title":"Features","text":"<ul> <li> <p>Real-time monitoring: The solution is designed to monitor shipping data in real-time, allowing for rapid detection and response to suspicious shipping activity. </p> </li> <li> <p>Alert system: The model is programmed to send alerts or notify the security personnel in case of any suspicious shipping activity.</p> </li> <li> <p>Easy to deploy: The solution can be deployed easily with minimal effort and can be integrated with the existing camera infrastructure.</p> </li> <li> <p>Customizable: The solution can be customized to meet the specific requirements of the organization.</p> </li> </ul>"},{"location":"scenarios/shipping-activity/#training-with-custom-data","title":"Training with custom data","text":"<p>The scenario is provided as part of our GPL-v3 package for VisionAI. If you wish to train this with custom datasets, please contact us and we can provide you with the training code. You can do custom training with your own datasets for free, as long as it complies with GPLv3 license (you give back the code to the community). If you are interested in a custom license, please contact us.</p>"},{"location":"scenarios/shipping-activity/#contact-us","title":"Contact Us","text":"<ul> <li>For technical issues, you can open a Github issue here.</li> <li>For business inquiries, you can contact us through our website.</li> </ul>"},{"location":"scenarios/skin-temperature/","title":"Temperature monitoring","text":"<p>Monitor the temperature of workers at your facility in real-time and alert if the temperature exceeds or falls a threshold value.</p>"},{"location":"scenarios/skin-temperature/#overview","title":"Overview","text":"<p>Temperature monitoring is a vital need for all organizations. From hand-held temperature screening devices to no-contact thermal imaging systems, companies use various tools to monitor the temperature of workers. </p> <p>While hand-held screening devices are suitable for one-to-one inspection but unsuitable for real-time monitoring, they don't provide accurate outcomes for long-distances (i.e. 2 to 3 meters). IR-powered thermal imaging systems offer the best solution for quick and precise workplace temperature monitoring.</p>"},{"location":"scenarios/skin-temperature/#ir-camera-based-monitoring-for-a-temperature-of-an-equipment","title":"IR camera based monitoring for a temperature of an equipment","text":"<p>Visionify's thermal temperature monitoring solution monitors the skin temperature of people within the work premises. It consists of three main components i.e., a thermal camera, a computer, and a temperature reference which is gained through real-time images and videos collected from different sources. </p> <p>The IR camera sensors read skin surface temperature and calculate an estimated core-body temperature. Suppose that temperature is above a particular range. In that case, an alarm can be triggered, and the alerts can be shared with the respective personnel for one-to-one inspection with a clinical thermometer.</p> <p>By installing this in your facility, you can ensure the following:</p> <ul> <li>Real-time person temperature monitoring</li> <li>Restricted access to certain rooms or areas</li> <li>Centralized monitoring of all cameras</li> </ul> <p> </p> Detection of Person temperature event"},{"location":"scenarios/skin-temperature/#events","title":"Events","text":"<p>VisionAI model's generated events would be:</p> <ul> <li>Person temperature exceeds limit</li> </ul> <p>It is recommended that any instance of person's temperature exceeding should be reported, especially if it is related to a fire safety system.</p> <p>An event data for a person temperature not maintaining its limit may include the following information:</p> <ul> <li> <p>Date and Time: The date and time when the temperature reading was taken and when the issue was first detected.</p> </li> <li> <p>Location: The specific location where the temperature reading was taken, such as a room or area within a factory.</p> </li> <li> <p>Device: The device that detected the temperature issue.      </p> </li> </ul>"},{"location":"scenarios/skin-temperature/#configuration","title":"Configuration","text":"<p>Here are the basic steps to set up an infrared (IR) camera for person temperature monitoring:</p> <ul> <li> <p>Choose a suitable location: The camera should be installed in a location where there is sufficient space for people to pass through without obstruction. The ideal location is at the entrance of a building or a room where people can be screened before entering.</p> </li> <li> <p>Set up the camera: The camera should be positioned so that it is at a level that is higher than the heads of the people being screened. The camera should also be angled downwards to capture the forehead region where the temperature is usually measured. </p> </li> <li> <p>Configure the camera settings: The camera settings should be adjusted to optimize the temperature measurement accuracy. </p> </li> <li> <p>Set up a screening process: A screening process should be established to ensure that everyone passing through the camera is screened. </p> </li> </ul> <p>Note</p> <p>It's important to note that the above steps are general guidelines, and the specific setup and procedures may vary depending on the type of camera and the intended use case. </p>"},{"location":"scenarios/skin-temperature/#model-details","title":"Model Details","text":"<p>The model to detect temperature exceeds or subceeds event is in progress and it will be released soon.</p>"},{"location":"scenarios/skin-temperature/#scenario-details","title":"Scenario details","text":"<p>When a person's temperature crosses the threshold value, the model can trigger an alarm, and the person can be sent for inspection. Internal email and text notifications in case of negative tests. Speed up temperature monitoring by leveraging facial recognition capabilities.</p> Test now with online Web-CamWith RTSP Camera - PipelinesWith Azure Setup <p>To test this model &amp; scenario, you can use the following steps:</p> <ul> <li> <p>Install the visionai package from PyPI</p> <pre><code>$ pip install visionai\n</code></pre> </li> <li> <p>Test the scenario from your local web-cam</p> <pre><code>$ visionai scenario test person-temp-detection\n\nDownloading models for scenario: person-temp-detection\nModel: person-temp-detection: https://workplaceos.blob.core.windows.net/models/yolov5s-people/yolov5s-people-0.0.4.zip\nStarting scenario: person-temp-detection..\n</code></pre> </li> <li> <p>You should be able to see the events generated on your console window with the detections of person temperature exceeding event within the camera field of view.</p> </li> </ul> <p>[TODO]</p> <p>VisionAI app is available at a Azure Market place, one can download and use it by following steps mentioned here</p>"},{"location":"scenarios/skin-temperature/#features","title":"Features","text":"<p>VisionAI's thermal camera based solution is suitable for person temperature monitoring. Thermal cameras detect infrared radiation emitted from an object or person and convert that radiation into a temperature reading, which can be used to measure a person's body temperature</p>"},{"location":"scenarios/skin-temperature/#training-with-custom-data","title":"Training with custom data","text":"<p>The scenario is provided as part of our GPL-v3 package for VisionAI. If you wish to train this with custom datasets, please contact us and we can provide you with the training code. You can do custom training with your own datasets for free, as long as it complies with GPLv3 license (you give back the code to the community). If you are interested in a custom license, please contact us.</p>"},{"location":"scenarios/skin-temperature/#contact-us","title":"Contact Us","text":"<ul> <li>For technical issues, you can open a Github issue here.</li> <li>For business inquiries, you can contact us through our website.</li> </ul>"},{"location":"scenarios/slip-and-fall-detection/","title":"Slip and Fall Detection","text":"<p>Ensure the safety of employees by inspecting slip and fall instances. Get real-time alerts when such kind of accidents occur at the workplace. </p> <p></p>"},{"location":"scenarios/slip-and-fall-detection/#overview","title":"Overview","text":"<p>Slip and Falls are among the leading causes of occupational fatalities. Timely action in the event of a Fall/Slip accident can minimize damage and save lives. However, working alone or in a noisy environment hinders timely assistance.  </p> <p>Here are some situations where a slip-fall detection system would be useful. </p> <ul> <li> <p>Construction sites: Detect slips and falls among workers and alert supervisors to potential hazards. Potential deployment zones include ladders, ramps, and scaffolds.\u202f </p> </li> <li> <p>Working at heights: Falls from higher to lower levels are the most common cause of fatalities in Slip and Fall accidents, making Slip and Fall detection for workers at height vital. </p> </li> <li> <p>Working Alone: Slip and Fall detection is crucial for specific job settings where employees are required to work alone, particularly during off-hours. Deployment areas could be for the following category of workers. </p> </li> <li> <p>Construction Sites: Electricians, plumbers, and HVAC (Heating, ventilation, and air conditioning) technicians work alone on specific tasks where slip and fall would be useful. </p> </li> <li> <p>Oil and gas: The system will be useful for remote location workers such as drill operators, pipeline inspectors, and pump operators working in oil and gas mining regions. </p> </li> <li> <p>Telecommunications: Workers such as tower climbers and cable technicians often work alone at high-rise towers and other elevated locations and their safety can be ensured by slip and detection system. </p> </li> <li> <p>Mining Industry: Workers such as underground miners and drill operators often work alone in remote and confined spaces. </p> </li> <li> <p>Maintenance: Building engineers, window cleaners, painters, facility maintenance workers, etc., often work alone during the night shift and in isolated places at heights.</p> </li> <li> <p>Noisy Environments: Like working alone, a noisy environment can hinder the process of quick response. Deployment areas could be for the following category of workers.  </p> </li> <li> <p>Construction: Heavy equipment operators like jackhammer operators at construction sites work in extremely noisy environments. </p> </li> <li> <p>Manufacturing: Workers in factories and assembly lines are exposed to a lot of noise due to the machinery and equipment used. </p> </li> <li> <p>Airports: Workers at airports, such as ground crew and baggage handlers, are often exposed to high noise levels from aircraft engines and other airport equipment. </p> </li> <li> <p>Elderly care facilities and Smart Homes: Detect slip and fall hazards in elderly care facilities. Such systems can also be deployed in homes to monitor the elderly or disabled, alerting caregivers or family members. </p> </li> </ul>"},{"location":"scenarios/slip-and-fall-detection/#vision-ai-based-monitoring","title":"Vision AI based monitoring","text":"<p>Vision AI-based system can be used to detect slip and fall with high accuracy. Additionally, our model trained on real-world images minimizes false-positives or false-negatives.  </p> <p>The cameras scan every frame to ensure there are no accidents related to slip and fall cases. </p> <p>To ensure accuracy and reliability for the model, these camera-based monitoring services should be supplemented by strong compliance processes. Furthermore, workers working in different factory units should always be made aware of these accidents and how to safeguard them. </p>"},{"location":"scenarios/slip-and-fall-detection/#model-details","title":"Model Details","text":""},{"location":"scenarios/slip-and-fall-detection/#dataset","title":"Dataset","text":"<p>Model training is carried out with Microsoft COCO: Common Objects in Context dataset. Only person class is considered for model building. COCO is a  large-scale dataset that addresses three core research problems in scene understanding: detecting non-iconic views (or non-canonical perspectives of objects), contextual reasoning between objects and the precise 2D localization of objects. </p> <p>COCO dataset has an even distribution of: </p> <ul> <li> <p>Different(indoor/outdoor) environments </p> </li> <li> <p>Male vs Female  </p> </li> <li> <p>Different light settings </p> </li> <li> <p>Variations in camera orientations </p> </li> <li> <p>Using security camera feeds </p> </li> </ul>"},{"location":"scenarios/slip-and-fall-detection/#model","title":"Model","text":"<p>The model is built using Yolov5 pre-trained model for detecting a person followed by a media pipe library used to estimate the pose of the person. The following performance metrics are recorded: </p> Model Name Precision Recall  mAP   SLIP AND FALL DETECTION 65.0%  71.6%  71.0%  <p>The model is adaptable enough to run on any edge computing device. </p>"},{"location":"scenarios/slip-and-fall-detection/#scenario-details","title":"Scenario details","text":"<p>The business logic for this scenario is as follows: </p> <ul> <li> <p>We use existing camera feeds from the premises to monitor and detect occurrences of slip and fall incidents. </p> </li> <li> <p>VisionAI system is able to run on edge devices. It uses camera feeds for processing. </p> </li> <li> <p>We detect human poses to identify slip and fall accidents in the camera feed. \u00a0</p> </li> <li>If either slip or fall is detected, an alert is raised.</li> </ul> Test now with online Web-CamWith RTSP Camera - PipelinesWith Azure Setup <p>To test this model &amp; scenario, you can use the following steps:</p> <ul> <li> <p>Install the visionai package from PyPI</p> <pre><code>$ pip install visionai\n</code></pre> </li> <li> <p>Test the scenario from your local web-cam</p> <pre><code>$ visionai scenario test slip-and-fall-detection\n\nDownloading models for scenario: slip-and-fall-detection\nModel: slip-and-fall-detection: https://workplaceos.blob.core.windows.net/models/yolov5s-slip-and-fall-detection/yolov5s-slip-and-fall-detection-0.0.1.zip\nStarting scenario: slip-and-fall-detection..\n</code></pre> </li> <li> <p>You should be able to see the events generated on your console window with slip and fall being detected within the camera field of view.</p> </li> </ul> <p>[TODO]</p> <p>VisionAI app is available at a Azure Market place, one can download and use it by following steps mentioned here</p>"},{"location":"scenarios/slip-and-fall-detection/#training-with-custom-data","title":"Training with custom data","text":"<p>The scenario is provided as part of our GPL-v3 package for VisionAI. If you wish to train this with custom datasets, please contact us and we can provide you with the training code. You can do custom training with your own datasets for free, as long as it complies with GPLv3 license (you give back the code to the community). If you are interested in a custom license, please (contact us)[contact.md].</p>"},{"location":"scenarios/slip-and-fall-detection/#contact-us","title":"Contact Us","text":"<ul> <li>For technical issues, you can open a Github issue here.</li> <li>For business inquiries, you can contact us through our website.</li> </ul>"},{"location":"scenarios/smoke-and-fire-detection/","title":"Early Fire Signs Detection","text":"<p>An intelligent Early Fire Signs Detection system aimed at safeguarding people and facilities</p> <p> </p> Detection of Smoke and Fire event"},{"location":"scenarios/smoke-and-fire-detection/#overview","title":"Overview","text":"<p>Fire can be one of the most catastrophic disasters that can happen anywhere and cause major destruction. Fire hazards exist in all types of industrial buildings and business environments. Fire incident in the workplace not only accounts for a large number of injuries but also for downtime and running costs to repair the damage to the premises and machinery. </p> <p>Conventional heat detector-based fire warning systems cannot detect an early fire. Heat detectors only alert when the temperature of the surrounding environment reaches a certain level, and it doesn\u2019t happen until fire spreads considerably, not leaving many opportunities to avoid the incident. Therefore, it is important to have an early fire signs detection method that would allow authorities to detect and put out fires before it goes out of control.  </p> <p>To monitor and detect early signs of fire at workplaces, cameras can be used. </p>"},{"location":"scenarios/smoke-and-fire-detection/#vision-ai-based-monitoring","title":"Vision AI-based monitoring","text":"<p>Vision AI-based Model for Early Fire Signs Detection is designed to spot early signs of smoke and fire and helps save lives and mitigate damages caused by industry fires. We aim to create safe workplaces by offering innovative, reliable, flexible, and scalable solutions. </p> <p>To ensure accuracy and reliability, these camera-based monitoring services should be supplemented by effective practices to ensure and prevent fire hazards. Furthermore, workers working in different factory units should always be made aware of fire signs to look for. </p>"},{"location":"scenarios/smoke-and-fire-detection/#model-details","title":"Model Details","text":""},{"location":"scenarios/smoke-and-fire-detection/#dataset","title":"Dataset","text":"<p>The dataset for this scenario is based on early fire detection algorithms that are currently in practice.  </p> <p>The dataset is made up of images and videos gathered from varied sources. The dataset has been designed to ensure real-world situations. It has an even distribution of: </p> <ul> <li>Different(indoor/outdoor) environments </li> <li>Variations in foregroung objects including persons, equipments etc </li> <li>Different lighting conditions</li> <li>Variations in weather conditions </li> <li>Using security camera feeds </li> <li>Multiple instances of fire and smoke </li> <li>Variations in camera orientations </li> <li>Classes considered for model building are smoke and fire</li> </ul> <p>Total number of images used was 63,055</p>"},{"location":"scenarios/smoke-and-fire-detection/#model","title":"Model","text":"<p>The model is based off of the YOLOv5 algorithm. The model is trained on a custom dataset of images and videos. The model is trained based on the above dataset compiled by our team. We intend to develop a model that generalizes well in real world situations. </p> <p>The model recorded the following performance metrics:</p> Precision Recall mAP  <p>The model is adaptable enough to run on any edge computing device.</p>"},{"location":"scenarios/smoke-and-fire-detection/#scenario-details","title":"Scenario details","text":"<p>The business logic for this scenario is as follows: </p> <ul> <li>We use existing camera feeds from the premises to monitor the early signs of fire in the workplace to ensure the safety of human lives in the workplace. </li> <li>VisionAI system is able to run on edge devices. It uses camera feeds for processing. </li> <li>We detect the presence of smoke and fire in the camera feed. </li> <li>An alarming system is inplace as part of early fire signs detection solution.</li> </ul>"},{"location":"scenarios/smoke-and-fire-detection/#try-it-now","title":"Try it now","text":""},{"location":"scenarios/smoke-and-fire-detection/#quick-method-using-your-local-web-cam","title":"Quick method - using your local web-cam","text":"<p>To test this model &amp; scenario, you can use the following steps:</p> <ul> <li>Install the visionai package from PyPI</li> </ul> <pre><code>$ pip install visionai\n---&gt; 100%\n</code></pre> <ul> <li>Test the scenario from your local web-cam</li> </ul> <pre><code>$ visionai scenario test smoke-and-fire-detection\n\nDownloading models for scenario: smoke-and-fire-detection\nModel: smoke-and-fire-detection: https://workplaceos.blob.core.windows.net/models/yolov5s-people/yolov5s-people-0.0.4.zip\n---&gt; 100%\nStarting scenario: smoke-and-fire-detection..\n</code></pre> <ul> <li>You should be able to see the events generated on your console window with smoke and fire being detected within the camera field of view.</li> </ul>"},{"location":"scenarios/smoke-and-fire-detection/#in-an-actual-environment","title":"In an actual environment","text":"<p>To use this scenario in an actual environment, you can follow these steps:</p> <ul> <li>Install the visionai package from PyPI</li> </ul> <pre><code>$ pip install visionai\n---&gt; 100%\n</code></pre> <ul> <li>Download the scenario</li> </ul> <pre><code>$ visionai scenario download smoke-and-fire-detection\n\nDownloading models for scenario: smoke-and-fire-detection\nModel: smoke-and-fire-detection\nhttps://workplaceos.blob.core.windows.net/models/yolov5s-people/yolov5s-people-0.0.4.zip\n---&gt; 100%\n</code></pre> <ul> <li>Add the camera feed to the scenario</li> </ul> <pre><code>$ visionai camera add OFFICE-01 --url rtsp://192.168.0.1/stream1\n$ visionai camera OFFICE-01 add-scenario smoke-and-fire-detection\n$ visionai run\n\nStarting scenario: smoke-and-fire-detection..\n</code></pre> <ul> <li>You should be able to see the events generated on your console window with smoke and fire being detected within the camera field of view.</li> </ul> <p>For more details visit VisionAI web application.</p>"},{"location":"scenarios/smoke-and-fire-detection/#features","title":"Features:","text":"<ul> <li> <p>Unparalleled Accuracy and faster detection</p> <p>VisionAI's Fire Signs detection model is capable of detecting fire incidents at an outstanding accuracy of up to 98% and a detection speed of 36 FPS.</p> </li> <li> <p>Flexible and Scalable </p> <p>VisionAI's Fire Signs detection is an end-to-end solution that integrates seamlessly with your existing camera network and is ready to detect. It can fit any building size and is easily expandable by adding more IP cameras to the network. </p> </li> <li> <p>Integrated Solution </p> <p>It is an integrated system combining surveillance and early fire signs detection in one system.</p> </li> <li> <p>Deployment Ready </p> <p>Our pre-trained AI models are ready for immediate industrial deployments</p> </li> <li> <p>Versatile Framework </p> <p>We offer flexibility in deployment; the model can operate at the Edge, in the cloud, or any self-hosted environment </p> </li> <li> <p>Compatible </p> <p>VisionAI's Early Fire Signs detection model has a broad potential and can be efficiently used for indoor and outdoor applications. </p> </li> <li> <p>Privacy Protection</p> <p>We understand your concerns about data privacy and take a proactive approach to preserve it. Our models are privacy oriented by design.</p> </li> </ul>"},{"location":"scenarios/smoke-and-fire-detection/#training-with-custom-data","title":"Training with custom data","text":"<p>The scenario is provided as part of our GPL-v3 package for VisionAI. If you wish to train this with custom datasets, please contact us and we can provide you with the training code. You can do custom training with your own datasets for free, as long as it complies with GPLv3 license (you give back the code to the community). If you are interested in a custom license, please (contact us)[contact.md].</p>"},{"location":"scenarios/smoke-and-fire-detection/#contact-us","title":"Contact Us","text":"<ul> <li>For technical issues, you can open a Github issue here.</li> <li>For business inquiries, you can contact us through our website.</li> </ul>"},{"location":"scenarios/social-distance/","title":"Social Distancing","text":"<p>Creating Safe Workplaces: Companies Implement Measures to Ensure Social Distancing in the Workplace with Vision AI.</p> <p> </p> Detection of Social distancing event"},{"location":"scenarios/social-distance/#overview","title":"Overview","text":"<p>Maintaining social distancing in workplaces and industries is crucial to prevent the spread of diseases including COVID-19 and protect the health and safety of employees or workers. </p> <p>In workplaces and industries, where employees or workers are often in close proximity to each other for extended periods, social distancing can help to reduce the spread of the virus. By keeping a safe distance from each other, employees or workers can avoid coming into contact with respiratory droplets and reduce the risk of infection.</p> <p>Overall, maintaining social distancing in workplaces and industries is an important part of a comprehensive approach to controlling the spread of certain diseases. By implementing social distancing measures and other best practices, employers can help to protect the health and safety of their employees or workers and prevent the spread of the virus.</p>"},{"location":"scenarios/social-distance/#vision-ai-based-monitoring","title":"Vision AI based monitoring","text":"<p>Vision AI based monitors can be used to push out events for different people observed in the frame and the distances between them by providing real-time video feeds of the factory area. The cameras scan every frame to ensure social distancing is well maintained.</p>"},{"location":"scenarios/social-distance/#events","title":"Events","text":"<p>VisionAI model's generated events would be: - Person distance events detected</p>"},{"location":"scenarios/social-distance/#configuration","title":"Configuration","text":"<p>It is recommended to set up cameras in ceiling view for monitoring social distancing guidelines.           </p>"},{"location":"scenarios/social-distance/#model-details","title":"Model Details","text":""},{"location":"scenarios/social-distance/#dataset","title":"Dataset","text":"<p>The dataset for this scenario is based on real-world social distancing events. The dataset consists of images and videos collected from various sources. </p>"},{"location":"scenarios/social-distance/#model","title":"Model","text":"<p>The model for social distancing event is in progress and it will be released soon.</p>"},{"location":"scenarios/social-distance/#scenario-details","title":"Scenario details","text":"<p>The business logic for this scenario is as follows:</p> <ul> <li>We use existing camera feeds from the premises and raise social distancing events.</li> <li>VisionAI system is able to run on edge devices. It uses camera feeds for processing.</li> <li>We detect people and the distance between them form the camera feed and raise a alert if social distancing is not maintained.</li> </ul> Test now with online Web-CamWith RTSP Camera - PipelinesWith Azure Setup <p>To test this model &amp; scenario, you can use the following steps:</p> <ul> <li> <p>Install the visionai package from PyPI</p> <pre><code>$ pip install visionai\n</code></pre> </li> <li> <p>Test the scenario from your local web-cam</p> <pre><code>$ visionai scenario test social-distancing\n\nDownloading models for scenario: social distancing\nStarting scenario: social distancing..\n</code></pre> </li> <li> <p>You should be able to see the events generated on your console window with the detections of social distancing within the camera field of view.</p> </li> </ul> <p>[TODO]</p> <p>VisionAI app is available at a Azure Market place, one can download and use it by following steps mentioned here</p>"},{"location":"scenarios/social-distance/#features","title":"Features","text":"<p>The VisionAI solution is the most efficient way of implementing this scenario, as evidenced by the following features:</p> <ul> <li> <p>Unmatched accuracy</p> <p>Trained and Tested to give the best results. Our systems are trained to detect social distancing with an accuracy of 99%</p> </li> <li> <p>Lightning Fast and Response Time</p> <p>Our Ultra-fast Processing provides real-time inference results and feedback (~30 frames per second processing). </p> </li> <li> <p>Minimizing false-positives/negatives</p> <p>Our systems create a fail-proof system by ensuring there are no false-positives or false-negatives. </p> </li> <li> <p>Scalability and Deployment </p> <p>Our pre-trained/custom models can be deployed instantly and are camera independent which means they can be pre-installed with existing cameras on site. We also offer cameras, IoT sensors and edge devices with strategic placement that helps scale a large workplace area with minimum installations. </p> </li> <li> <p>Custom Integrations</p> <p>Our detection system can be integrated with other safety systems, such as building management systems or alarm systems, allowing for a coordinated response to emergencies.</p> </li> </ul>"},{"location":"scenarios/social-distance/#training-with-custom-data","title":"Training with custom data","text":"<p>The scenario is provided as part of our GPL-v3 package for VisionAI. If you wish to train this with custom datasets, please contact us and we can provide you with the training code. You can do custom training with your own datasets for free, as long as it complies with GPLv3 license (you give back the code to the community). If you are interested in a custom license, please contact us.</p>"},{"location":"scenarios/social-distance/#contact-us","title":"Contact Us","text":"<ul> <li>For technical issues, you can open a Github issue here.</li> <li>For business inquiries, you can contact us through our website.</li> </ul>"},{"location":"scenarios/solicitation/","title":"Solicitation detection","text":"<p>A smarter way to unveil solicitation</p>"},{"location":"scenarios/solicitation/#overview","title":"Overview","text":"<p>Solicitation is the act of requesting or offering something in return for a favor, service, or product. In industries, Solicitation can take various forms, for example, employees soliciting other employees for money in exchange for job-related favors, any outsiders approaching factory employees or workers for different purposes like obtaining confidential information, for employment etc. Solicitation can also occur in public places, including malls, hotels, casinos, public transportation, clubs, etc. However, solicitation is often prohibited in these areas due to some specific rules and regulations depending upon the location and jurisdiction. </p> <p>To maintain a safe work environment, sustain an organization\u2019s values and prevent unethical behaviors within the companies and in public spaces, it is important to detect solicitation, take appropriate measures to address such behaviors and prevent it from happening. Computer Vision solutions can help effectively detect acts of solicitation before they occur.</p>"},{"location":"scenarios/solicitation/#vision-ai-based-monitoring","title":"Vision AI based monitoring","text":"<p>Implement VisionAI solution to address the problems associated with Solicitation by timely detection of any unethical behaviors in public spaces or companies. Our AI and deep learning-based solution can identify behavioral anomalies that may indicate solicitation.    </p> <p>Our smart solution seamlessly integrates with the existing camera infrastructure and analyzes the real-time video feed. It can help in different ways and offers a comprehensive solution for Solicitation;</p> <ul> <li>Facial recognition technology can help identify individuals involved in solicitation activities</li> <li>The algorithm can also analyze patterns of behaviors associated with solicitation, such as loitering or approaching strangers</li> <li>The algorithm also detects objects generally related to solicitation, like signs, posters, and flyers</li> <li>The algorithm can identify any unusual or suspicious behaviors in public spaces that indicate solicitation and help security and law enforcement personnel to respond more quickly to potential issues </li> </ul>"},{"location":"scenarios/solicitation/#model-details","title":"Model Details","text":"<p>The model to detect solicitation events is in progress and it will be released soon.</p>"},{"location":"scenarios/solicitation/#dataset","title":"Dataset","text":"<p>The dataset consists of images and videos in large numbers collected from diverse sources and is designed to reflect real-world scenarios. The dataset is representative of all types of people from different ages, genders and backgrounds engaging in solicitation behavior. Also, it includes all settings, situations and different types of solicitation behavior, such as advertising or sales pitches. It is evenly distributed with;</p> <ul> <li> <p>Different locations - in public places, there are various locations where solicitation can take place, like walkways, public parks, parking lots, public transportation hubs, malls, clubs, hotels, casinos, and retail environments.</p> </li> <li> <p>Different angles and perspectives - The dataset includes images or videos captured from different angles and lighting conditions to ensure that the model can detect solicitation behavior in various real-world scenarios.</p> </li> <li> <p>Different versions - The dataset undertakes different types of solicitation to ensure the model is robust enough and can generalize well to any new situation.</p> </li> </ul>"},{"location":"scenarios/solicitation/#model","title":"Model","text":"<p>The model to detect solicitation events is in progress and it will be released soon.</p>"},{"location":"scenarios/solicitation/#scenario-details","title":"Scenario details","text":"<p>Our VisionAI solution for solicitation detection works in different scenarios to detect any unethical behavior indicating solicitation within an industrial setting or in public spaces. The model is equipped to detect the following;</p> <ul> <li>Identify through facial recognition: known solicitors</li> <li>Identify a single person going and talking to multiple people</li> <li>Identify and track scantily clad persons and whether they are talking to people</li> <li>Identify similar patterns like one person repeating the same type of behavior</li> </ul> Test now with online Web-CamWith RTSP Camera - PipelinesWith Azure Setup <p>To test this model &amp; scenario, you can use the following steps:</p> <ul> <li> <p>Install the visionai package from PyPI</p> <pre><code>$ pip install visionai\n</code></pre> </li> <li> <p>Test the scenario from your local web-cam</p> <pre><code>$ visionai scenario test solicitation-detection\n\nDownloading models for scenario: solicitation-detection\nModel: solicitation-detection: https://workplaceos.blob.core.windows.net/models/yolov5s-solicitation-detection/yolov5s-solicitation-detection-0.0.1.zip\nStarting scenario: solicitation-detection..\n</code></pre> </li> <li> <p>You should be able to see the events generated on your console window with the detections of light sensor monitoring within the camera field of view.</p> </li> </ul> <p>[TODO]</p> <p>VisionAI app is available at a Azure Market place, one can download and use it by following steps mentioned here</p>"},{"location":"scenarios/solicitation/#features","title":"Features","text":"<ul> <li> <p>Real-time monitoring: The model can be deployed in real-time to monitor the public spaces and industrial settings for any unethical behavior indicating solicitation.</p> </li> <li> <p>Easy to deploy: The solution can be deployed easily with minimal effort and can be integrated with the existing camera infrastructure.</p> </li> <li> <p>Customizable: The solution can be customized to meet the specific requirements of the organization.</p> </li> </ul>"},{"location":"scenarios/solicitation/#training-with-custom-data","title":"Training with custom data","text":"<p>The scenario is provided as part of our GPL-v3 package for VisionAI. If you wish to train this with custom datasets, please contact us and we can provide you with the training code. You can do custom training with your own datasets for free, as long as it complies with GPLv3 license (you give back the code to the community). If you are interested in a custom license, please contact us.</p>"},{"location":"scenarios/solicitation/#contact-us","title":"Contact Us","text":"<ul> <li>For technical issues, you can open a Github issue here.</li> <li>For business inquiries, you can contact us through our website.</li> </ul>"},{"location":"scenarios/spills-and-leaks-hazard/","title":"Spills &amp; Leaks detection through VisionAI","text":"<p>Spills and Leaks detection through Vision AI.</p>"},{"location":"scenarios/spills-and-leaks-hazard/#overview","title":"Overview","text":"<p>Spills and leaks in industries can have significant health impacts on both humans and wildlife. The severity of the health impact depends on the type of substance that is spilled or leaked, the duration and extent of the exposure, and the vulnerability of the exposed population. Some potential health impacts of spills and leaks are Respiratory problems, skin irritation, Neurological effects, Cancer, Reproductive problems and Environmental impact.</p> <p>Preventing and mitigating spills and leaks is crucial for protecting the environment and human health. Existing solitions could be regular inspections and maintenance of equipment. Manual inspection is not foolproof and can be prone to errors and oversights. Human inspectors may miss small leaks or spills that may go undetected until they become larger and more severe.</p>"},{"location":"scenarios/spills-and-leaks-hazard/#vision-ai-based-monitoring","title":"Vision AI based monitoring","text":"<p>Manual inspections can be time-consuming and labor-intensive, which can make them impractical for large or complex industrial facilities.</p> <p>Vision AI-based model is designed to detect spills and leaks including water puddles, water leaks and slippery surfaces. The model can analyze images and video footage to identify visual anomalies, such as the appearance of a spill or leak, which can be missed by human inspectors.</p> Oil leak Water leak in pipes"},{"location":"scenarios/spills-and-leaks-hazard/#events","title":"Events","text":"<p>VisionAI model's generated events would be:</p> <ul> <li>Water puddle detected</li> <li>Water leak from equipment detected</li> <li>Spill event detected</li> <li>Slippery sign detected</li> </ul>"},{"location":"scenarios/spills-and-leaks-hazard/#model-details","title":"Model Details","text":""},{"location":"scenarios/spills-and-leaks-hazard/#dataset","title":"Dataset","text":"<p>Dataset for spills/leakages is properly curated and validated to ensure that the models are accurate and reliable. </p> <p>Some of the sources used to take images are:</p> <ul> <li>CAMEO Chemicals dataset</li> <li>The NOAA Hazardous Material Incident database</li> <li> <p>The Oil Spill Dataset</p> </li> <li> <p>The Pipeline and Hazardous Materials Safety Administration (PHMSA) dataset</p> </li> <li> <p>The Spill Impact Mitigation Assessment (SIMA) dataset</p> </li> </ul>"},{"location":"scenarios/spills-and-leaks-hazard/#model","title":"Model","text":"<p>The model to detect leak/spill event is in progress and it will be released soon.</p>"},{"location":"scenarios/spills-and-leaks-hazard/#scenario-details","title":"Scenario details","text":"<p>The business logic for this scenario is as follows: </p> <ul> <li>We use existing camera feeds from the premises to monitor the signs of leakage, spills in the workplace to ensure the safety of human lives in the workplace. </li> <li>VisionAI system is able to run on edge devices. It uses camera feeds for processing. </li> <li>We detect any kind of leakage in the camera feed.</li> <li>An alarming system is inplace as part of solution.</li> </ul> Test now with online Web-CamWith RTSP Camera - PipelinesWith Azure Setup <p>To test this model &amp; scenario, you can use the following steps:</p> <ul> <li> <p>Install the visionai package from PyPI</p> <pre><code>$ pip install visionai\n</code></pre> </li> <li> <p>Test the scenario from your local web-cam</p> <pre><code>$ visionai scenario test no-leak-detection\n\nDownloading models for scenario: no-smoking-detection\nModel: no-leak-detection: https://workplaceos.blob.core.windows.net/models/yolov5s-people/yolov5s-people-0.0.4.zip\nStarting scenario: no-leak-detection..\n</code></pre> </li> <li> <p>You should be able to see the events generated on your console window with the detections of spills and leak within the camera field of view.</p> </li> </ul> <p>[TODO]</p> <p>VisionAI app is available at a Azure Market place, one can download and use it by following steps mentioned here</p>"},{"location":"scenarios/spills-and-leaks-hazard/#features","title":"Features","text":"<p>VisionAI's Spill and leak detection  identifies and classifies spills and leaks in real-time. Here are some features of spill and leak detection:</p> <ul> <li> <p>Real-time monitoring: AI-based spill and leak detection systems can continuously monitor facilities and pipelines in real-time, allowing for quick detection and response times.</p> </li> <li> <p>Automated detection and alerts: AI-based systems can detect spills and leaks automatically and issue alerts to relevant personnel or systems, allowing for quick response and mitigation of the issue.</p> </li> <li> <p>Increased accuracy and reliability: VisionAI models can analyze large amounts of data quickly and accurately, allowing for the identification of even small leaks or spills that may be missed by human inspectors.</p> </li> <li> <p>Integration with other systems: VisionAI solution can be integrated with other systems such as alarm systems and spill response plans, allowing for a more comprehensive and effective response to spills and leaks.</p> </li> <li> <p>Predictive analytics: VisionAI models  can analyze historical data and patterns to identify potential risks and prevent future spills and leaks.</p> </li> <li> <p>Remote monitoring:  The system allows continuous monitoring of facilities and pipelines in remote or hard-to-reach areas.</p> </li> </ul> <p>Note</p> <p>Overall, spill and leak detection using our VisionAI's solution provides a powerful tool for industries to improve the accuracy, speed, and efficiency of spill and leak detection and response. The use of AI can also help to reduce the risk of human exposure to hazardous materials and prevent environmental damage caused by spills and leaks.</p>"},{"location":"scenarios/spills-and-leaks-hazard/#training-with-custom-data","title":"Training with custom data","text":"<p>The scenario is provided as part of our GPL-v3 package for VisionAI. If you wish to train this with custom datasets, please contact us and we can provide you with the training code. You can do custom training with your own datasets for free, as long as it complies with GPLv3 license (you give back the code to the community). If you are interested in a custom license, please (contact us)[contact.md].</p>"},{"location":"scenarios/spills-and-leaks-hazard/#contact-us","title":"Contact Us","text":"<ul> <li>For technical issues, you can open a Github issue here.</li> <li>For business inquiries, you can contact us through our website.</li> </ul>"},{"location":"scenarios/spills-and-leaks/","title":"Spills &amp; Leaks detection through VisionAI","text":"<p>Spills and Leaks detection through Vision AI.</p>"},{"location":"scenarios/spills-and-leaks/#overview","title":"Overview","text":"<p>Spills and leaks in industries can have significant health impacts on both humans and wildlife. The severity of the health impact depends on the type of substance that is spilled or leaked, the duration and extent of the exposure, and the vulnerability of the exposed population. Some potential health impacts of spills and leaks are Respiratory problems, skin irritation, Neurological effects, Cancer, Reproductive problems and Environmental impact.</p> <p>Preventing and mitigating spills and leaks is crucial for protecting the environment and human health. Existing solitions could be regular inspections and maintenance of equipment. Manual inspection is not foolproof and can be prone to errors and oversights. Human inspectors may miss small leaks or spills that may go undetected until they become larger and more severe.</p>"},{"location":"scenarios/spills-and-leaks/#vision-ai-based-monitoring","title":"Vision AI based monitoring","text":"<p>Manual inspections can be time-consuming and labor-intensive, which can make them impractical for large or complex industrial facilities.</p> <p>Vision AI-based model is designed to detect spills and leaks including water puddles, water leaks and slippery surfaces. The model can analyze images and video footage to identify visual anomalies, such as the appearance of a spill or leak, which can be missed by human inspectors.</p> Oil leak Water leak in pipes"},{"location":"scenarios/spills-and-leaks/#events","title":"Events","text":"<p>VisionAI model's generated events would be:</p> <ul> <li>Water puddle detected</li> <li>Water leak from equipment detected</li> <li>Spill event detected</li> <li>Slippery sign detected</li> </ul>"},{"location":"scenarios/spills-and-leaks/#model-details","title":"Model Details","text":""},{"location":"scenarios/spills-and-leaks/#dataset","title":"Dataset","text":"<p>Dataset for spills/leakages is properly curated and validated to ensure that the models are accurate and reliable. </p> <p>Some of the sources used to take images are:</p> <ul> <li>CAMEO Chemicals dataset</li> <li>The NOAA Hazardous Material Incident database</li> <li> <p>The Oil Spill Dataset</p> </li> <li> <p>The Pipeline and Hazardous Materials Safety Administration (PHMSA) dataset</p> </li> <li> <p>The Spill Impact Mitigation Assessment (SIMA) dataset</p> </li> </ul>"},{"location":"scenarios/spills-and-leaks/#model","title":"Model","text":"<p>The model to detect leak/spill event is in progress and it will be released soon.</p>"},{"location":"scenarios/spills-and-leaks/#scenario-details","title":"Scenario details","text":"<p>The business logic for this scenario is as follows: </p> <ul> <li>We use existing camera feeds from the premises to monitor the signs of leakage, spills in the workplace to ensure the safety of human lives in the workplace. </li> <li>VisionAI system is able to run on edge devices. It uses camera feeds for processing. </li> <li>We detect any kind of leakage in the camera feed.</li> <li>An alarming system is inplace as part of solution.</li> </ul> Test now with online Web-CamWith RTSP Camera - PipelinesWith Azure Setup <p>To test this model &amp; scenario, you can use the following steps:</p> <ul> <li> <p>Install the visionai package from PyPI</p> <pre><code>$ pip install visionai\n</code></pre> </li> <li> <p>Test the scenario from your local web-cam</p> <pre><code>$ visionai scenario test no-leak-detection\n\nDownloading models for scenario: no-smoking-detection\nModel: no-leak-detection: https://workplaceos.blob.core.windows.net/models/yolov5s-people/yolov5s-people-0.0.4.zip\nStarting scenario: no-leak-detection..\n</code></pre> </li> <li> <p>You should be able to see the events generated on your console window with the detections of spills and leak within the camera field of view.</p> </li> </ul> <p>[TODO]</p> <p>VisionAI app is available at a Azure Market place, one can download and use it by following steps mentioned here</p>"},{"location":"scenarios/spills-and-leaks/#features","title":"Features","text":"<p>VisionAI's Spill and leak detection  identifies and classifies spills and leaks in real-time. Here are some features of spill and leak detection:</p> <ul> <li> <p>Real-time monitoring: AI-based spill and leak detection systems can continuously monitor facilities and pipelines in real-time, allowing for quick detection and response times.</p> </li> <li> <p>Automated detection and alerts: AI-based systems can detect spills and leaks automatically and issue alerts to relevant personnel or systems, allowing for quick response and mitigation of the issue.</p> </li> <li> <p>Increased accuracy and reliability: VisionAI models can analyze large amounts of data quickly and accurately, allowing for the identification of even small leaks or spills that may be missed by human inspectors.</p> </li> <li> <p>Integration with other systems: VisionAI solution can be integrated with other systems such as alarm systems and spill response plans, allowing for a more comprehensive and effective response to spills and leaks.</p> </li> <li> <p>Predictive analytics: VisionAI models  can analyze historical data and patterns to identify potential risks and prevent future spills and leaks.</p> </li> <li> <p>Remote monitoring:  The system allows continuous monitoring of facilities and pipelines in remote or hard-to-reach areas.</p> </li> </ul> <p>Note</p> <p>Overall, spill and leak detection using our VisionAI's solution provides a powerful tool for industries to improve the accuracy, speed, and efficiency of spill and leak detection and response. The use of AI can also help to reduce the risk of human exposure to hazardous materials and prevent environmental damage caused by spills and leaks.</p>"},{"location":"scenarios/spills-and-leaks/#training-with-custom-data","title":"Training with custom data","text":"<p>The scenario is provided as part of our GPL-v3 package for VisionAI. If you wish to train this with custom datasets, please contact us and we can provide you with the training code. You can do custom training with your own datasets for free, as long as it complies with GPLv3 license (you give back the code to the community). If you are interested in a custom license, please (contact us)[contact.md].</p>"},{"location":"scenarios/spills-and-leaks/#contact-us","title":"Contact Us","text":"<ul> <li>For technical issues, you can open a Github issue here.</li> <li>For business inquiries, you can contact us through our website.</li> </ul>"},{"location":"scenarios/station-occupancy/","title":"Station Occupancy","text":"<p>Revolutionize your workspace with our Smart Desk Occupancy Tracker.</p>"},{"location":"scenarios/station-occupancy/#overview","title":"Overview","text":"<p>Tracking Workplace Metrics is key for identifying problems and driving growth. One such metric that organizations need to keep tabs on is desk occupancy. Tracking Desk Occupancy provides multiple valuable insights like worker productivity, worker behavioral analysis, floor planning, and utilization of space, all of which are required for workspace optimization and efficient resource management.</p> <p>Despite the increasing adoption of desk occupancy measurement across industries, present systems utilized to measure desk occupancy are fraught with several limitations, exhibit limited accuracy, lack the ability to provide multiple metrics, and can incur substantial installation costs.</p>"},{"location":"scenarios/station-occupancy/#vision-ai-based-monitoring","title":"Vision AI based monitoring","text":"<p>Introducing our fully automated Vision AI system for monitoring Desk Occupancy. Our next-gen AI models detect and count the presence of people within a specific area, whether they are performing a particular task or not, their dwell time, occupancy density and many more metrics. </p> <p>Our robust occupancy monitoring systems offer higher accuracy compared to current solutions, are cost-effective, and are capable of seamlessly integrating with existing cameras and infrastructure. With our system, there's no need to install multiple sensors or measurement devices, as a single camera can cover a wide area and enable users to easily leverage our AI-based real-time detection with minimal effort.</p> <p> </p> monitoring desk occupancy"},{"location":"scenarios/station-occupancy/#events","title":"Events","text":"<p>VisionAI model's generated events would be:</p> <ul> <li>Daily summary of occupancy metrics on a per desk/station basis</li> </ul> <p>It is recommended that any instance of an absence of a person from his/her desk be reported to the appropriate authority. An event data for desk occupancy scenario may include information such as:</p> <ul> <li>Date and time of the event</li> <li>Location of the event</li> </ul>"},{"location":"scenarios/station-occupancy/#configuration","title":"Configuration","text":"<p>It is recommended to set up camera in ceiling view to capture details about desk occupancy of the employees.</p>"},{"location":"scenarios/station-occupancy/#model-details","title":"Model Details","text":""},{"location":"scenarios/station-occupancy/#dataset","title":"Dataset","text":"<p>The dataset consists of images and videos collected from diverse sources and is designed to reflect real-world scenarios. It is evenly distributed with:</p> <ul> <li> <p>Positive images: The dataset includes images that contain people sitting at desks. These images should show a clear view of the desk and the person occupying it.</p> </li> <li> <p>Negative images: The dataset includes images that do not contain people sitting at desks. They could show empty desks or other objects in the workspace.</p> </li> <li> <p>Images with occlusions: The dataset includes images where the view of the person occupying the desk is partially obstructed, for example, by another object or person.</p> </li> <li> <p>Images with different lighting conditions: The dataset includes images that are taken under different lighting conditions, such as bright daylight, low-light, or artificial light.</p> </li> <li> <p>Images with different camera angles: The dataset includes images that are taken from different camera angles, such as top-down, side view, or angled view.</p> </li> <li> <p>Images with different desk layouts: The dataset includes images that show different types of desks, such as standing desks, shared desks, or cubicles.</p> </li> </ul>"},{"location":"scenarios/station-occupancy/#model","title":"Model","text":"<p>The model to monitor desk occupancy is in progress and it will be released soon.</p>"},{"location":"scenarios/station-occupancy/#scenario-details","title":"Scenario details","text":"<p>Real-time detection and alerts for different scenarios includes but are not limited to:</p> <ul> <li>When a person sits down at a desk that was previously unoccupied, the model can detect the change in occupancy.</li> <li>When a person gets up from a desk, the model can detect that the desk is now unoccupied.</li> <li>If the model detects an object on the desk that obstructs the view of the person occupying it, it may not be able to detect occupancy until the obstruction is removed.</li> <li>If the lighting conditions in the room change, the model may need to adjust its settings to continue accurately detecting occupancy.</li> <li>The model can also detect occupancy in real-time as people move around the workspace, allowing it to track changes in occupancy throughout the day.</li> </ul> Test now with online Web-CamWith RTSP Camera - PipelinesWith Azure Setup <p>To test this model &amp; scenario, you can use the following steps:</p> <ul> <li> <p>Install the visionai package from PyPI</p> <pre><code>$ pip install visionai\n</code></pre> </li> <li> <p>Test the scenario from your local web-cam</p> <pre><code>$ visionai scenario test desk-occupancy\n</code></pre> <p>Downloading models for scenario: desk-occupancy Model: miss-fire-exting-detection: https://workplaceos.blob.core.windows.net/models/yolov5s-people/yolov5s-people-0.0.4.zip</p> <p>Starting scenario: desk-occupancy..</p> <p>```</p> </li> <li> <p>You should be able to see the events generated on your console window with the detections of desk occupancy within the camera field of view.</p> </li> </ul> <p>[TODO]</p> <p>VisionAI app is available at a Azure Market place, one can download and use it by following steps mentioned here</p>"},{"location":"scenarios/station-occupancy/#features","title":"Features","text":"<p>Some potential features of VisionAI for monitoring desk occupancy could include:</p> <ul> <li> <p>Object Detection: This feature can help to monitor the occupancy of the desks and alert if a desk is occupied or not.</p> <ul> <li> <p>Heat Map: This feature can help to optimize the usage of the workspace and identify hotspots where there may be congestion.</p> </li> <li> <p>Occupancy Monitoring: This feature can help to optimize the usage of the workspace and ensure that all desks are being used efficiently.</p> </li> <li> <p>Desk Usage Patterns: This feature can help to optimize the usage of the workspace and identify areas that need improvement.</p> </li> <li> <p>Desk Reservation: This feature can help to optimize the usage of the workspace and ensure that all desks are being used efficiently.</p> </li> </ul> </li> </ul>"},{"location":"scenarios/station-occupancy/#training-with-custom-data","title":"Training with custom data","text":"<p>The scenario is provided as part of our GPL-v3 package for VisionAI. If you wish to train this with custom datasets, please contact us and we can provide you with the training code. You can do custom training with your own datasets for free, as long as it complies with GPLv3 license (you give back the code to the community). If you are interested in a custom license, please contact us.</p>"},{"location":"scenarios/station-occupancy/#contact-us","title":"Contact Us","text":"<ul> <li>For technical issues, you can open a Github issue here.</li> <li>For business inquiries, you can contact us through our website.</li> </ul>"},{"location":"scenarios/suspicious-activity/","title":"Suspicious Activity","text":""},{"location":"scenarios/suspicious-activity/#overview","title":"Overview","text":"<p>Suspicious activity detection refers to the process of identifying behavior or actions that deviate from the norm or expected patterns, and may indicate potential threats or risks. </p> <p>In security, suspicious activity detection can help identify potential threats or breaches in systems, networks, or physical environments. This can involve monitoring of access logs, network traffic, user behavior, or physical activity using video surveillance or other sensors.</p> <p>What\u2019s included in this suite:</p> <ul> <li>Loitering</li> <li>Unattended packages</li> <li>Aggressive behavior</li> <li>Vandalism &amp; property destruction</li> <li>Firearms &amp; knives</li> <li>Sexual harassments</li> <li>Solicitation</li> <li>Theft</li> <li>Shipping activity</li> <li>Intrusion detection</li> </ul>"},{"location":"scenarios/suspicious-package-detection/","title":"Suspicious Package Detection","text":"<p>Reliable and accurate Suspicious package detection for a safe and secure workplace environment</p>"},{"location":"scenarios/suspicious-package-detection/#overview","title":"Overview","text":"<p>Manual inspection of every package or parcel is time-consuming and can lead to delays in delivering important items. An automated detection model can quickly screen packages and prioritize those that require additional inspection. By detecting suspicious packages early, it may be possible to prevent an incident from occurring. This can save lives and minimize damage to property.</p> <p>Suspicious packages could contain hazardous materials such as explosives or chemicals, which could pose a significant risk to the safety of employees and the public. A detection model can quickly identify potential threats and allow for timely evacuation or other appropriate actions.</p> <p>Implementing a suspicious package detection model can enhance workplace safety and security, improve operational efficiency, and ensure compliance with legal requirements.</p>"},{"location":"scenarios/suspicious-package-detection/#vision-ai-based-monitoring","title":"Vision AI based monitoring","text":"<p>VisionAI suspicious package detection model is trained on a large dataset of known suspicious packages, as well as non-suspicious packages, to learn to recognize the characteristics that are most indicative of a threat.  VisionAI based suspicious package monitoring can be used to analyze new packages and determine whether they are suspicious or not. If a package is flagged as suspicious, security personnel can be alerted to investigate further and take appropriate action.</p>"},{"location":"scenarios/suspicious-package-detection/#model-details","title":"Model Details","text":""},{"location":"scenarios/suspicious-package-detection/#dataset","title":"Dataset","text":"<p>The dataset for this type of model typically consists of a large number of images or videos, captured from a variety of angles and under different lighting conditions. The images or videos may be collected from surveillance cameras or from other sources, such as social media posts or news reports.</p> <p>To ensure that the model is able to generalize to new and unseen images or videos, the dataset should include a diverse range of packages, with different sizes, shapes, colors, and markings. The dataset should also include examples of packages that are not suspicious or abandoned, in order to provide a balanced training set.</p>"},{"location":"scenarios/suspicious-package-detection/#model","title":"Model","text":"<p>The model to detect agressive behaviour events is in progress and it will be released soon.</p>"},{"location":"scenarios/suspicious-package-detection/#scenario-details","title":"Scenario details","text":"<p>The business logic for this scenario is as follows:</p> <ul> <li>We use existing camera feeds from the premises to monitor suspicious packages within the camera field of view.</li> <li>The model is able to detect suspicious packages and an alert system is in place to notify the appropriate authorities in the event that a suspicious package is detected. It is designed to minimize false alarms and provide timely and accurate information.</li> </ul> Test now with online Web-CamWith RTSP Camera - PipelinesWith Azure Setup <p>To test this model &amp; scenario, you can use the following steps:</p> <ul> <li> <p>Install the visionai package from PyPI</p> <pre><code>$ pip install visionai\n</code></pre> </li> <li> <p>Test the scenario from your local web-cam</p> <pre><code>$ visionai scenario test suspicious-package-detection\n\nDownloading models for scenario: suspicious-package-detection\nModel: suspicious-package-detection: https://workplaceos.blob.core.windows.net/models/yolov5s-suspicious-package-detection/yolov5s-suspicious-package-detection-0.0.1.zip\nStarting scenario: suspicious-package-detection..\n</code></pre> </li> <li> <p>You should be able to see the events generated on your console window with the detections of aggressive behavior within the camera field of view.</p> </li> </ul> <p>[TODO]</p> <p>VisionAI app is available at a Azure Market place, one can download and use it by following steps mentioned here</p>"},{"location":"scenarios/suspicious-package-detection/#features","title":"Features","text":"<ul> <li> <p>Real-time monitoring: The model is able to analyze data in real-time to detect potential suspicious packages.</p> </li> <li> <p>Integration with other systems: The model is able to integrate with other security systems, such as access control systems, to provide a comprehensive approach to package security.</p> </li> <li> <p>Alert system: The model is having an alert system that can notify the appropriate authorities in the event that a suspicious package is detected. It is designed to minimize false alarms and provide timely and accurate information.</p> </li> </ul>"},{"location":"scenarios/suspicious-package-detection/#training-with-custom-data","title":"Training with custom data","text":"<p>The scenario is provided as part of our GPL-v3 package for VisionAI. If you wish to train this with custom datasets, please contact us and we can provide you with the training code. You can do custom training with your own datasets for free, as long as it complies with GPLv3 license (you give back the code to the community). If you are interested in a custom license, please contact us.</p>"},{"location":"scenarios/suspicious-package-detection/#contact-us","title":"Contact Us","text":"<ul> <li>For technical issues, you can open a Github issue here.</li> <li>For business inquiries, you can contact us through our website.</li> </ul>"},{"location":"scenarios/test/","title":"Test","text":"Status Scenario name Details Additional considerations \u2705 <code>face-blurring</code> Blur any faces detected More details \u2705 <code>text-blurring</code> Blue any text detected (paper, computer screens etc) More details \u2705 <code>license-plate-blurring</code> Blur any license plates detected More details \u2705 <code>signs-blurring</code> Blur any signs detected More details \u2705 <code>obstructed-camera</code> If camera feed is obstructed, send an alert More details"},{"location":"scenarios/unauthorized-entry/","title":"Unauthorized Entry/Tailgating","text":"<p>Real-time alerts for any unauthorized entry, anywhere.</p>"},{"location":"scenarios/unauthorized-entry/#overview","title":"Overview","text":"<p>Tailgating or piggybacking is the entry or exit of more people, things, or vehicles than are permitted by access control rules into or out of a controlled area or through a controlled access gateway. Tailgating is one of the simplest forms of a social engineering attack. Some examples of tailgating are: - A tailgating event occurs when persons, generally on foot or in a vehicle, attempt to gain access to an area for which they do not have the required credentials. </p> <ul> <li> <p>A person without the necessary access credentials tries to follow another individual (again on foot or in a vehicle) into a controlled access location.</p> </li> <li> <p>Piggybacking - a person sits on the shoulders of another person or is carried in some other way by the other person into the controlled access area.</p> </li> <li> <p>Reverse entry - When someone tries to enter through a \"exit only\" access point on foot or in a car, a corresponding issue occurs. Reverse entrance is a term that can be used to describe this improper use of an exit portal.</p> </li> </ul>"},{"location":"scenarios/unauthorized-entry/#vision-ai-based-monitoring","title":"Vision AI based monitoring","text":"<ul> <li>Acquiring one or more stereoscopic images of an area of observation from client, wherein objects in the area of observation are characterized by:<ol> <li>An object type and </li> <li>An authorization status indicating whether the object is authorized with respect to the controlled access area.</li> </ol> </li> <li>Analyzing the one or more images using a machine vision processing system to identify a first object in the area of observation and to classify the first object in a first object type among a plurality of object types that are pre-defined in the machine vision processing system, said first object having supplied an authorization with respect to the controlled access area, and further using the machine vision processing system to identify a second object in the area of observation and to classify the second object in a second object type among the plurality of pre-defined object types, wherein the authorization status of each of the objects is determined separately from classifying the object in an object type; and</li> <li>Applying one or more access control rules to the information obtained from the image analysis to determine whether the second object is attempting to breach the controlled access area by utilizing the authorization supplied by the first object in violation thereof, wherein the controlled access area is limited to objects that are classified in a defined object type and have a status that is authorized with respect to the controlled access area, and wherein the one or more access control rules determine whether the second object is attempting to breach the controlled access area by separately determining:<ol> <li>Whether the classification of the second object is in the defined object type and </li> <li>Whether the second object has a status that is authorized with respect to the controlled access area. VisionAI based solution is focused on improving the performance of \"People object type detector\" using the YoloV3 Object detection model along with HaarCascades , for locating facial features/face identification by tuning parameters like Learning Rate, IoU, Momentum and identifying the best freezing layer.</li> </ol> </li> </ul>"},{"location":"scenarios/unauthorized-entry/#model-details","title":"Model Details","text":""},{"location":"scenarios/unauthorized-entry/#dataset","title":"Dataset","text":"<p>We have considered the following datasets to build unauthorized entry detection model:</p> <ul> <li>PETS2009 (Person Evaluation of Tracking and Surveillance)</li> <li>AVSS </li> <li>CLEAR</li> <li>NIST TRECVID (2021)</li> <li>CROWD11</li> <li>iLIDS</li> </ul> <p>All these datasets that are mainly focused on:</p> <ul> <li>People tracking</li> <li>Video analytics evaluation</li> <li>Loitering detection</li> <li>Crowd counting</li> <li>Attendance based evaluation</li> <li>Person re-identification</li> </ul>"},{"location":"scenarios/unauthorized-entry/#model","title":"Model","text":"<p>The model to perform tailgating detection is in progress and it will be released soon.</p>"},{"location":"scenarios/unauthorized-entry/#scenario-details","title":"Scenario details","text":"<p>The business logic for this scenario is as follows:</p> <ul> <li> <p>We use existing camera feeds from the premises to monitor unauthorized entry events.</p> </li> <li> <p>VisionAI system is able to run on edge devices. It uses camera feeds for processing.</p> </li> <li> <p>Along with camera feed the model uses various sensors and detectors to monitor the entry points of a secured area and once the sensors detects any motion or activity, then the signals are processed by the algorithm to determine whether they represent a potential unauthorized entry or not.</p> </li> </ul> Test now with online Web-CamWith RTSP Camera - PipelinesWith Azure Setup <p>To test this model &amp; scenario, you can use the following steps:</p> <ul> <li> <p>Install the visionai package from PyPI</p> <pre><code>$ pip install visionai\n</code></pre> </li> <li> <p>Test the scenario from your local web-cam</p> <pre><code>$ visionai scenario test unauthorized-entry-detection\n\nDownloading models for scenario: unauthorized-entry-detection\nModel: unauthorized-entry-detection: https://workplaceos.blob.core.windows.net/models/yolov5s-unauthorized-entry-detection/yolov5s-unauthorized-entry-detection-0.0.1.zip\nStarting scenario: unauthorized-entry-detection..\n</code></pre> </li> <li> <p>You should be able to see the events generated on your console window with detection of unauthorized entry within the camera field of view.</p> </li> </ul> <p>[TODO]</p> <p>VisionAI app is available at a Azure Market place, one can download and use it by following steps mentioned here</p>"},{"location":"scenarios/unauthorized-entry/#features","title":"Features:","text":"<p>The VisionAI solution is the most efficient way of implementing this scenario, as evidenced by the following features:</p> <ul> <li> <p>Real-time monitoring: The solution can be used to monitor the premises in real-time and raise alerts when an unauthorized entry or tailgating is detected.</p> </li> <li> <p>Customization: The model is customizable and can be trained with custom datasets to suit your specific needs.</p> </li> <li> <p>Integration: The solution can be integrated with existing camera infrastructure systems effortlessly.</p> </li> </ul>"},{"location":"scenarios/unauthorized-entry/#training-with-custom-data","title":"Training with custom data","text":"<p>The scenario is provided as part of our GPL-v3 package for VisionAI. If you wish to train this with custom datasets, please contact us and we can provide you with the training code. You can do custom training with your own datasets for free, as long as it complies with GPLv3 license (you give back the code to the community). If you are interested in a custom license, please contact us.</p>"},{"location":"scenarios/unauthorized-entry/#contact-us","title":"Contact Us","text":"<ul> <li>For technical issues, you can open a Github issue here.</li> <li>For business inquiries, you can contact us through our website.</li> </ul>"},{"location":"scenarios/vandalism/","title":"Vandalism and property destruction","text":"<p>Safeguard your assets with our advanced vandalism detection model.</p>"},{"location":"scenarios/vandalism/#overview","title":"Overview","text":"<p>Vandalism and property destruction can have serious consequences, both for individuals and for society as a whole. For example, it can lead to physical harm, emotional distress, financial losses, and damage to public infrastructure. By developing a model that can accurately detect and predict incidents of vandalism and property destruction, we can take proactive measures to prevent them from occurring or minimize their impact if they do occur. This can include increasing surveillance, enhancing security measures, and improving emergency response protocols. Ultimately, a vandalism and property destruction model can help protect people and property, reduce costs associated with damage, and promote a safer and more secure society. We need a vandalism and property destruction model to help prevent and mitigate damage caused by these types of incidents. </p> <p>Technically, vandalism is described as a video event that is instantiated by a video object that inflicts temporally consistent static changes (such as damage) inside a preset restricted region that is purportedly left unaltered by normal (i.e., legal) interaction with video objects.</p>"},{"location":"scenarios/vandalism/#visionai-based-monitoring","title":"VisionAI Based Monitoring","text":""},{"location":"scenarios/vandalism/#vandalism","title":"Vandalism","text":"<p>VisionAI based Monitoring is an effective approach to investigate if the site has temporally  consistent and significant static changes, indicative of damage, when an object is detected departing such a place. A vandalism event is declared and the vandals are located if there are such changes and given that the site is typically unaltered following legal use. The proposed method has a 96% detection rate when applied to video clips of actual and simulated vandalism in action. It recognises several types of vandalism, including theft and graffiti, and it can deal with abrupt illumination changes, occlusions, and segmentation mistakes. The frame rate of the suggested approach is 13 frames per second.</p>"},{"location":"scenarios/vandalism/#constraints","title":"Constraints","text":"<p>The automatic detection of vandalism in video surveillance is a challenging task because of: - The complex and unpredictable nature of a vandalism act and the speed at which it may occur - The underlying difficulty of finding a unique definition for vandalism which may vary based on social contexts and applications - The difficulty in distinguishing between normal and vandal interaction between persons and vandalism-prone objects or sites and - The lack of real vandalism test video sequences publicly available for training or testing.</p>"},{"location":"scenarios/vandalism/#proposed-method-for-detection-of-vandalism","title":"Proposed Method for detection of vandalism","text":"<p>A video object refers to a temporally consistent region (over a short period)in a video sequence. Video objects have spatio-temporal features such as contour,area, motion, and trajectory. For example, a video object has a unique identifier (ID) maintained by the tracking algorithm during the life-time of an object in the videosequence. A video event is an interpreted spatio-temporal relationship associating one or multiple objects (e.g., moving, staying long and is inside). Video events have information associated with them such as the IDs of the video objects involved in the event, the time at which it is detected, and its duration which is the number of consecutive frames the event is detected.We only consider rigid vandalism-prone objects that do not change over time.This includes pay-phones, vending machines, and paying stations in parking lots.</p> <p>For example, vandalism of electronic street signs switching content periodically is not considered. Also, we expect that the vandalism act alters the normal appearance of objects. Meaning, after the site is vandalized, there is visible damage (i.e., change) to the site. We use video object segmentation and ID tracking.</p>"},{"location":"scenarios/vandalism/#graffiti","title":"Graffiti","text":"<p>Graffiti can have a negative effect on a community's property value and tourism. Moreover, it may cause a decline in retail sales and an increase in public dread, both of which might drain tax funds intended for prevention. </p> <p>The Graffiti Image classifier can help law enforcement more effectively recognise Graffiti Images on the streets in order to lessen damage.</p> <p>VisionAI based solution is focused on improving the performance of Graffiti \u201cclassifier\u201d using the ResNet50 neural network by tuning parameters like Learning Rate, Batch Size and identifying the best freezing layer.</p>"},{"location":"scenarios/vandalism/#model-details","title":"Model Details","text":""},{"location":"scenarios/vandalism/#dataset","title":"Dataset","text":"<p>The dataset consists of images and videos collected from various sources. </p>"},{"location":"scenarios/vandalism/#model","title":"Model","text":"<p>The model to perform graffiti-detection is in progress and it will be released soon.</p>"},{"location":"scenarios/vandalism/#scenario-details","title":"Scenario details","text":"<p>The business logic for this scenario is as follows:</p> <ul> <li> <p>We use existing camera feeds from the premises to monitor an area or property in real-time, detecting any instances of vandalism or destruction as they occur.</p> </li> <li> <p>VisionAI system is able to run on edge devices. It uses camera feeds for processing.</p> </li> <li> <p>When instances of vandalism or destruction is detected, an alert will be raised.</p> </li> </ul> Test now with online Web-CamWith RTSP Camera - PipelinesWith Azure Setup <p>To test this model &amp; scenario, you can use the following steps:</p> <ul> <li> <p>Install the visionai package from PyPI</p> <pre><code>$ pip install visionai\n</code></pre> </li> <li> <p>Test the scenario from your local web-cam</p> <pre><code>$ visionai scenario test vandalism-graffiti-detection\n\nDownloading models for scenario: vandalism-graffiti-detection\nModel: vandalism-graffiti-detection: https://workplaceos.blob.core.windows.net/models/yolov5s-vandalism-graffiti-detection/yolov5s-vandalism-graffiti-detection-0.0.1.zip\nStarting scenario: vandalism-graffiti-detection..\n</code></pre> </li> <li> <p>You should be able to see the events generated on your console window with graffiti vandalism being detected within the camera field of view.</p> </li> </ul> <p>[TODO]</p> <p>VisionAI app is available at a Azure Market place, one can download and use it by following steps mentioned here</p>"},{"location":"scenarios/vandalism/#features","title":"Features","text":"<p>The VisionAI solution is the most efficient way of implementing this scenario, as evidenced by the following features:</p> <ul> <li> <p>Prediction: Our vandalism graffiti detection model uses data and historical patterns to predict when and where vandalism and destruction might occur. For example, it can analyze patterns of past vandalism incidents to predict where future incidents might occur.</p> </li> <li> <p>Real-time monitoring: Vandalism graffiti detection model can continuously monitor an area or property in real-time, detecting any instances of vandalism or destruction as they occur. This allows for a rapid response and intervention.</p> </li> <li> <p>Automated alerts: Alerts can automatically be generated to authorities or property owners when incidents of vandalism or destruction are detected. This can help to improve response times and prevent further damage.</p> </li> </ul>"},{"location":"scenarios/vandalism/#training-with-custom-data","title":"Training with custom data","text":"<p>The scenario is provided as part of our GPL-v3 package for VisionAI. If you wish to train this with custom datasets, please contact us and we can provide you with the training code. You can do custom training with your own datasets for free, as long as it complies with GPLv3 license (you give back the code to the community). If you are interested in a custom license, please contact us.</p>"},{"location":"scenarios/vandalism/#contact-us","title":"Contact Us","text":"<ul> <li> <p>For technical issues, you can open a Github issue here</p> </li> <li> <p>For business inquiries, you can contact us through our website</p> </li> </ul>"},{"location":"scenarios/vehicle-activity/","title":"Vehicle Activity Suite","text":"<p>Vehicle activity in and around the factory refers to the movement of vehicles within the factory premises and its surrounding areas. This activity can be monitored and analyzed using various techniques, including the use of sensors, cameras, and other tracking technologies.</p> <p>This suite consists of various scenarios that are designed to manage, analyze, and optimize vehicle activity in and around the factory. These include:</p> <ul> <li>vehicle activity detection</li> <li>vehicle speed detection</li> <li>vehicle policies enforcement</li> <li>vehicle usage monitoring</li> <li>vehicle cargo detection</li> <li>vehicle licence plate detection</li> </ul>"},{"location":"scenarios/vehicle-cargo/","title":"Vehicle Cargo Monitoring","text":"<p>An intelligent alarm system that could be used to detect cargo in vehicles</p>"},{"location":"scenarios/vehicle-cargo/#overview","title":"Overview","text":"<p>Vehicle cargo monitoring is a system that uses cameras to monitor the cargo in vehicles. It is used to enforce cargo limits and calculate fine amounts, as well as to manage the flow of traffic. It is also used to detect vehicles that are carrying cargo that is not allowed for the conditions, such as when the road is wet or icy, or when there is heavy traffic.</p>"},{"location":"scenarios/vehicle-cargo/#vision-ai-based-monitoring","title":"Vision AI based monitoring","text":"<p>VisionAI's cargo monitoring technology can be used to monitor and enforce vehicle cargo in the workplace. With our Vision AI monitoring you can authorize access as well as continuous monitor live feeds inside a restricted area for real-time detection of unauthorized personnel. Our fully automated detection models are not only more powerful and accurate than existing systems but also more affordable and easy to integrate into existing infrastructure allowing users to scale the power of i-based real-time detection with a few simple clicks.</p>"},{"location":"scenarios/vehicle-cargo/#events","title":"Events","text":"<p>VisionAI model's generated events would be:</p> <ul> <li>Vehicle cargo exceeds volume limit</li> <li>Vehicle cargo subceeds volume limit</li> </ul> <p>It is recommended that any instance of such event be reported to the appropriate authority. An event data may include information such as:</p> <ul> <li>Date and time of the event</li> <li>Location of the event</li> <li>license plate number</li> <li>Image of the event</li> <li>Video of the event</li> </ul>"},{"location":"scenarios/vehicle-cargo/#configuration","title":"Configuration","text":"<p>To set up a camera system to detect cargo usage in vehicles, you will need to consider the following:</p> <ul> <li> <p>Camera Placement: Cameras should be placed in locations where they can capture clear images of license plates, such as at entrances and exits to parking lots, toll booths, or intersections. Cameras should be mounted at an appropriate height and angle to capture the entire license plate.</p> </li> <li> <p>Camera Type: High-resolution cameras with a minimum resolution of 1080p are recommended for license plate detection. Cameras with a wide field of view (FOV) are also recommended to capture license plates from a distance.</p> </li> <li> <p>Lighting: Adequate lighting is essential for license plate detection. The lighting should be bright and evenly distributed to minimize shadows and glare.</p> </li> </ul>"},{"location":"scenarios/vehicle-cargo/#model-details","title":"Model Details","text":""},{"location":"scenarios/vehicle-cargo/#dataset","title":"Dataset","text":"<p>The dataset consists of images and videos collected from diverse sources and is designed to reflect real-world scenarios. It is evenly distributed with;</p> <ul> <li>Different environments: Both indoor and outdoor with varying/contrasting surrounding and infrastructure details</li> <li>Different lighting conditions: Day and night with varying light intensities</li> <li>Different camera angles: Front, side, and rear views</li> <li>Different vehicle types: Cars, trucks, buses, and motorcycles</li> <li>Different vehicle colors etc.</li> </ul>"},{"location":"scenarios/vehicle-cargo/#model","title":"Model","text":"<p>The model to monitor enforcement of vehicle speeding event is in progress and it will be released soon.</p>"},{"location":"scenarios/vehicle-cargo/#scenario-details","title":"Scenario details","text":"<p>The business logic for this scenario is as follows: </p> <ul> <li> <p>We use existing camera feeds from the premises to monitor occurrences of vehicle cargo usage events. </p> </li> <li> <p>VisionAI systemis able to run on edge devices. It uses camera feeds for processing. </p> </li> <li> <p>We detect vehicle cargos in the camera feed, an alert is raised.</p> </li> </ul> Test now with online Web-CamWith RTSP Camera - PipelinesWith Azure Setup <p>To test this model &amp; scenario, you can use the following steps:</p> <ul> <li> <p>Install the visionai package from PyPI</p> <pre><code>$ pip install visionai\n</code></pre> </li> <li> <p>Test the scenario from your local web-cam</p> <pre><code>$ visionai scenario test vehicle-cargo-detection\n\nDownloading models for scenario: vehicle-cargo-detection\nModel: vehicle-cargo-detection: https://workplaceos.blob.core.windows.net/models/yolov5s-people/yolov5s-people-0.0.4.zip\nStarting scenario: vehicle-cargo-detection..\n</code></pre> </li> <li> <p>You should be able to see the events generated on your console window with the detections of vehicle cargo  monitoring event within the camera field of view.</p> </li> </ul> <p>[TODO]</p> <p>VisionAI app is available at a Azure Market place, one can download and use it by following steps mentioned here</p>"},{"location":"scenarios/vehicle-cargo/#features","title":"Features","text":"<p>VisionAI based cargo monitoring system exhibits following features:</p> <ul> <li> <p>Real-time detection: VisionAI based cargo monitoring system can detect cargo in vehicles in real-time. This is achieved by running the detection model on the camera feed.</p> </li> <li> <p>Scalable: VisionAI based cargo monitoring system can be scaled to monitor multiple cameras at the same time. This is achieved by running the detection model on the camera feed.</p> </li> <li> <p>Accurate: VisionAI based cargo monitoring system can detect cargo in vehicles with high accuracy. This is achieved by running the detection model on the camera feed.</p> </li> </ul>"},{"location":"scenarios/vehicle-cargo/#training-with-custom-data","title":"Training with custom data","text":"<p>The scenario is provided as part of our GPL-v3 package for VisionAI. If you wish to train this with custom datasets, please contact us and we can provide you with the training code. You can do custom training with your own datasets for free, as long as it complies with GPLv3 license (you give back the code to the community). If you are interested in a custom license, please contact us.</p>"},{"location":"scenarios/vehicle-cargo/#contact-us","title":"Contact Us","text":"<ul> <li>For technical issues, you can open a Github issue here.</li> <li>For business inquiries, you can contact us through our website.</li> </ul>"},{"location":"scenarios/vehicle-license-plate/","title":"Vehicle Licence Plate Detection","text":"<p>An intelligent alarm system that could be used to detect vehicle license plate numbers</p>"},{"location":"scenarios/vehicle-license-plate/#overview","title":"Overview","text":"<p>Vehicle license plate detection uses image processing algorithms to identify and extract the license plate information from images or video frames captured by a camera. License plate detection can be used for various purposes, including enforcing traffic laws, toll collection, parking management, and law enforcement.</p>"},{"location":"scenarios/vehicle-license-plate/#vision-ai-based-monitoring","title":"Vision AI based monitoring","text":"<p>VisionAI's license plate monitoring is an advanced technology that uses computer vision algorithms and machine learning models to detect and read license plates in real-time.</p>"},{"location":"scenarios/vehicle-license-plate/#events","title":"Events","text":"<p>VisionAI model's generated events would be:</p> <ul> <li>Vehicle licence plate detected event</li> </ul> <p>It is recommended that any instance of such event be reported to the appropriate authority. An event data may include information such as:</p> <ul> <li>Date and time of the event</li> <li>Location of the event</li> <li>license plate number</li> <li>Image of the event</li> <li>Video of the event</li> </ul>"},{"location":"scenarios/vehicle-license-plate/#configuration","title":"Configuration","text":"<p>To set up a camera system to detect license plates, you need to consider several factors including:</p> <ul> <li> <p>Camera Placement: Cameras should be placed in locations where they can capture clear images of license plates, such as at entrances and exits to parking lots, toll booths, or intersections. Cameras should be mounted at an appropriate height and angle to capture the entire license plate.</p> </li> <li> <p>Camera Type: High-resolution cameras with a minimum resolution of 1080p are recommended for license plate detection. Cameras with a wide field of view (FOV) are also recommended to capture license plates from a distance.</p> </li> <li> <p>Lighting: Adequate lighting is essential for license plate detection. The lighting should be bright and evenly distributed to minimize shadows and glare.</p> </li> </ul> <p>Note</p> <p>Overall, camera setup for license plate detection requires careful planning and optimization to ensure accurate and efficient identification of vehicles. With the right equipment and image processing algorithms, license plate detection can provide valuable insights for traffic management, parking management, law enforcement, and security applications. </p>"},{"location":"scenarios/vehicle-license-plate/#model-details","title":"Model Details","text":""},{"location":"scenarios/vehicle-license-plate/#dataset","title":"Dataset","text":"<p>The dataset consists of images and videos collected from diverse sources and is designed to reflect real-world scenarios. It is evenly distributed with;</p> <ul> <li>Different environments: Both indoor and outdoor with varying/contrasting surrounding and infrastructure details</li> <li>Different lighting conditions: Day and night with varying light intensities</li> <li>Different camera angles: Front, side, and rear views</li> <li>Different vehicle types: Cars, trucks, buses, and motorcycles</li> <li>Different vehicle colors etc.</li> </ul>"},{"location":"scenarios/vehicle-license-plate/#model","title":"Model","text":"<p>The model is based on the YOLOv5 algorithm to detect licence plates. It is trained on the curated dataset. Licence plate blurring is performed using computer vision-based blurring operations. The model is developed in a way that it generalizes well for different environments and situations.</p> <p>The licence plate detection model based on Yolov5 recorded the following performance metrics:</p> Precision Recall mAP  <p>The model is adaptable enough to run on any edge computing device.</p>"},{"location":"scenarios/vehicle-license-plate/#scenario-details","title":"Scenario details","text":"<p>The business logic for this scenario is as follows: </p> <ul> <li> <p>We use existing camera feeds from the premises to monitor occurrences of vehicle licence events. </p> </li> <li> <p>VisionAI systemis able to run on edge devices. It uses camera feeds for processing. </p> </li> <li> <p>We detect vehicle licence plates in the camera feed, an alert is raised.</p> </li> </ul> Test now with online Web-CamWith RTSP Camera - PipelinesWith Azure Setup <p>To test this model &amp; scenario, you can use the following steps:</p> <ul> <li> <p>Install the visionai package from PyPI</p> <pre><code>$ pip install visionai\n</code></pre> </li> <li> <p>Test the scenario from your local web-cam</p> <pre><code>$ visionai scenario test vehicle-licence-detection\n\nDownloading models for scenario: vehicle-licence-detection\nModel: vehicle-licence-detection: https://workplaceos.blob.core.windows.net/models/yolov5s-people/yolov5s-people-0.0.4.zip\nStarting scenario: vehicle-licence-detection..\n</code></pre> </li> <li> <p>You should be able to see the events generated on your console window with the detections of vehicle licence plate monitoring event within the camera field of view.</p> </li> </ul> <p>[TODO]</p> <p>VisionAI app is available at a Azure Market place, one can download and use it by following steps mentioned here</p>"},{"location":"scenarios/vehicle-license-plate/#features","title":"Features","text":"<p>VisionAI based vehicle monitoring can offer several features to enhance vehicle monitoring and policy enforcement. Here are some examples of features:</p> <ul> <li> <p>Object Detection: Our monitoring systems can use computer vision to detect objects such as other vehicles, , which can provide additional information about the driving environment and help identify potential hazards.</p> </li> <li> <p>Anomaly Detection: AI-based systems can use machine learning algorithms to detect anomalous behavior, such as unusual driving patterns or irregular fuel consumption, which can help identify potential policy violations or security breaches.</p> </li> <li> <p>Real-time Alerts: Our AI-based systems can provide real-time alerts for vehicle policy violations, allowing for prompt corrective action to be taken.</p> </li> </ul> <p>Data Analytics: VisionAI system can provide detailed analytics and reports on vehicle usage, and compliance with policies, which can help identify areas for improvement and inform policy adjustments.</p> <p>Note</p> <p>Overall, AI-based vehicle monitoring can provide enhanced monitoring capabilities and valuable insights into vehicle usage and driver behavior, which can help improve safety, efficiency, and compliance with vehicle policies.</p>"},{"location":"scenarios/vehicle-license-plate/#training-with-custom-data","title":"Training with custom data","text":"<p>The scenario is provided as part of our GPL-v3 package for VisionAI. If you wish to train this with custom datasets, please contact us and we can provide you with the training code. You can do custom training with your own datasets for free, as long as it complies with GPLv3 license (you give back the code to the community). If you are interested in a custom license, please contact us.</p>"},{"location":"scenarios/vehicle-license-plate/#contact-us","title":"Contact Us","text":"<ul> <li>For technical issues, you can open a Github issue here.</li> <li>For business inquiries, you can contact us through our website.</li> </ul>"},{"location":"scenarios/vehicle-policies/","title":"Vehicle Policies","text":"<p>An intelligent alarm system that could be used to detect fraudulent vehicle activity</p>"},{"location":"scenarios/vehicle-policies/#overview","title":"Overview","text":"<p>Vehicle policies at the workplace are a set of guidelines and rules that govern the use of vehicles owned or leased by a company or used by employees for work-related purposes. These policies are designed to ensure the safe and responsible use of company vehicles, as well as to protect the company from liability in case of accidents or other incidents.</p>"},{"location":"scenarios/vehicle-policies/#vision-ai-based-monitoring","title":"Vision AI based monitoring","text":"<p>VisionAI technology can be used to monitor and enforce vehicle policies in the workplace. With our Vision AI monitoring you can authorize access as well as continuous monitor live feeds inside a restricted area for real-time detection of unauthorized personnel. Our fully automated detection models are not only more powerful and accurate than existing systems but also more affordable and easy to integrate into existing infrastructure allowing users to scale the power of i-based real-time detection with a few simple clicks.</p>"},{"location":"scenarios/vehicle-policies/#events","title":"Events","text":"<p>VisionAI model's generated events would be:</p> <ul> <li>Vehicle activity detected in non-designtated areas</li> <li>Vehicle activity detected during after-hours</li> <li>Collision event detected</li> <li>Near collision event detected</li> </ul> <p>It is recommended that any instance of such events be reported to the appropriate authority. An event data may include information such as:</p> <ul> <li>Date and time of the event</li> <li>Location of the event</li> <li>Type of event (collision, near collision, etc.)</li> <li>Image of the event</li> <li>Video of the event</li> <li>Vehicle license plate number</li> </ul>"},{"location":"scenarios/vehicle-policies/#configuration","title":"Configuration","text":"<p>Camera setups can be used to detect and enforce vehicle policies in the workplace. The location of cameras to monitor vehicle policies will depend on the specific policies being enforced and the nature of the work environment. For example, </p> <ul> <li>if the company has a policy that prohibits employees from using company vehicles for personal use, then cameras should be installed in areas where employees are likely to park their vehicles. </li> </ul>"},{"location":"scenarios/vehicle-policies/#model-details","title":"Model Details","text":""},{"location":"scenarios/vehicle-policies/#dataset","title":"Dataset","text":"<p>The dataset consists of images and videos collected from diverse sources and is designed to reflect real-world scenarios. It is evenly distributed with;</p> <ul> <li>Different environments: Both indoor and outdoor with varying/contrasting surrounding and infrastructure details</li> <li>Different lighting conditions: Day and night with varying light intensities</li> <li>Different camera angles: Front, side, and rear views</li> <li>Different vehicle types: Cars, trucks, buses, and motorcycles</li> <li>Different vehicle colors etc.</li> </ul>"},{"location":"scenarios/vehicle-policies/#model","title":"Model","text":"<p>The model to detect enforcement of vehicle policies event is in progress and it will be released soon.</p>"},{"location":"scenarios/vehicle-policies/#scenario-details","title":"Scenario details","text":"<p>The business logic for this scenario is as follows: </p> <ul> <li> <p>We use existing camera feeds from the premises to monitor and detect occurrences of vehicle non-compliance events. </p> </li> <li> <p>VisionAI systemis able to run on edge devices. It uses camera feeds for processing. </p> </li> <li> <p>We detect various vehicle events in the camera feed, an alert is raised.</p> </li> </ul> Test now with online Web-CamWith RTSP Camera - PipelinesWith Azure Setup <p>To test this model &amp; scenario, you can use the following steps:</p> <ul> <li> <p>Install the visionai package from PyPI</p> <pre><code>$ pip install visionai\n</code></pre> </li> <li> <p>Test the scenario from your local web-cam</p> <pre><code>$ visionai scenario test vehicle-detection\n\nDownloading models for scenario: vehicle-detection\nModel: vehicle-detection: https://workplaceos.blob.core.windows.net/models/yolov5s-people/yolov5s-people-0.0.4.zip\nStarting scenario: vehicle-detection..\n</code></pre> </li> <li> <p>You should be able to see the events generated on your console window with the detections of vehicle monitoring event within the camera field of view.</p> </li> </ul> <p>[TODO]</p> <p>VisionAI app is available at a Azure Market place, one can download and use it by following steps mentioned here</p>"},{"location":"scenarios/vehicle-policies/#features","title":"Features","text":"<p>VisionAI based vehicle monitoring can offer several features to enhance vehicle monitoring and policy enforcement. Here are some examples of features:</p> <ul> <li> <p>Object Detection: Our monitoring systems can use computer vision to detect objects such as other vehicles, , which can provide additional information about the driving environment and help identify potential hazards.</p> </li> <li> <p>Anomaly Detection: AI-based systems can use machine learning algorithms to detect anomalous behavior, such as unusual driving patterns or irregular fuel consumption, which can help identify potential policy violations or security breaches.</p> </li> <li> <p>Real-time Alerts: Our AI-based systems can provide real-time alerts for vehicle policy violations, allowing for prompt corrective action to be taken.</p> </li> </ul> <p>Data Analytics: VisionAI system can provide detailed analytics and reports on vehicle usage, and compliance with policies, which can help identify areas for improvement and inform policy adjustments.</p> <p>Note</p> <p>Overall, AI-based vehicle monitoring can provide enhanced monitoring capabilities and valuable insights into vehicle usage and driver behavior, which can help improve safety, efficiency, and compliance with vehicle policies.</p>"},{"location":"scenarios/vehicle-policies/#training-with-custom-data","title":"Training with custom data","text":"<p>The scenario is provided as part of our GPL-v3 package for VisionAI. If you wish to train this with custom datasets, please contact us and we can provide you with the training code. You can do custom training with your own datasets for free, as long as it complies with GPLv3 license (you give back the code to the community). If you are interested in a custom license, please contact us.</p>"},{"location":"scenarios/vehicle-policies/#contact-us","title":"Contact Us","text":"<ul> <li>For technical issues, you can open a Github issue here.</li> <li>For business inquiries, you can contact us through our website.</li> </ul>"},{"location":"scenarios/vehicle-speed/","title":"Vehicle Speed Monitoring","text":"<p>An intelligent alarm system that could be used to monitor vehicle speed</p>"},{"location":"scenarios/vehicle-speed/#overview","title":"Overview","text":"<p>Vehicle speed monitoring is a system that uses cameras to monitor the speed of vehicles on a road. It is used to enforce speed limits and calculate fine amounts, as well as to manage the flow of traffic. It is also used to detect vehicles that are travelling at an unsafe speed for the conditions, such as when the road is wet or icy, or when there is heavy traffic.</p>"},{"location":"scenarios/vehicle-speed/#vision-ai-based-monitoring","title":"Vision AI based monitoring","text":"<p>VisionAI based monitoring technology can be used to monitor and enforce vehicle speed in the workplace. With our Vision AI monitoring you can authorize access as well as continuous monitor live feeds inside a restricted area for real-time detection of unauthorized personnel. Our fully automated detection models are not only more powerful and accurate than existing systems but also more affordable and easy to integrate into existing infrastructure allowing users to scale the power of i-based real-time detection with a few simple clicks.</p>"},{"location":"scenarios/vehicle-speed/#events","title":"Events","text":"<p>VisionAI model's generated events would be:</p> <ul> <li>Vehicle speed detected above the speed limit</li> </ul> <p>It is recommended that any instance of such events be reported to the appropriate authority. An event data may include information such as:</p> <ul> <li>Date and time of the event</li> <li>Location of the event</li> <li>type of event (speeding, etc.)</li> <li>Image of the event</li> <li>Video of the event</li> <li>Vehicle license plate number</li> </ul>"},{"location":"scenarios/vehicle-speed/#configuration","title":"Configuration","text":"<p>Camera setups can be used to detect and enforce vehicle speeding in the workplace. The location of cameras to monitor vehicle speeding will depend on the specific policies being enforced and the nature of the work environment.      </p>"},{"location":"scenarios/vehicle-speed/#model-details","title":"Model Details","text":""},{"location":"scenarios/vehicle-speed/#dataset","title":"Dataset","text":"<p>The dataset consists of images and videos collected from diverse sources and is designed to reflect real-world scenarios. It is evenly distributed with;</p> <ul> <li>Different environments: Both indoor and outdoor with varying/contrasting surrounding and infrastructure details</li> <li>Different lighting conditions: Day and night with varying light intensities</li> <li>Different camera angles: Front, side, and rear views</li> <li>Different vehicle types: Cars, trucks, buses, and motorcycles</li> <li>Different vehicle colors etc.</li> </ul>"},{"location":"scenarios/vehicle-speed/#model","title":"Model","text":"<p>The model to monitor enforcement of vehicle speeding event is in progress and it will be released soon.</p>"},{"location":"scenarios/vehicle-speed/#scenario-details","title":"Scenario details","text":"<p>The business logic for this scenario is as follows: </p> <ul> <li> <p>We use existing camera feeds from the premises to monitor vehicle speeding events. </p> </li> <li> <p>VisionAI system is able to run on edge devices. It uses camera feeds for processing. </p> </li> <li> <p>We detect vehicle speeding event in the camera feed, an alert is raised.</p> </li> </ul> Test now with online Web-CamWith RTSP Camera - PipelinesWith Azure Setup <p>To test this model &amp; scenario, you can use the following steps:</p> <ul> <li> <p>Install the visionai package from PyPI</p> <pre><code>$ pip install visionai\n</code></pre> </li> <li> <p>Test the scenario from your local web-cam</p> <pre><code>$ visionai scenario test vehicle-detection\n\nDownloading models for scenario: vehicle-speeding-detection\nModel: vehicle-speeding-detection: https://workplaceos.blob.core.windows.net/models/yolov5s-people/yolov5s-people-0.0.4.zip\nStarting scenario: vehicle-speeding-detection..\n</code></pre> </li> <li> <p>You should be able to see the events generated on your console window with the monitoring of vehicle speed event within the camera field of view.</p> </li> </ul> <p>[TODO]</p> <p>VisionAI app is available at a Azure Market place, one can download and use it by following steps mentioned here</p>"},{"location":"scenarios/vehicle-speed/#features","title":"Features","text":"<p>VisionAI based vehicle speed monitoring can offer several features to enhance vehicle monitoring and policy enforcement. Here are some examples of features:</p> <ul> <li> <p>Vehicle speed monitoring: VisionAI can be used to monitor vehicle speed and enforce speed limits. This can be used to enforce speed limits and calculate fine amounts, as well as to manage the flow of traffic. It can also be used to detect vehicles that are travelling at an unsafe speed for the conditions, such as when the road is wet or icy, or when there is heavy traffic.</p> </li> <li> <p>Vehicle speed detection: VisionAI can be used to detect vehicle speed and alert when a vehicle is travelling at an unsafe speed. This can be used to detect vehicles that are travelling at an unsafe speed for the conditions, such as when the road is wet or icy, or when there is heavy traffic.</p> </li> <li> <p>Vehicle speed enforcement: VisionAI can be used to enforce vehicle speed limits and calculate fine amounts. This can be used to enforce speed limits and calculate fine amounts, as well as to manage the flow of traffic.</p> </li> </ul>"},{"location":"scenarios/vehicle-speed/#training-with-custom-data","title":"Training with custom data","text":"<p>The scenario is provided as part of our GPL-v3 package for VisionAI. If you wish to train this with custom datasets, please contact us and we can provide you with the training code. You can do custom training with your own datasets for free, as long as it complies with GPLv3 license (you give back the code to the community). If you are interested in a custom license, please contact us.</p>"},{"location":"scenarios/vehicle-speed/#contact-us","title":"Contact Us","text":"<ul> <li>For technical issues, you can open a Github issue here.</li> <li>For business inquiries, you can contact us through our website.</li> </ul>"},{"location":"scenarios/vehicle-usage/","title":"Vehicle Usage Detection","text":"<p>An intelligent alarm system that could be used to detect vehicle usage</p>"},{"location":"scenarios/vehicle-usage/#overview","title":"Overview","text":"<p>Vehicle usage monitoring is a system that uses cameras to monitor the usage of vehicles on a road. It is used to enforce usage limits and calculate fine amounts, as well as to manage the flow of traffic. It is also used to detect vehicles that are travelling at an unsafe speed for the conditions, such as when the road is wet or icy, or when there is heavy traffic.</p>"},{"location":"scenarios/vehicle-usage/#vision-ai-based-monitoring","title":"Vision AI based monitoring","text":"<p>VisionAI's vehicle usage detection model can be used to monitor and enforce vehicle usage in the workplace. With our Vision AI monitoring you can authorize access as well as continuous monitor live feeds inside a restricted area for real-time detection of unauthorized personnel. Our fully automated detection models are not only more powerful and accurate than existing systems but also more affordable and easy to integrate into existing infrastructure allowing users to scale the power of i-based real-time detection with a few simple clicks.              </p>"},{"location":"scenarios/vehicle-usage/#events","title":"Events","text":"<p>VisionAI model's generated events would be:</p> <ul> <li>Daily summary event of vehicle usage</li> <li>Path-map of vehicle usage</li> </ul> <p>It is recommended that any instance of such event be reported to the appropriate authority.</p>"},{"location":"scenarios/vehicle-usage/#configuration","title":"Configuration","text":"<p>To set up a camera system to detect vehicle usage, you need to consider several factors including:</p> <ul> <li> <p>Camera type: The type of camera you use will depend on the environment you are monitoring. For example, if you are monitoring a parking lot, you may want to use a camera with a wide field of view. If you are monitoring a road, you may want to use a camera with a narrow field of view. You may also want to consider the camera\u2019s resolution, frame rate, and other specifications to ensure that it can capture license plates clearly and accurately.</p> </li> <li> <p>Camera placement: The location of cameras to monitor license plates will depend on the specific policies being enforced and the nature of the work environment. For example, if you are monitoring a parking lot, you may want to place cameras at the entrance and exit of the lot. If you are monitoring a road, you may want to place cameras at intersections or other locations where vehicles are likely to stop. You may also want to consider the camera\u2019s field of view and other specifications to ensure that it can capture license plates clearly and accurately.</p> </li> </ul>"},{"location":"scenarios/vehicle-usage/#model-details","title":"Model Details","text":""},{"location":"scenarios/vehicle-usage/#dataset","title":"Dataset","text":"<p>The dataset consists of images and videos collected from diverse sources and is designed to reflect real-world scenarios. It is evenly distributed with;</p> <ul> <li>Different environments: Both indoor and outdoor with varying/contrasting surrounding and infrastructure details</li> <li>Different lighting conditions: Day and night with varying light intensities</li> <li>Different camera angles: Front, side, and rear views</li> <li>Different vehicle types: Cars, trucks, buses, and motorcycles</li> <li>Different vehicle colors etc.</li> </ul>"},{"location":"scenarios/vehicle-usage/#model","title":"Model","text":"<p>The model to detect enforcement of vehicle policies event is in progress and it will be released soon.</p>"},{"location":"scenarios/vehicle-usage/#scenario-details","title":"Scenario details","text":"<p>The business logic for this scenario is as follows: </p> <ul> <li> <p>We use existing camera feeds from the premises to monitor occurrences of vehicle licence events. </p> </li> <li> <p>VisionAI systemis able to run on edge devices. It uses camera feeds for processing. </p> </li> <li> <p>We detect vehicle licence plates in the camera feed, an alert is raised.</p> </li> </ul> Test now with online Web-CamWith RTSP Camera - PipelinesWith Azure Setup <p>To test this model &amp; scenario, you can use the following steps:</p> <ul> <li> <p>Install the visionai package from PyPI</p> <pre><code>$ pip install visionai\n</code></pre> </li> <li> <p>Test the scenario from your local web-cam</p> <pre><code>$ visionai scenario test vehicle-usage-detection\n\nDownloading models for scenario: vehicle-usage-detection\nModel: vehicle-usage-detection: https://workplaceos.blob.core.windows.net/models/yolov5s-people/yolov5s-people-0.0.4.zip\nStarting scenario: vehicle-usage-detection..\n</code></pre> </li> <li> <p>You should be able to see the events generated on your console window with the detections of vehicle usage event within the camera field of view.</p> </li> </ul> <p>[TODO]</p> <p>VisionAI app is available at a Azure Market place, one can download and use it by following steps mentioned here</p>"},{"location":"scenarios/vehicle-usage/#features","title":"Features","text":"<p>VisionAI based vehicle usage monitoring can offer several features to enhance vehicle monitoring and policy enforcement. Here are some examples of features:</p> <ul> <li> <p>Daily Summary report: A daily summary of vehicle usage events can provide valuable insights into the performance, efficiency, and compliance of a fleet of vehicles. This information can be used to identify areas for improvement, optimize vehicle utilization, reduce costs, and ensure compliance with policies and regulations.</p> </li> <li> <p>Path-map of vehicle usage: This refers to a graphical representation of the routes taken by a vehicle over a given period of time. This map can provide a visual representation of the vehicle's movements, including the starting and ending locations, stops along the way, and the route taken.</p> </li> </ul>"},{"location":"scenarios/vehicle-usage/#training-with-custom-data","title":"Training with custom data","text":"<p>The scenario is provided as part of our GPL-v3 package for VisionAI. If you wish to train this with custom datasets, please contact us and we can provide you with the training code. You can do custom training with your own datasets for free, as long as it complies with GPLv3 license (you give back the code to the community). If you are interested in a custom license, please contact us.</p>"},{"location":"scenarios/vehicle-usage/#contact-us","title":"Contact Us","text":"<ul> <li>For technical issues, you can open a Github issue here.</li> <li>For business inquiries, you can contact us through our website.</li> </ul>"},{"location":"scenarios/worker-fatigue-detection/","title":"Worker Fatigue Detection","text":"<p>Stay alert, stay safe with our advanced worker fatigue detection model.</p> <p>Worker fatigue detection event</p>"},{"location":"scenarios/worker-fatigue-detection/#overview","title":"Overview","text":"<p>Fatigue can have a significant impact on a person's performance, productivity, and safety. Fatigue is a state of physical or mental exhaustion that can result from prolonged periods of work, inadequate rest, or sleep disturbances. When workers are fatigued, they may experience a range of symptoms, such as slower reaction times, decreased alertness and vigilance, impaired decision-making, and reduced coordination and motor skills. These symptoms can increase the risk of accidents, injuries, and errors in the workplace, particularly in high-risk industries such as transportation, aviation, and mining. </p>"},{"location":"scenarios/worker-fatigue-detection/#vision-ai-based-monitoring","title":"Vision AI based monitoring","text":"<p>VisionAI based monitoring is an effective approach for fatigue detection because it allows for non-intrusive and continuous monitoring of individuals. These systems can monitor various facial and eye movements, such as eyelid closure, head drooping, yawning, and pupil dilation, to detect changes in behavior that may indicate fatigue. And its ability to provide real-time alerts and warnings to individuals and operators when signs of fatigue are detected. This can help prevent accidents and improve safety in high-risk environments. </p> <p>Overall, visionAI based monitoring is an effective and non-intrusive approach for fatigue detection, with potential applications in a range of industries and environments. By using advanced computer vision techniques and machine learning algorithms, visionAI systems can help improve safety, productivity, and overall well-being in the workplace. </p>"},{"location":"scenarios/worker-fatigue-detection/#model-details","title":"Model Details","text":""},{"location":"scenarios/worker-fatigue-detection/#dataset","title":"Dataset","text":"<p>The dataset for this scenario is based on real-world posture, behavior, and movements and also to detect signs of fatigue, such as slouching, yawning, or slowing down in work events. The dataset consists of images and videos collected from various sources. </p>"},{"location":"scenarios/worker-fatigue-detection/#model","title":"Model","text":"<p>The model to detect worker fatigue event is in progress and it will be released soon. </p>"},{"location":"scenarios/worker-fatigue-detection/#scenario-details","title":"Scenario details","text":"<p>The business logic for this scenario is as follows: </p> <ul> <li> <p>We use existing camera feeds from the premises to monitor the safety of its workers who operate heavy machinery, particularly during long shifts. These systems can monitor various facial and eye movements, such as eyelid closure, head drooping, yawning, and pupil dilation, to detect changes in behavior that may indicate fatigue.</p> </li> <li> <p>VisionAI system is able to run on edge devices. It uses camera feeds for processing. </p> </li> <li> <p>We detect signs of fatigue such as slouching, yawning, or slowing down in work and alert the worker to take a break or switch with another worker, in the camera feed. </p> </li> <li> <p>An alarming system is in place as part of an worker fatigue detection solution. </p> </li> </ul> Test now with online Web-CamWith RTSP Camera - PipelinesWith Azure Setup <p>To test this model &amp; scenario, you can use the following steps:</p> <ul> <li> <p>Install the visionai package from PyPI</p> <pre><code>$ pip install visionai\n</code></pre> </li> <li> <p>Test the scenario from your local web-cam</p> <pre><code>$ visionai scenario test worker-fatigue-detection\n\nDownloading models for scenario: worker-fatigue-detection\nModel: worker-fatigue-detection: https://workplaceos.blob.core.windows.net/models/yolov5s-worker-fatigue-detection/yolov5s-worker-fatigue-detection-0.0.4.zip\nStarting scenario: worker-fatigue-detection..\n</code></pre> </li> <li> <p>You should be able to see the events generated on your console window with the detections of worker fatigue event within the camera field of view.</p> </li> </ul> <p>[TODO]</p> <p>VisionAI app is available at a Azure Market place, one can download and use it by following steps mentioned here</p>"},{"location":"scenarios/worker-fatigue-detection/#features","title":"Features","text":"<ul> <li> <p>Real-time monitoring: Worker fatigue detection model is capable of real-time monitoring of workers' fatigue levels. This means that it is be able to detect signs of fatigue as they occur, rather than relying on post hoc analysis.</p> </li> <li> <p>Alert system: The model is able to alert supervisors or managers when a worker's fatigue level exceeds the threshold. This can be in the form of an automated alert or a visual warning on a dashboard.</p> </li> <li> <p>Customization: The model should be customizable to the specific needs of the workplace, including factors such as lighting, noise levels, and work schedules, which can all impact the detection of worker fatigue.</p> </li> </ul>"},{"location":"scenarios/worker-fatigue-detection/#training-with-custom-data","title":"Training with custom data","text":"<p>The scenario is provided as part of our GPL-v3 package for VisionAI. If you wish to train this with custom datasets, please contact us and we can provide you with the training code. You can do custom training with your own datasets for free, as long as it complies with GPLv3 license (you give back the code to the community). If you are interested in a custom license, please contact us.</p>"},{"location":"scenarios/worker-fatigue-detection/#contact-us","title":"Contact Us","text":"<ul> <li>For technical issues, you can open a Github issue here.</li> <li>For business inquiries, you can contact us through our website.</li> </ul>"},{"location":"scenarios/worker-health-and-safety/","title":"Worker Health and Safety","text":"<p>Workplace injuries are a growing concern for employers and employees. In 2021, the US recorded 5,190 fatal work injuries, with the private sector alone reporting 2.6 million non-fatal injuries. These numbers highlight the need for companies to prioritize workplace safety and implement measures to prevent injuries. Unsafe workplace environments not only result in injuries and fatalities but also account for downtime, reduced productivity, increased healthcare costs, and legal fines.</p> <p>Despite advancements in safety equipment and protective gear, injuries still occur due to non-compliance with protocols and Standard Operating Procedures (SOPs). Therefore, employers are responsible for providing up-to-date safety gear and equipment and ensuring their correct usage and compliance. This should involve live monitoring and enforcing adherence to established protocols and SOPs. By taking these measures, employers can effectively mitigate workplace hazards and prevent injuries from occurring.</p> <p>Establishing protocols and guidelines and providing safety gear might not seem difficult, but ensuring that every individual is consistently complying is challenging. A lapse in compliance and usage of protective equipment (PPE), even for a short duration, can result in a workplace accident. Therefore, you need a mechanism to ensure everyone complies and adheres to guidelines. But the question is, how do you do it?</p>"},{"location":"scenarios/worker-health-and-safety/#eliminate-occupational-hazards-with-visionifys-workplace-health-and-safety-suite","title":"Eliminate Occupational Hazards with Visionify\u2019s Workplace Health and Safety Suite","text":"<p>Leverage Fully Automated, Vision AI-based real-time Detection and Monitoring systems for different workplace scenarios and eliminate occupational hazards and injury risks. Stay on top of the situation with instant alerts and notifications that allow for quick response and resolution of any potential safety concerns, ensuring the well-being of your employees and promoting a culture of safety within your organization. Our next-gen Vision AI models Pre-trained can be deployed instantly to work with any existing camera infrastructure.</p> <p>What\u2019s included in this suite:</p> <ul> <li>PPE Detection </li> <li>Slip and Fall Detection </li> <li>Working at Heights </li> <li>Environment monitoring</li> <li>Slip, trip and fall detection</li> <li>Posture &amp; Ergonomics</li> <li>Empty pallets</li> <li>Spills &amp; Leaks detection (Liquids)</li> <li>Hand sanitizer/hand-wash</li> <li>Worker fatigue detection</li> <li>Worker skin tempreature monitoring</li> <li>Confined spaces monitoring</li> </ul>"},{"location":"scenarios/working-at-heights/","title":"Working at Heights","text":"<p>Ensure the safety of employees at workplace.</p> <p> </p> Events: Working at heights <p></p> <p>Working at heights is a hazardous activity and has the potential to cause serious injuries or fatalities. It is important for employers to ensure that the work place is set up to prevent employees from falling off of elevated surfaces. Employers must provide proper training and equipment to employees who work at heights and must ensure that safety regulations are followed. Employers should also provide periodic reviews to ensure that employees are following safety protocols and that the work environment is safe and secure.</p> <p>Working at heights, such as on a roof or in a tall building, requires specialized safety equipment and training to ensure the safety of the workers. Depending on the job, you may need to wear a safety harness or other protective gear.</p> <p>Falling from heights is a serious hazard, and can result in serious injury or even death. Timely action in the event of a Fall/Slip accident can minimize damage and save lives.</p>"},{"location":"scenarios/working-at-heights/#vision-ai-based-monitoring","title":"Vision AI based monitoring","text":"<p>Vision AI-based system can be used to detect slip and fall with high accuracy. Additionally, our model trained on real-world images minimizes false-positives or false-negatives.  </p> <p>The cameras scan every frame to ensure there are no accidents related to slip and fall cases. </p> <p>To ensure accuracy and reliability for the model, these camera-based monitoring services should be supplemented by strong compliance processes. Furthermore, workers working in different factory units should always be made aware of these accidents and how to safeguard them. </p>"},{"location":"scenarios/working-at-heights/#model-details","title":"Model Details","text":""},{"location":"scenarios/working-at-heights/#dataset","title":"Dataset","text":"<p>Model training is carried out with Microsoft COCO: Common Objects in Context dataset. Only person class is considered for model building. COCO is a  large-scale dataset that addresses three core research problems in scene understanding: detecting non-iconic views (or non-canonical perspectives of objects), contextual reasoning between objects and the precise 2D localization of objects. </p> <p>COCO dataset has an even distribution of: </p> <ul> <li> <p>Different(indoor/outdoor) environments </p> </li> <li> <p>Male vs Female  </p> </li> <li> <p>Different light settings </p> </li> <li> <p>Variations in camera orientations </p> </li> <li> <p>Using security camera feeds </p> </li> </ul>"},{"location":"scenarios/working-at-heights/#model","title":"Model","text":"<p>The model is built using Yolov5 pre-trained model for detecting a person followed by a media pipe library used to estimate the pose of the person. The following performance metrics are recorded: </p> Model Name Precision Recall  mAP   SLIP AND FALL DETECTION 65.0%  71.6%  71.0%  <p>The model is adaptable enough to run on any edge computing device.</p>"},{"location":"scenarios/working-at-heights/#scenario-details","title":"Scenario details","text":"<p>The business logic for this scenario is as follows: </p> <ul> <li> <p>We use existing camera feeds from the premises to monitor and detect occurrences of slip and fall incidents. </p> </li> <li> <p>VisionAI s able to run on edge devices. It uses camera feeds for processing. </p> </li> <li> <p>We detect human poses to identify slip and fall accidents in the camera feed. \u00a0</p> </li> <li>If either slip or fall is detected, an alert is raised.</li> </ul>"},{"location":"scenarios/working-at-heights/#try-it-now","title":"Try it now","text":""},{"location":"scenarios/working-at-heights/#quick-method-using-your-local-web-cam","title":"Quick method - using your local web-cam","text":"<p>To test this model &amp; scenario, you can use the following steps:</p> <ul> <li>Install the visionai package from PyPI</li> </ul> <pre><code>$ pip install visionai\n---&gt; 100%\n</code></pre> <ul> <li>Test the scenario from your local web-cam</li> </ul> <pre><code>$ visionai scenario test slip-and-fall-detection\n\nDownloading models for scenario: slip-and-fall-detection\nModel: slip-and-fall-detection: https://workplaceos.blob.core.windows.net/models/yolov5s-people/yolov5s-people-0.0.4.zip\n---&gt; 100%\nStarting scenario: slip-and-fall-detection..\n</code></pre> <ul> <li>You should be able to see the events generated on your console window with slip and fall being detected within the camera field of view.</li> </ul>"},{"location":"scenarios/working-at-heights/#in-an-actual-environment","title":"In an actual environment","text":"<p>To use this scenario in an actual environment, you can follow these steps:</p> <ul> <li>Install the visionai package from PyPI</li> </ul> <pre><code>$ pip install visionai\n---&gt; 100%\n</code></pre> <ul> <li>Download the scenario</li> </ul> <pre><code>$ visionai scenario download slip-and-fall-detection\n\nDownloading models for scenario: slip-and-fall-detection\nModel: slip-and-fall-detection\nhttps://workplaceos.blob.core.windows.net/models/yolov5s-people/yolov5s-people-0.0.4.zip\n---&gt; 100%\n</code></pre> <ul> <li>Add the camera feed to the scenario</li> </ul> <pre><code>$ visionai camera add OFFICE-01 --url rtsp://192.168.0.1/stream1\n$ visionai camera OFFICE-01 add-scenario slip-and-fall-detection\n$ visionai run\n\nStarting scenario: slip-and-fall-detection..\n</code></pre> <ul> <li>You should be able to see the events generated on your console window with slip and fall being detected within the camera field of view.</li> </ul> <p>For more details visit VisionAI web application.</p>"},{"location":"scenarios/working-at-heights/#training-with-custom-data","title":"Training with custom data","text":"<p>The scenario is provided as part of our GPL-v3 package for VisionAI. If you wish to train this with custom datasets, please contact us and we can provide you with the training code. You can do custom training with your own datasets for free, as long as it complies with GPLv3 license (you give back the code to the community). If you are interested in a custom license, please (contact us)[contact.md].</p>"},{"location":"scenarios/working-at-heights/#contact-us","title":"Contact Us","text":"<ul> <li>For technical issues, you can open a Github issue here.</li> <li>For business inquiries, you can contact us through our website.</li> </ul>"}]}